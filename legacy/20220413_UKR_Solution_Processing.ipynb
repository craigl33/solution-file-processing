{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution processing---> Choose folder and run all cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    " \n",
    "proj_dir = 'Y:/Modelling/Ukraine/2023_UKR_ST_Security/'\n",
    "gen_params_path = 'I:/DOCS/RISE-GER/UKR/Power_system_modernisation/Data/2023_10_20_generator_parameters_UKR.xlsx'\n",
    "\n",
    "soln_idx = pd.read_excel(gen_params_path,  sheet_name='SolutionIndex', engine='openpyxl')\n",
    "# tech_info_idx = pd.read_excel(gen_params_path,  sheet_name='Indice', engine='openpyxl')\n",
    "full_soln_idx = soln_idx.copy()\n",
    "\n",
    "rootdir = proj_dir + '04_SolutionFiles/'\n",
    "soln_dirs = [f for f in os.listdir(rootdir) if os.path.isdir(os.path.join(rootdir, f)) & (f != 'archived')]\n",
    "soln_dirs.sort(reverse=True)\n",
    "\n",
    "soln_choice = widgets.Dropdown(\n",
    "                options=soln_dirs,\n",
    "                value=soln_dirs[0],\n",
    "                description='Model runs:',\n",
    "                disabled=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co2_targets = pd.read_excel('C:/Users/HART_C/International Energy Agency/EMS-RISE - Ukraine/Ukraine report 2022/02_Data/01_Generation/CarbonTarget_v2_edit.xlsx').set_index('Year').iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idn_style_idx = False\n",
    "geo_cols = ['Region', 'Subregion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2110b80412284a859af3f3b0cfd09d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model runs:', options=('test',), value='test')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(soln_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`You can now run all with the above model runs being processed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import geopandas as gpd\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files(root_folder, file_type, id_text, subfolder=\"\", return_type=0):\n",
    "    \"\"\"Basic function to walk through folder and return all files of a certain type containing specific text in its name. \n",
    "    Can return either a list of full paths or two lkists odf directories and filenames seperately depending on the argument\n",
    "    return type =0/1\"\"\"\n",
    "    \n",
    "    searched_files = []\n",
    "    searched_file_paths = []\n",
    "    searched_files_fullpath = []\n",
    "    \n",
    "    folder = os.path.join(root_folder, subfolder)\n",
    "    \n",
    "    for dirpath, subdirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if (file.endswith(file_type)) & (id_text in file) :\n",
    "                searched_files_fullpath.append(os.path.join(os.path.normpath(dirpath), file))\n",
    "                searched_files.append(file)\n",
    "                searched_file_paths.append(os.path.normpath(dirpath))\n",
    "\n",
    "    if return_type==0:\n",
    "        return searched_files_fullpath\n",
    "    else:\n",
    "        return searched_file_paths, searched_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Import Julia stuff....\n",
    "\n",
    "Not sure on the difference between Main or Base and other variants of starting Julia/PyCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "JuliaNotFound",
     "evalue": "Julia executable `julia` cannot be found.\n\nIf you have installed Julia, make sure Julia executable is in the\nsystem PATH.  Alternatively, specify file path to the Julia executable\nusing `julia` keyword argument.\n\nIf you have not installed Julia, download Julia from\nhttps://julialang.org/downloads/ and install it.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------\u001b[0m",
      "\u001b[1;31mJuliaNotFound\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-0293c08126b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjulia\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mjulia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\site-packages\\julia\\tools.py\u001b[0m in \u001b[0;36minstall\u001b[1;34m(julia, color, python, quiet)\u001b[0m\n",
      "\u001b[1;31mJuliaNotFound\u001b[0m: Julia executable `julia` cannot be found.\n\nIf you have installed Julia, make sure Julia executable is in the\nsystem PATH.  Alternatively, specify file path to the Julia executable\nusing `julia` keyword argument.\n\nIf you have not installed Julia, download Julia from\nhttps://julialang.org/downloads/ and install it.\n"
     ]
    }
   ],
   "source": [
    "# ## If running for the first time!!\n",
    "\n",
    "import julia\n",
    "julia.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-aee0267fa881>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# from julia import Main\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjulia\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjulia\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJulia\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjulia\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJuliaError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_backward_compatible\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\site-packages\\julia\\core.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\site-packages\\julia\\core.py\u001b[0m in \u001b[0;36mjulia\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\site-packages\\julia\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, init_julia, jl_init_path, runtime, jl_runtime_path, debug, **julia_options)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\site-packages\\julia\\juliainfo.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(cls, julia, **popen_kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    798\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    801\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\geo_env\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1205\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1208\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "# from julia import Main\n",
    "from julia import Base\n",
    "from julia.api import Julia\n",
    "from julia import JuliaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl = Julia(compiled_modules=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "jl.using(\"H5PLEXOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add paths and get list of solution files and folders to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_folder = os.path.join(rootdir, soln_choice.value)\n",
    "\n",
    "### Just use the same name in the DaaProcessing section, to save changing 2 names\n",
    "save_dir = proj_dir + '05_DataProcessing/{}/'.format(soln_choice.value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y:/Modelling/Ukraine/2023_UKR_ST_Security/05_DataProcessing/test/'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_dirs, soln_files = get_files(soln_folder, file_type='.zip', id_text='Solution', return_type=1)\n",
    "existing_h5_files = get_files(soln_folder, file_type='.h5', id_text='Solution', return_type=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "soln_files = [s.replace('\\\\','/') for s in soln_files]\n",
    "h5_files = [s.replace('.zip', '.h5').replace('\\\\','/') for s in soln_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model IDN_2021_Validation Solution.zip']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soln_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Process using H5PLEXOS.jl\n",
    "Run code to check if h5 file exists already, and if not to run H5PLEXOS.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-da6b0867873f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoln_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mjl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cd(\\\"{}\\\")\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoln_dirs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoln_dirs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexisting_h5_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mjl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"process(\\\"{}\\\", \\\"{}\\\")\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoln_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'jl' is not defined"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(soln_files):\n",
    "    jl.eval(\"cd(\\\"{}\\\")\".format(soln_dirs[i].replace('\\\\','/')))\n",
    "    if (os.path.join(soln_dirs[i], h5_files[i])) not in existing_h5_files:\n",
    "        try:\n",
    "            jl.eval(\"process(\\\"{}\\\", \\\"{}\\\")\".format(soln_files[i], h5_files[i]))\n",
    "        except JuliaError:\n",
    "            print(\"Falied to process {} with {}\".format(soln_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cd(\"Z:/Ukraine/2022-3_next_step_modelling/04_SolutionFiles/20230622_UKR_VREPlus_FINAL\")'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"cd(\\\"{}\\\")\".format(soln_dirs[i].replace('\\\\','/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Process H5 solution files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h5plexos.query import PLEXOSSolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some parts of the solution index were added specifically for the Indonesia project.....\n",
    "There is a need to generalise some these parts//\n",
    "For example we could read in the cost data as oer technology from the Indices tab[['name', 'Object_type']+ geo_cols + ['CapacityCategory', 'Category' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Memory allocation errors occur due to large size of arrays, mainly due to string columns of dataframes. This way, we can limit this\n",
    "gen_inertia_idx = full_soln_idx[['name', 'InertiaLOW','InertiaHI']]\n",
    "idn_style_idx = True\n",
    "\n",
    "soln_idx = full_soln_idx[['name', 'Object_type']+ geo_cols + ['CapacityCategory', 'Category']]\n",
    "\n",
    "if idn_style_idx == True:\n",
    "    gen_cost_idx = full_soln_idx[['name', 'VOM', 'FOM', 'RampCost']]\n",
    "    line_info_idx = full_soln_idx[['name', 'nodeFrom', 'nodeTo', 'regFrom', 'regTo', 'islFrom', 'islTo']]\n",
    "    gen_addl_idx = full_soln_idx[['name', 'FlexCategory', 'CostCategory','CofiringCategory','Cofiring', 'CCUS', 'IPP']]\n",
    "else:\n",
    "    gen_cost_idx = full_soln_idx[['name']]\n",
    "    line_info_idx = full_soln_idx[['name']]\n",
    "    gen_addl_idx =  full_soln_idx[['name']]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Check if save_dir exists....otherwise make new directory\n",
    "if os.path.exists(save_dir) is False:\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get properties per object class in PLEXOS file\n",
    "This currently assumes that the properties will be the same across all files. \n",
    "\n",
    "There may also be a need to change things for different intervals (i.e. year, month, day) and MT/PASA if used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_full_path = get_files(soln_folder, file_type='.h5', id_text='Solution', return_type=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying H5PLEXOS v0.6.2 file\n"
     ]
    }
   ],
   "source": [
    "with PLEXOSSolution(h5_full_path[-1]) as db:\n",
    "    plexos_objects = list(db.h5file['data/ST/interval/'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in solution file objects and properties using the first solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying H5PLEXOS v0.6.2 file\n"
     ]
    }
   ],
   "source": [
    "### This may be housed under a 'with'\n",
    "with PLEXOSSolution(h5_full_path[-1]) as db:\n",
    "    plexos_objects = list(db.h5file['data/ST/interval/'].keys())\n",
    "    plexos_props = {}\n",
    "    plexos_props_yr = {}\n",
    "    for o in plexos_objects:\n",
    "        plexos_props[o] = list(db.h5file['data/ST/interval/{}/'.format(o)].keys())\n",
    "        plexos_props_yr[o] = list(db.h5file['data/ST/year/{}/'.format(o)].keys())\n",
    "        \n",
    "    ### As Zone (or sometimes region) usually has only one object, this can be used to get time_idx\n",
    "    time_idx = db.region(\"Load\").reset_index().timestamp.drop_duplicates()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties to filter for interval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As time-series data can be quite large, filter only certain data\n",
    "filter_props = {}\n",
    "filter_out_objs = ['constraints', 'contingencyreserves_regions', 'decisionvariables', 'storages', 'timeslices', 'variables']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['generators'] = ['Available Capacity', 'Generation', 'Installed Capacity','Min Energy Violation', 'Units',\n",
    "                             'Units Generating','Units Out', 'Units Started', 'Forced Outage', 'Maintenance', 'Firm Capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['batteries'] = ['Age', 'Energy', 'Generation', 'Hours Charging', 'Hours Discharging', 'Hours Idle', 'Installed Capacity', 'Load', 'Losses', 'Lower Reserve', 'Net Generation', 'Raise Reserve',\n",
    " 'Regulation Lower Reserve', 'Regulation Raise Reserve', 'SoC', 'Generation Capacity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['regions'] = ['Capacity Reserve Margin', 'Capacity Reserves', 'Customer Load', 'Dump Energy', 'Forced Outage', \n",
    "                'Generation', 'Generation Capacity', 'Load', 'Maintenance', 'Native Load', 'Pump Load', 'Battery Load',\n",
    "                'Transmission Losses', 'Unserved Energy', 'Unserved Energy Hours', 'Price', 'Shadow Price', 'SRMC'] # 'Exports', 'Imports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['nodes'] = ['Customer Load', 'Exports','Generation', 'Generation Capacity', \n",
    "              'Imports', 'Load', 'Min Load', 'Native Load', 'Peak Load', 'Price', 'Pump Load', 'Battery Load', 'Unserved Energy',  'Price', 'Shadow Price', 'SRMC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['lines'] = ['Export Limit', 'Flow', 'Import Limit', 'Loading',  'Loss',  'Units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## differ by emission_gens or plain emissions\n",
    "# filter_props['emissions_generators'] = ['Cost', 'Intensity', 'Generator Production', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['emissions'] = ['Intensity', 'Price', 'Production', 'Shadow Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['fuels'] = ['Cost', 'Offtake', 'Price', 'Time-weighted Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_props['reserves'] = ['Shortage', 'Risk']\n",
    "filter_props['reserves_generators'] = ['Available Response', 'Cleared Offer Cost', 'Cleared Offer Price',  'Non-spinning Reserve Provision',\n",
    "                 'Provision']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analysis and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Errors may arise from using input as output. Check back if weird behavior\n",
    "\n",
    "def add_df_column(df, column_name, value, reset_index=True): \n",
    "    \n",
    "    if type(df) == pd.Series:\n",
    "        out_df=pd.DataFrame(df)\n",
    "    else:\n",
    "        out_df = df.copy()\n",
    "        \n",
    "    if column_name in out_df.columns:\n",
    "        print('Updating /\"{}/\" column with new values')\n",
    "        out_df.loc[:,column_name] = value\n",
    "    else:\n",
    "        out_df[column_name] = value\n",
    "        \n",
    "    if reset_index:\n",
    "        out_df = out_df.reset_index().rename(columns={0:'value'})\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cycle through all solution files in folder and all specified properties\n",
    "1. Note that this should be kept to the minimum for speed purposes.\n",
    "2. Additional performance benefits may be obtained from seperate querying of interval/yearly data (currently all done in one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNE\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "\n",
    "    del gen_df, res_gen_df, em_gen_df, em_df, res_df, reg_df, node_df, line_df, fuel_df, zone_df\n",
    "    del gen_yr_df,  em_gen_yr_df, em_yr_df, res_yr_df, reg_yr_df, node_yr_df, line_yr_df, fuel_yr_df, zone_yr_df\n",
    "except:\n",
    "    print('DNE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IDN_2021_Validation...\n",
      "Querying H5PLEXOS v0.6.2 file\n"
     ]
    }
   ],
   "source": [
    "obj_df_dict = {}\n",
    "\n",
    "for f in h5_full_path:\n",
    "    \n",
    "    ### Any spaces in the file will break it\n",
    "    soln_name = f.split('\\\\')[-1].split('Model ')[-1].split(' Solution.h5')[0]\n",
    "\n",
    "    print(\"Processing {}...\".format(soln_name))\n",
    "    \n",
    "    with PLEXOSSolution(f) as db:\n",
    "        \n",
    "        plexos_objs = [ p for p in db.h5file['data/ST/interval/'].keys() if p not in filter_out_objs] #['variables']#\n",
    "        plexos_props = {}\n",
    "        for o in plexos_objs:\n",
    "            plexos_props[o] = list(db.h5file['data/ST/interval/{}/'.format(o)].keys())\n",
    "        \n",
    "        for o in plexos_objs:\n",
    "            \n",
    "            ### New method is to populate as you run through the PLEXOS dbs\n",
    "            ### If its the first loop, create the empty dataframe to concatanate\n",
    "            if o not in obj_df_dict.keys():\n",
    "                obj_df_dict[o] = pd.DataFrame(None)\n",
    "            \n",
    "            ### Filter properties for time-series data\n",
    "            ### If filter props is not defined, all properties are used\n",
    "            try:\n",
    "                props = [ p for p in plexos_props[o] if p in filter_props[o]]\n",
    "            except KeyError:\n",
    "                props = plexos_props[o]\n",
    "                \n",
    "            \n",
    "            for prop in props:          \n",
    "                \n",
    "                ### Relations (i.e. membership-related props) have underscores in them\n",
    "                if '_' not in o:\n",
    "                    ## object class is queried without the 's' at the end of its name (this may be a bug that is correccted in future versions)\n",
    "                    o_key = o[:-1]\n",
    "                    obj_df_dict[o] = pd.concat(\n",
    "                    [obj_df_dict[o], \n",
    "                     add_df_column(\n",
    "                         df = db.query_object_property(\n",
    "                             object_class=o_key, prop = prop, timescale=\"interval\", phase=\"ST\").reset_index(),\n",
    "                         column_name='model',\n",
    "                         value=soln_name)], axis=0)\n",
    "                else:\n",
    "                    obj_df_dict[o] = pd.concat(\n",
    "                    [obj_df_dict[o], \n",
    "                     add_df_column(\n",
    "                         df = db.query_relation_property(\n",
    "                         relation=o, prop = prop, timescale=\"interval\", phase=\"ST\").reset_index(),\n",
    "                         column_name='model',\n",
    "                         value=soln_name)], axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip unnecessary columns (e.g. bands), rename '0' column to 'Value'  & add SolutionIndex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IDN_2021_Validation...\n",
      "Querying H5PLEXOS v0.6.2 file\n"
     ]
    }
   ],
   "source": [
    "obj_df_yr_dict = {}\n",
    "\n",
    "for f in h5_full_path:\n",
    "    \n",
    "    ### Any spaces in the file will break it\n",
    "    soln_name = f.split('\\\\')[-1].split('Model ')[-1].split(' Solution.h5')[0]\n",
    "\n",
    "    print(\"Processing {}...\".format(soln_name))\n",
    "    \n",
    "    with PLEXOSSolution(f) as db:\n",
    "        \n",
    "        plexos_objs = [ p for p in db.h5file['data/ST/interval/'].keys() if p not in filter_out_objs] #['variables']#\n",
    "        plexos_props = {}\n",
    "        for o in plexos_objs:\n",
    "            plexos_props[o] = list(db.h5file['data/ST/year/{}/'.format(o)].keys())\n",
    "        \n",
    "        for o in plexos_objs:\n",
    "            \n",
    "            ### New method is to populate as you run through the PLEXOS dbs\n",
    "            ### If its the first loop, create the empty dataframe to concatanate\n",
    "            if o not in obj_df_yr_dict.keys():\n",
    "                obj_df_yr_dict[o] = pd.DataFrame(None)\n",
    "            \n",
    "            ### No need to filter annual\n",
    "            props = plexos_props[o]\n",
    "            \n",
    "            for prop in props:          \n",
    "                \n",
    "                ### Relations (i.e. membership-related props) have underscores in them\n",
    "                if '_' not in o:\n",
    "                    ## object class is queried without the 's' at the end of its name (this may be a bug that is correccted in future versions)\n",
    "                    o_key = o[:-1]\n",
    "                    obj_df_yr_dict[o] = pd.concat(\n",
    "                    [obj_df_yr_dict[o], \n",
    "                     add_df_column(\n",
    "                         df = db.query_object_property(\n",
    "                             object_class=o_key, prop = prop, timescale=\"year\", phase=\"ST\").reset_index(),\n",
    "                         column_name='model',\n",
    "                         value=soln_name)], axis=0)\n",
    "                else:\n",
    "                    obj_df_yr_dict[o] = pd.concat(\n",
    "                    [obj_df_yr_dict[o], \n",
    "                     add_df_column(\n",
    "                         df = db.query_relation_property(\n",
    "                         relation=o, prop = prop, timescale=\"year\", phase=\"ST\").reset_index(),\n",
    "                         column_name='model',\n",
    "                         value=soln_name)], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_df(df,  soln_idx, common_yr = None, out_type='direct',pretty_model_names={}):\n",
    "    \n",
    "    ### Output can relative type (i.e. emissions from generators) or direct type (i.e. just emissions)\n",
    "    if out_type=='rel':\n",
    "        df = df.rename(columns={0:'value'})[[ 'parent', 'child', 'property', 'timestamp', 'model', 'value']]\n",
    "        ## Add soln idx\n",
    "        df = pd.merge(df,soln_idx, left_on='child', right_on='name')\n",
    "    else:\n",
    "        df = df.rename(columns={0:'value'})[['name', 'property', 'timestamp', 'model', 'value']]\n",
    "        ## Add soln idx\n",
    "        df = pd.merge(df,soln_idx, left_on='name', right_on='name')\n",
    "    \n",
    "    if common_yr:\n",
    "    ### Add common year date .... will in future need to add line for removing leap days\n",
    "        df.loc[:,'timestamp'] = pd.to_datetime({'Year':[common_yr]*len(df), 'Month':df.timestamp.dt.month.values, 'Day': df.timestamp.dt.day.values,\n",
    "                                                            'Hour':df.timestamp.dt.hour.values, 'Minute':df.timestamp.dt.minute.values})\n",
    "    else:\n",
    "        df.loc[:,'timestamp'] = df.timestamp\n",
    "\n",
    "    \n",
    "    try:\n",
    "        df.loc[:,'model'] = df.model.apply(lambda x: pretty_model_names[x] if x in pretty_model_names.keys() else x.split('Model ')[-1].split(' Solution.h5')[0])\n",
    "    except:\n",
    "        print('Error re-defining model names')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip unnecessary columns (e.g. bands), rename '0' column to 'Value'  & add SolutionIndex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No leap years....but this could become a factor soon\n",
    "model_yrs = obj_df_dict['regions'].groupby(['model']).first().timestamp.dt.year.values\n",
    "if len(model_yrs) > 1:\n",
    "    common_yr = model_yrs[-1]\n",
    "else:\n",
    "    common_yr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_df(df,  soln_idx, common_yr = None, out_type='direct',pretty_model_names={}):\n",
    "    \n",
    "    ### Output can relative type (i.e. emissions from generators) or direct type (i.e. just emissions)\n",
    "    if out_type=='rel':\n",
    "        df = df.rename(columns={0:'value'})[[ 'parent', 'child', 'property', 'timestamp', 'model', 'value']]\n",
    "        ## Add soln idx\n",
    "        df = pd.merge(df,soln_idx, left_on='child', right_on='name')\n",
    "    else:\n",
    "        df = df.rename(columns={0:'value'})[['name', 'property', 'timestamp', 'model', 'value']]\n",
    "        ## Add soln idx\n",
    "        df = pd.merge(df,soln_idx, left_on='name', right_on='name')\n",
    "    \n",
    "    if common_yr:\n",
    "    ### Add common year date .... will in future need to add line for removing leap days\n",
    "        df.loc[:,'timestamp'] = pd.to_datetime({'Year':[common_yr]*len(df), 'Month':df.timestamp.dt.month.values, 'Day': df.timestamp.dt.day.values,\n",
    "                                                            'Hour':df.timestamp.dt.hour.values, 'Minute':df.timestamp.dt.minute.values})\n",
    "    else:\n",
    "        df.loc[:,'timestamp'] = df.timestamp\n",
    "\n",
    "    \n",
    "    try:\n",
    "        df.loc[:,'model'] = df.model.apply(lambda x: pretty_model_names[x] if x in pretty_model_names.keys() else x.split('Model ')[-1].split(' Solution.h5')[0])\n",
    "    except:\n",
    "        print('Error re-defining model names')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prettify names\n",
    "\n",
    "pretty_model_names = { \n",
    "#         'UKR22_2021_Validation': '2021 Validation',\n",
    "#         'UKR22_2025_Base': '2025 Base',\n",
    "#         'UKR22_2030_Base': '2030 Base',\n",
    "#         'UKR22_2037_Base': '2037 Base',\n",
    "#         'UKR22_2040_Base': '2040 Base',\n",
    "    \n",
    "    \n",
    "    \n",
    "#         'UKR22_2037_EVFlex': '2037 (Smart Charging)',\n",
    "#         'UKR22_2025_NoTOP': '2025 (Contractual Flex)',\n",
    "#         'UKR22_2030_NoTOP': '2030 (Contractual Flex)',\n",
    "#         'UKR22_2030plus_NoTOP': '2030 VRE Plus (Contractual Flex)',\n",
    "#         'UKR22_2037_EVFlex_EVRampPen': '2037 (Smart Charging, limited ramping)',\n",
    "#         'UKR22_2037_NoTOP': '2037 (Contractual Flex)',\n",
    "#         'UKR22_2037plus_NoTOP': '2037 VRE Plus',\n",
    "#         'UKR22_2037plus_NoTOP_EVFlex': '2037 VRE Plus (Smart Charging)',\n",
    "#         'UKR22_2037plus_NoTOP_EVFlex_EVRampPen': '2037 (ContrFlex, Smart Charging, limited ramping)',\n",
    "#         'UKR22_2030plus_NoTOP_Batt_CO2cap': '2030 (Battery & CO2 price)',\n",
    "#         'UKR22_2037plus_NoTOP_EVFlex_Batt': '2037 (Smart Charging & Battery)',\n",
    "#         'UKR22_2037plus_NoTOP_EVFlex_Batt_CO2cap': '2037 (Smart Charging, Battery & CO2 price)'\n",
    "    \n",
    "    \n",
    "        'UKR22_2021_Validation' : '2021 Validation',\n",
    "        'UKR22_2025_Base' : '2025 Base',\n",
    "    \n",
    "    \n",
    "        'UKR22_2030_Base' : '2030 Base',\n",
    "        'UKR22_2030plus': '2030 VRE Plus',\n",
    "        'UKR22_2030plus_CO2p': '2030 VRE Plus + CO2p',\n",
    "\n",
    "        'UKR22_2030plus_PPflex': '2030 VRE Plus + PPFlex',\n",
    "        'UKR22_2030plus_Batt': '2030 VRE Plus + Sto',\n",
    "        'UKR22_2030plus_Batt_PPflex': '2030 VRE Plus + PPFlex + Sto',\n",
    "        'UKR22_2030plus_Batt_NoTOP_PPflex': '2030 VRE Plus + PPFlex + Sto + ContrFlex',\n",
    "\n",
    "\n",
    "        'UKR22_2037_Base' : '2037 Base',\n",
    "        'UKR22_2037plus': '2037 VRE Plus',\n",
    "        'UKR22_2037plus_CO2p': '2037 VRE Plus + CO2p',\n",
    "        'UKR22_2037plus_Batt': '2037 VRE Plus + Sto',\n",
    "        'UKR22_2037plus_Batt_CO2p': '2037 VRE Plus + Sto + CO2p',\n",
    "        'UKR22_2037plus_EVFlex_Batt_PPflex': '2037 VRE Plus + PPFlex + Sto + EV',\n",
    "        'UKR22_2037plus_EVFlex_Batt_PPflex_CO2p': '2037 VRE Plus + PPFlex + Sto + EV + CO2p',\n",
    "\n",
    "        'UKR22_2037plus_PPflex': '2037 VRE Plus + PPFlex',\n",
    "        'UKR22_2037plus_EVFlex': '2037 VRE Plus + EV',\n",
    "        'UKR22_2037plus_EVFlex_Batt_PPflex': '2037 VRE Plus + PPFlex + Sto + EV',\n",
    "        'UKR22_2037plus_EVFlex_Batt_NoTOP_PPflex': '2037 VRE Plus + PPFlex + Sto + EV + ContrFlex',\n",
    "      \n",
    " \n",
    "        'UKR22_2030plus_Batt_NoTOP': '2030 VRE Plus + Sto + ContrFlex',\n",
    "        'UKR22_2037plus_EVFlex_Batt': '2037 VRE Plus + Sto + EV',\n",
    "        'UKR22_2037plus_EVFlex_Batt_NoTOP': '2037 VRE Plus + Sto + ContrFlex',\n",
    "        \"UKR22_2037plus_EVFlex25\": '2037 VRE Plus + EV25',\n",
    "        \"UKR22_2037plus_EVFlex50\": '2037 VRE Plus + EV50',\n",
    "        \"UKR22_2037plus_Batt25\": '2037 VRE Plus + Sto25',\n",
    "        \"UKR22_2037plus_Batt50\": '2037 VRE Plus + Sto50',\n",
    "        \"UKR22_2037plus_EVFlex25_Batt_NoTOP_PPflex\": '2037 VRE Plus + Sto + EV25 + ContrFlex',\n",
    "        \"UKR22_2037plus_EVFlex25_Batt_PPflex\": '2037 VRE Plus + Sto + EV25',\n",
    "        \"UKR22_2037plus_EVFlex50_Batt_NoTOP_PPflex\": '2037 VRE Plus + Sto + EV50 + ContrFlex',\n",
    "        \"UKR22_2037plus_EVFlex50_Batt_PPflex\": '2037 VRE Plus + Sto + EV50',\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out regions with no generation or load!\n",
    "With the expansion of the model for NZE, there are now many regions which have zero load. This step serves to save memory and plot space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_geo_cols = [geo_cols[0], 'Zone',  geo_cols[1], 'Node']\n",
    "geo_cols = [c for c in soln_idx.columns if c in possible_geo_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_reg_by_gen = enrich_df(obj_df_yr_dict['generators'], soln_idx=soln_idx[soln_idx.Object_type.str.lower() == 'generator'].drop(columns='Object_type'), pretty_model_names=pretty_model_names)\n",
    "filter_reg_by_load =  enrich_df(obj_df_yr_dict['nodes'], soln_idx=soln_idx[soln_idx.Object_type.str.lower() == 'node'].drop(columns='Object_type'), pretty_model_names=pretty_model_names)\n",
    "### This is subregion in Indonesia. -1 is where the finest geo resolution would/should be found\n",
    "filter_col = geo_cols[-1]\n",
    "\n",
    "\n",
    "### Filter out nodes that have zero load\n",
    "filter_reg_by_load = filter_reg_by_load[(filter_reg_by_load.property == 'Load')&(filter_reg_by_load.value != 0)]\n",
    "\n",
    "filter_reg_by_gen = filter_reg_by_gen[geo_cols[-1]].unique()\n",
    "filter_reg_by_load = filter_reg_by_load[geo_cols[-1]].unique()\n",
    "\n",
    "####\n",
    "\n",
    "filter_regs = list(set([reg for reg in filter_reg_by_gen] + [reg for reg in filter_reg_by_load] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in obj_df_dict.keys():\n",
    "\n",
    "    o_key = o.replace('ies','ys')\n",
    "        \n",
    "    if '_' in o_key:\n",
    "    ### Memebrship properties can used the generator/battery part which is the second part\n",
    "    ### Also o_key is for singular form, hence drops s\n",
    "        o_key = o_key.split('_')[-1][:-1]\n",
    "    else:\n",
    "        o_key = o_key[:-1]        \n",
    "    \n",
    "    ### Remove unnecessary columns, so object_type can be removed for the o_idx\n",
    "    o_idx = soln_idx[soln_idx.Object_type.str.lower().str.replace(' ','')== o_key].drop(columns='Object_type')\n",
    "    if len(o_idx) > 0:\n",
    "        if '_' not in o:\n",
    "            obj_df_dict[o] = enrich_df(obj_df_dict[o], soln_idx=o_idx, common_yr=common_yr, out_type='direct', pretty_model_names=pretty_model_names)\n",
    "        else:\n",
    "            obj_df_dict[o] = enrich_df(obj_df_dict[o], soln_idx=o_idx, common_yr=common_yr, out_type='rel', pretty_model_names=pretty_model_names)\n",
    "        \n",
    "        ### Filter out regions with no generation nor load\n",
    "        if (o == 'nodes')|(o == 'regions'):\n",
    "            obj_df_dict[o] = obj_df_dict[o][obj_df_dict[o][filter_col].isin(filter_regs)]\n",
    "\n",
    "for o in obj_df_yr_dict.keys():\n",
    "    \n",
    "    o_key = o.replace('ies','ys')\n",
    "        \n",
    "    if '_' in o_key:\n",
    "    ### Memebrship properties can used the generator/battery part which is the second part\n",
    "    ### Also o_key is for singular form, hence drops s\n",
    "        o_key = o_key.split('_')[-1][:-1]\n",
    "    else:\n",
    "        o_key = o_key[:-1]\n",
    "\n",
    "    ### No need to filter out solnb_idx for the annual data as the size wont be an issue\n",
    "    o_idx = full_soln_idx[full_soln_idx.Object_type.str.lower().str.replace(' ','')== o_key].drop(columns='Object_type')\n",
    "    if len(o_idx) > 0:\n",
    "        if '_' not in o:\n",
    "            obj_df_yr_dict[o] = enrich_df(obj_df_yr_dict[o], soln_idx=o_idx, common_yr=common_yr, out_type='direct', pretty_model_names=pretty_model_names)\n",
    "        else:\n",
    "            obj_df_yr_dict[o] = enrich_df(obj_df_yr_dict[o], soln_idx=o_idx, common_yr=common_yr, out_type='rel', pretty_model_names=pretty_model_names)\n",
    "        \n",
    "        ### Filter out regions with no generation nor load\n",
    "        if (o == 'nodes')|(o == 'regions'):\n",
    "            obj_df_yr_dict[o] = obj_df_yr_dict[o][obj_df_yr_dict[o][filter_col].isin(filter_regs)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here assign to more traditional variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o_idx = soln_idx[soln_idx.Object_type.str.lower() == 'line']\n",
    "# line_exporting_reg = geo_cols[0]\n",
    "# line_importing_reg = geo_cols[1]\n",
    "# line_df = line_df[['category', 'name', 'property', 'timestamp', 'model', 'value']]\n",
    "# line_yr_df = line_yr_df[['category', 'name', 'property', 'timestamp', 'model', 'value']]\n",
    "\n",
    "# line_df = enrich_df(line_df, soln_idx=o_idx, common_yr=common_yr, out_type='direct', pretty_model_names=pretty_model_names).rename(columns={line_exporting_reg:'regFrom', line_importing_reg:'regTo'})\n",
    "# line_yr_df = enrich_df(line_yr_df, soln_idx=o_idx, common_yr=common_yr, out_type='direct', pretty_model_names=pretty_model_names).rename(columns={line_exporting_reg:'regFrom', line_importing_reg:'regTo'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = obj_df_yr_dict['batteries']\n",
    "# x[(x.name.str.contains('Battery'))&(x.property=='Age')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'batteries'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-661280f1840e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobj_df_yr_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batteries'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_df_yr_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batteries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Installed Capacity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Storage Capacity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Generation Capacity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Installed Capacity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mobj_df_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batteries'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_df_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batteries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Installed Capacity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Storage Capacity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Generation Capacity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Installed Capacity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'batteries'"
     ]
    }
   ],
   "source": [
    "obj_df_yr_dict['batteries'] = obj_df_yr_dict['batteries'].replace('Installed Capacity', 'Storage Capacity').replace('Generation Capacity', 'Installed Capacity')\n",
    "obj_df_dict['batteries'] = obj_df_dict['batteries'].replace('Installed Capacity', 'Storage Capacity').replace('Generation Capacity', 'Installed Capacity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_yr_df = obj_df_yr_dict['generators']\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    gen_yr_df = pd.concat([gen_yr_df, obj_df_yr_dict['batteries']], axis=0)\n",
    "except KeyError:\n",
    "    print(\"No batteries for current scenarios\")\n",
    "\n",
    "try:\n",
    "    purch_yr_df = obj_df_yr_dict['purchasers']\n",
    "except KeyError:\n",
    "    purch_yr_df = pd.DataFrame(None)\n",
    "    \n",
    "try:\n",
    "    var_yr_df = obj_df_yr_dict['variables']\n",
    "except KeyError:\n",
    "    var_yr_df = pd.DataFrame(None)\n",
    "    \n",
    "try:\n",
    "    fuelcontract_yr_df = obj_df_yr_dict['fuelcontracts']\n",
    "except KeyError:\n",
    "    fuelcontract_yr_df = pd.DataFrame(None)\n",
    "try:\n",
    "    em_fuel_yr_df = obj_df_yr_dict['emissions_fuels']\n",
    "except KeyError:\n",
    "    em_fuel_yr_df = pd.DataFrame(None)\n",
    "\n",
    "try:\n",
    "    line_yr_df = obj_df_yr_dict['lines']\n",
    "except KeyError:\n",
    "    line_yr_df = pd.DataFrame(None)\n",
    "\n",
    "    \n",
    "    \n",
    "reg_yr_df = obj_df_yr_dict['regions']\n",
    "node_yr_df = obj_df_yr_dict['nodes']\n",
    "em_yr_df = obj_df_yr_dict['emissions']\n",
    "fuel_yr_df = obj_df_yr_dict['fuels']\n",
    "\n",
    "try:\n",
    "    res_yr_df = obj_df_yr_dict['reserves']\n",
    "except KeyError:\n",
    "    res_yr_df = pd.DataFrame(None)\n",
    "    \n",
    "zone_yr_df =obj_df_yr_dict['zones']\n",
    "fuelcontract_yr_df = obj_df_yr_dict['fuelcontracts']\n",
    "\n",
    "\n",
    "### Emissions generators is missing....maybe good to have some fail safe so all downstream stuff can work, regardless. i.e. placeholder empty data\n",
    "em_gen_yr_df =obj_df_yr_dict['emissions_generators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_df =obj_df_dict['generators']\n",
    "reg_df = obj_df_dict['regions']\n",
    "node_df = obj_df_dict['nodes']\n",
    "em_df = obj_df_dict['emissions']\n",
    "fuel_df = obj_df_dict['fuels']\n",
    "\n",
    "zone_df =obj_df_dict['zones']\n",
    "\n",
    "try:\n",
    "    line_df = pd.merge(obj_df_dict['lines'], line_info_idx, on='name')\n",
    "except KeyError:\n",
    "    line_df = pd.DataFrame(None)\n",
    "    \n",
    "try:\n",
    "    purch_df = obj_df_dict['purchasers']\n",
    "except KeyError:\n",
    "    purch_df = pd.DataFrame(None)\n",
    "    \n",
    "    \n",
    "try:\n",
    "    var_df = obj_df_dict['variables']\n",
    "except KeyError:\n",
    "    var_df = pd.DataFrame(None)  \n",
    "    \n",
    "try:\n",
    "    res_df = obj_df_dict['reserves']\n",
    "except KeyError:\n",
    "    res_df = pd.DataFrame(None)\n",
    "\n",
    "## Note different structure for objects derived from memberships\n",
    "em_gen_df = obj_df_dict['emissions_generators']\n",
    "\n",
    "try:\n",
    "    res_gen_df = obj_df_dict['reserves_generators']\n",
    "except:\n",
    "    res_gen_df = pd.DataFrame(None)\n",
    "    \n",
    "try:\n",
    "    \n",
    "    gen_df = pd.concat([gen_df, obj_df_dict['batteries']], axis=0)\n",
    "except KeyError:\n",
    "    print(\"No batteries objects\")\n",
    "    \n",
    "try:\n",
    "    res_gen_df = pd.concat([res_gen_df, obj_df_dict['reserves_batteries']], axis=0)\n",
    "except KeyError:\n",
    "    print(\"No reserve batteries objects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For WEO_tech simpl. probably should add something to soln_idx\n",
    "# gen_yr_df.loc[:,'WEO_Tech_simpl'] = gen_yr_df['WEO tech'].apply(lambda x: x.replace('NEW ','').replace(' 1','').replace(' 2','').replace(' 3','').replace(' 4','').replace(' 5','')   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prevent double dosing in memory\n",
    "\n",
    "del obj_df_dict\n",
    "# del obj_df_yr_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip unnecessary columns (e.g. bands), rename '0' column to 'Value'  & add SolutionIndex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Drop addl columns\n",
    "# gen_yr_df = gen_yr_df[[ 'name', 'property', 'timestamp', 'model', 'value',\n",
    "#        geo_cols[0], geo_cols[1], geo_cols[0], 'CapacityCategory',\n",
    "#        'Category',]]\n",
    "# gen_df = gen_df[['name', 'property', 'timestamp', 'model', 'value',\n",
    "#        geo_cols[0], geo_cols[1], geo_cols[0], 'CapacityCategory',\n",
    "#        'Category',]]\n",
    "# res_gen_df = res_gen_df[['parent', 'child', 'property', 'timestamp', 'model', 'value',\n",
    "#        geo_cols[0], geo_cols[1], geo_cols[0], 'CapacityCategory',\n",
    "#        'Category',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Cofiring change here! \n",
    "# ### Due to memory allocation errors, additional columns from soln_idx are used and then discarded\n",
    "# cofiring_scens = [c for c in pretty_model_names.values() if ('2030' in c)| (c == '2025 Base') | (c == '2025 Enforced Cofiring')]\n",
    "\n",
    "\n",
    "# # Add additional columns for interval dataframes (these are not added at first due to memory issues with the big size of the df)\n",
    "# gen_df = pd.merge(gen_df, gen_addl_idx[['name', 'Cofiring', 'CofiringCategory']], on='name', how='left')\n",
    "# res_gen_df = pd.merge(res_gen_df, gen_addl_idx[['name', 'Cofiring', 'CofiringCategory']], left_on='child', right_on='name',how='left')\n",
    "\n",
    "\n",
    "# ### Add category\n",
    "# gen_yr_df.loc[(gen_yr_df.Cofiring=='Y')&(gen_yr_df.model.isin(cofiring_scens)), 'Category'] = gen_yr_df.loc[(gen_yr_df.Cofiring=='Y')&(gen_yr_df.model.isin(cofiring_scens)), 'CofiringCategory']\n",
    "# gen_df.loc[(gen_df.Cofiring=='Y')&(gen_df.model.isin(cofiring_scens)), 'Category'] = gen_df.loc[(gen_df.Cofiring=='Y')&(gen_df.model.isin(cofiring_scens)), 'CofiringCategory']\n",
    "# res_gen_df.loc[(res_gen_df.Cofiring=='Y')&(res_gen_df.model.isin(cofiring_scens)), 'Category'] = res_gen_df.loc[(res_gen_df.Cofiring=='Y')&(res_gen_df.model.isin(cofiring_scens)), 'CofiringCategory']\n",
    "\n",
    "# ### And capacity category\n",
    "# gen_yr_df.loc[(gen_yr_df.Cofiring=='Y')&(gen_yr_df.model.isin(cofiring_scens)), 'CapacityCategory'] = gen_yr_df.loc[(gen_yr_df.Cofiring=='Y')&(gen_yr_df.model.isin(cofiring_scens)), 'CofiringCategory']\n",
    "# gen_df.loc[(gen_df.Cofiring=='Y')&(~gen_df.model.isin(cofiring_scens)), 'CapacityCategory'] = gen_df.loc[(gen_df.Cofiring=='Y')&(~gen_df.model.isin(cofiring_scens)), 'CofiringCategory']\n",
    "# res_gen_df.loc[(res_gen_df.Cofiring=='Y')&(res_gen_df.model.isin(cofiring_scens)), 'CapacityCategory'] = res_gen_df.loc[(res_gen_df.Cofiring=='Y')&(res_gen_df.model.isin(cofiring_scens)), 'CofiringCategory']\n",
    "\n",
    "# ### Drop addl columns for interval df\n",
    "# gen_df = gen_df.drop(columns=['Cofiring', 'CofiringCategory'])\n",
    "# res_gen_df = res_gen_df.drop(columns=['Cofiring', 'CofiringCategory'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do we want to reproduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a) Annual load by region\n",
    "   b) Load time-series (net load?)\n",
    "2. Unserved energy by region\n",
    "3. Operating costs and variants (e.g. ToP constraints as fuel costs, incl. penalty costs, etc.) by property\n",
    "4. a) Generation by technology/region\n",
    "   b) Generation by plant\n",
    "5. Unit starts by technology\n",
    "6. a) Gen max by region/technology\n",
    "   b) Sum of regional gen max by technology\n",
    "7. Transmission losses\n",
    "8. VRE availability (absolute & normalised) by tech/combined, VRE gen vs the rest (VRE share)\n",
    "9. VRE undispatched by tech/region or summed by tech\n",
    "10. a) Line flows per line/interface\n",
    "    b) Line capacity per interface (import/export)\n",
    "    c) Line flow time-series per interface (as % of capacity?)\n",
    "11. a) CFs per technology/region\n",
    "    b) CFs per tech only\n",
    "12. Installed cap per region/technology\n",
    "\n",
    ".... additional may include ramping, emissions, heat rate by generator, SRMC by generator, fuel offtake by generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic information for sharing across calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_periods = len(time_idx)\n",
    "nr_days = len(time_idx.dt.date.drop_duplicates())\n",
    "daily_periods = interval_periods/nr_days\n",
    "hour_corr = 24/daily_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In past models, nodes were effectively regions while there was only a single geo_cols[0] modelled. \n",
    "## In later models, Zones are this single entity, while Regions represent control regions and \n",
    "## nodes are smaller subentities in those regions (e.g. provinces)\n",
    "## node_regs is therefore a flag whether regions should be based on Nodes or Regions\n",
    "\n",
    "node_regs = False\n",
    "model_regs = list(gen_yr_df.Region.unique())\n",
    "\n",
    "if node_regs:\n",
    "    \n",
    "    load_regs = list(node_yr_df[(node_yr_df.property == 'Load')&(node_yr_df.value != 0)].name.unique())\n",
    "else:\n",
    "    load_regs = list(reg_yr_df[(reg_yr_df.property == 'Load')&(reg_yr_df.value != 0)].name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = list(np.sort(reg_df.model.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Annual summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_sum = os.path.join(save_dir,'summary_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(save_dir_sum) is False:\n",
    "    os.mkdir(save_dir_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cap_actuals = pd.read_excel(gen_params_path, sheet_name='AllGenerators', usecols='A:AX', engine='openpyxl', skiprows=2)\n",
    "#\n",
    "gen_cap_actuals.loc[:,'Historical (2021)'] = gen_cap_actuals.MaxCap_2021*gen_cap_actuals.Units_2021\n",
    "gen_cap_actuals.loc[:,'Input_2025'] = gen_cap_actuals.MaxCap_2025*gen_cap_actuals.Units_2025\n",
    "gen_cap_actuals.loc[:,'Input_2030'] = gen_cap_actuals.MaxCap_2030*gen_cap_actuals.Units_2030\n",
    "gen_cap_actuals.loc[:,'Input_2037'] = gen_cap_actuals.MaxCap_2037*gen_cap_actuals.Units_2037\n",
    "\n",
    "gen_cap_actuals.loc[:,'Input2030_NNTp2025'] = gen_cap_actuals.Input_2030\n",
    "gen_cap_actuals.loc[:,'Input2037_NNTp2025'] = gen_cap_actuals.Input_2037\n",
    "#\n",
    "thermal_Tech = ['Gas', 'Coal', 'Oil']\n",
    "gen_cap_actuals.loc[(gen_cap_actuals.Tech_simple.isin(thermal_Tech))&(gen_cap_actuals.BuiltP2025 == True),'Input2030_NNTp2025'] = 0\n",
    "gen_cap_actuals.loc[(gen_cap_actuals.Tech_simple.isin(thermal_Tech))&(gen_cap_actuals.BuiltP2025 == True),'Input2037_NNTp2025'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cap_actuals_tech = gen_cap_actuals.fillna(0).groupby([ 'ValidationTech']).sum().rename_axis('model', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_actuals_tech = pd.read_excel(gen_params_path, sheet_name='Validation2021', usecols='A:B', engine='openpyxl')\n",
    "gen_actuals_tech = gen_actuals_tech.rename(columns={'Tech':'ValidationTech', 'GWh': 'Generation' })\n",
    "gen_actuals_tech.loc[:,'model'] = 'Historical (2021)'\n",
    "gen_actuals_tech = gen_actuals_tech.groupby(['model', 'ValidationTech'])['Generation'].sum()\n",
    "#\n",
    "\n",
    "gen_cap_actuals = pd.read_excel(gen_params_path, sheet_name='AllGenerators', usecols='A:AX', engine='openpyxl', skiprows=2)\n",
    "\n",
    "gen_cap_actuals.loc[:,'Input_2021'] = gen_cap_actuals.MaxCap_2021*gen_cap_actuals.Units_2021\n",
    "gen_cap_actuals.loc[:,'Input_2025'] = gen_cap_actuals.MaxCap_2025*gen_cap_actuals.Units_2025\n",
    "gen_cap_actuals.loc[:,'Input_2030'] = gen_cap_actuals.MaxCap_2030*gen_cap_actuals.Units_2030\n",
    "gen_cap_actuals.loc[:,'Input_2037'] = gen_cap_actuals.MaxCap_2037*gen_cap_actuals.Units_2037\n",
    "\n",
    "gen_cap_actuals.loc[:,'Input2030_NNTp2025'] = gen_cap_actuals.Input_2030\n",
    "gen_cap_actuals.loc[:,'Input2037_NNTp2025'] = gen_cap_actuals.Input_2037\n",
    "#\n",
    "thermal_Tech = ['Gas', 'Coal', 'Oil']\n",
    "gen_cap_actuals.loc[(gen_cap_actuals.Tech_simple.isin(thermal_Tech))&(gen_cap_actuals.BuiltP2025 == True),'Input2030_NNTp2025'] = 0\n",
    "gen_cap_actuals.loc[(gen_cap_actuals.Tech_simple.isin(thermal_Tech))&(gen_cap_actuals.BuiltP2025 == True),'Input2037_NNTp2025'] = 0\n",
    "\n",
    "\n",
    "gen_cap_actuals = gen_cap_actuals[['PLEXOSname', 'ValidationTech'] + [c for c in gen_cap_actuals if 'Input' in c]].dropna(how='all').fillna(0)\n",
    "\n",
    "gen_cap_actuals_tech = gen_cap_actuals.groupby([ 'ValidationTech'])[[c for c in gen_cap_actuals.columns if 'Input' in c]].sum().rename_axis('model', axis=1).stack('model').reorder_levels([ 'model', 'ValidationTech'])\n",
    "gen_cap_actuals_plant = gen_cap_actuals.groupby(['PLEXOSname'])[[c for c in gen_cap_actuals.columns if 'Input' in c]].sum().rename_axis('model', axis=1).stack('model').reorder_levels([ 'model', 'PLEXOSname'])\n",
    "##########\n",
    "\n",
    "\n",
    "\n",
    "gen_valid_cap_tech = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(\n",
    "    [ 'model', 'ValidationTech']).value.sum()\n",
    "\n",
    "gen_valid_by_tech = gen_yr_df[gen_yr_df.property == 'Generation'].groupby(\n",
    "    [ 'model', 'ValidationTech']).value.sum()\n",
    "\n",
    "gen_valid_cap_plant =gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(['model', 'name',]).value.sum().rename_axis(['model', 'PLEXOSname'])\n",
    "\n",
    "\n",
    "\n",
    "gen_valid_cap_tech = pd.concat([gen_cap_actuals_tech, gen_valid_cap_tech], axis=0).unstack('ValidationTech')\n",
    "gen_valid_cap_plant = pd.concat([gen_cap_actuals_plant, gen_valid_cap_plant], axis=0).unstack('model')\n",
    "gen_valid_by_tech = pd.concat([gen_actuals_tech, gen_valid_by_tech], axis=0).unstack('ValidationTech')\n",
    "\n",
    "try:\n",
    "    add_df_column(gen_valid_cap_tech, 'units', 'MW').to_csv(os.path.join(save_dir_sum, '00a_gen_cap_valid_by_tech.csv'), index=False)\n",
    "    add_df_column(gen_valid_by_tech, 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '00b_gen_valid_by_tech.csv'), index=False)\n",
    "    add_df_column(gen_valid_cap_plant, 'units', 'MW').to_csv(os.path.join(save_dir_sum, '00c_gen_cap_valid_by_plant.csv'), index=False)\n",
    "\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regs = np.sort(np.unique((node_df[geo_cols].values.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df.Region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 1\n",
    "### To allow scaling, the group index is maintained (i.e. as_index=True) before resetting the index\n",
    "\n",
    "load_by_reg = node_yr_df[node_yr_df.property == 'Load'].groupby(['model', 'timestamp']+geo_cols).sum().value\n",
    "customer_load_by_reg = node_yr_df[(node_yr_df.property == 'Customer Load')|(node_yr_df.property == 'Unserved Energy')].groupby(['model', 'timestamp']+geo_cols).sum().value\n",
    "\n",
    "add_df_column(load_by_reg, 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '01a_load_by_reg.csv'), index=False)\n",
    "add_df_column(customer_load_by_reg, 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '01b_customer_load_by_reg.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 2: USE\n",
    "\n",
    "\n",
    "use_by_reg = node_yr_df[node_yr_df.property == 'Unserved Energy'].groupby(['model'] + geo_cols ).sum().value\n",
    "use_reg_daily_ts =  node_yr_df[node_yr_df.property == 'Unserved Energy'].groupby(['model'] + geo_cols + [pd.Grouper(key='timestamp',freq='D')] ).sum().value\n",
    "\n",
    "\n",
    "add_df_column(use_by_reg, 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '02a_use_reg.csv'), index=False)\n",
    "add_df_column(use_reg_daily_ts, 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '02b_use_reg_daily_ts.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_yr_df.WEO_tech.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ouput 3a:\n",
    "\n",
    "## Standard\n",
    "gen_by_tech_reg =  gen_yr_df[gen_yr_df.property == 'Generation'].groupby(['model']+geo_cols+ ['Category']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "gen_by_tech_reg_orig = gen_by_tech_reg.copy() ## From Indonesia to allow for co-firing fuels to be seperated ... kept only for now to avoid breaking other partss\n",
    "gen_by_subtech_reg =  gen_yr_df[gen_yr_df.property == 'Generation'].groupby(['model']+geo_cols+ ['CostTech']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "gen_techs = gen_yr_df.Category.drop_duplicates().values\n",
    "\n",
    "##cofiring removed. to be added as standard?\n",
    "    \n",
    "    \n",
    "# gen_by_costTech_reg = gen_by_tech_reg.groupby(['model']+geo_cols+ ['CostCategory']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "# gen_by_tech_reg = gen_by_tech_reg\n",
    "# gen_by_tech_reg_orig = gen_by_tech_reg_orig.groupby(['model']+geo_cols+ ['Category']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "# gen_by_weoTech_reg = gen_yr_df[gen_yr_df.property == 'Generation'].groupby(['model'] + geo_cols + ['WEO_Tech_simpl']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "\n",
    "try:\n",
    "    add_df_column(gen_by_tech_reg.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '03a_gen_by_tech_reg.csv'), index=False)\n",
    "#     add_df_column(gen_by_tech_reg.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '03b_gen_by_tech_reg_w_Validation.csv'), index=False)\n",
    "    add_df_column(gen_by_subtech_reg.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '03a_gen_by_subtech_reg.csv'), index=False)\n",
    "\n",
    "#     add_df_column(gen_by_costTech_reg.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '03c_gen_by_costTech_reg.csv'), index=False)\n",
    "#     add_df_column(gen_by_weoTech_reg.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '03d_gen_by_weoTech_reg.csv'), index=False)\n",
    "#     add_df_column(gen_by_tech_subreg.stack(), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '04b_gen_by_tech_subreg.csv'), index=False)\n",
    "#     add_df_column(gen_by_tech_isl.stack(), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '04c_gen_by_tech_isl.csv'), index=False)\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ouput xx: Fuel offtake and emissions\n",
    "\n",
    "## Doesn't work as emissions.Fuels doesnt exist in model. need to add some check?\n",
    "\n",
    "## Standard\n",
    "fuel_by_type =  fuel_yr_df[fuel_yr_df.property == 'Offtake']\n",
    "fuel_by_type = fuel_by_type.groupby(['model','Type','Category']).sum().value\n",
    "\n",
    "\n",
    "co2_fuels_by_reg = em_fuel_yr_df[(em_fuel_yr_df.property == 'Production')&(em_fuel_yr_df.parent == 'CO2')]\n",
    "co2_fuels_by_reg = co2_fuels_by_reg.groupby(['model']+geo_cols + ['Type','Category']).sum().value\n",
    "\n",
    "    \n",
    "# gen_by_costTech_reg = gen_by_tech_reg.groupby(['model']+geo_cols+ ['CostCategory']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "# gen_by_tech_reg = gen_by_tech_reg.groupby(['model']+geo_cols+ ['Category']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "\n",
    "# if validation:\n",
    "#     ### This needs to be fixed to the region/subregion change\n",
    "#     idn_actuals_by_tech_reg  = idn_actuals_2019.groupby(['model'] +geo_cols +[ 'Category']).sum().Generation.unstack(level=geo_cols).fillna(0)\n",
    "#     gen_by_tech_reg = pd.concat([gen_by_tech_reg, idn_actuals_by_tech_reg], axis=0)\n",
    "\n",
    "try:\n",
    "    add_df_column(fuel_by_type, 'units', 'TJ').to_csv(os.path.join(save_dir_sum, '00x_fuel_by_reg.csv'), index=False)\n",
    "    add_df_column(co2_fuels_by_reg, 'units', 'tonnes').to_csv(os.path.join(save_dir_sum, '00x_co2_by_fuel.csv'), index=False)\n",
    "#     add_df_column(gen_by_tech_subreg.stack(), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '04b_gen_by_tech_subreg.csv'), index=False)\n",
    "#     add_df_column(gen_by_tech_isl.stack(), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '04c_gen_by_tech_isl.csv'), index=False)\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 3b: RE/VRE Shares:\n",
    "\n",
    "gen_techs = list(soln_idx[soln_idx.Object_type == 'Generator'].Category.drop_duplicates())\n",
    "vre_techs =['Solar', 'Wind']\n",
    "re_techs = ['Solar', 'Wind', 'Bioenergy', 'Geothermal', 'Other', 'Marine', 'Hydro']\n",
    "\n",
    "re_by_reg = gen_by_tech_reg.reset_index()\n",
    "re_by_reg.loc[:,'RE'] = re_by_reg.Category.apply(lambda x: 'RE' if x in re_techs else 'Non-RE')\n",
    "\n",
    "vre_by_reg = gen_by_tech_reg.reset_index()\n",
    "vre_by_reg.loc[:,'VRE'] = vre_by_reg.Category.apply(lambda x: 'VRE' if x in vre_techs else 'Non-VRE')\n",
    "\n",
    "\n",
    "re_by_reg = re_by_reg.groupby(['model','RE']).sum().groupby(level=geo_cols[0], axis=1).sum()\n",
    "re_by_reg.loc[:,'UKR'] = re_by_reg.sum(axis=1)\n",
    "re_by_reg = re_by_reg.loc[ix[:,'RE'],].droplevel('RE')/re_by_reg.groupby('model').sum()\n",
    "\n",
    "vre_by_reg = vre_by_reg.groupby(['model', 'VRE']).sum().groupby(level=geo_cols[0], axis=1).sum()\n",
    "vre_by_reg.loc[:,'UKR'] = vre_by_reg.sum(axis=1)\n",
    "vre_by_reg = vre_by_reg.loc[ix[:,'VRE'],].droplevel('VRE')/vre_by_reg.groupby('model').sum()\n",
    "\n",
    "\n",
    "try:\n",
    "    add_df_column(re_by_reg, 'units', '%').to_csv(os.path.join(save_dir_sum, '03b_re_by_reg.csv'), index=False)\n",
    "    add_df_column(vre_by_reg, 'units', '%').to_csv(os.path.join(save_dir_sum, '03c_vre_by_reg.csv'), index=False)\n",
    "#     add_df_column(gen_by_tech_subreg.stack(), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '04b_gen_by_tech_subreg.csv'), index=False)\n",
    "#     add_df_column(gen_by_tech_isl.stack(), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '04c_gen_by_tech_isl.csv'), index=False)\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ouput 5\n",
    "\n",
    "unit_starts_by_tech = gen_yr_df[gen_yr_df.property == 'Units Started'].groupby(\n",
    "    [ 'model', 'Category']).sum().value.unstack(level='Category')\n",
    "\n",
    "add_df_column(unit_starts_by_tech.stack(), 'units', 'starts').to_csv(\n",
    "    os.path.join(save_dir_sum, '05_unit_starts_by_tech.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ouput 6\n",
    "\n",
    "## Standard\n",
    "gen_max_by_tech_reg = gen_df[gen_df.property == 'Generation'].groupby(\n",
    "    ['model']+geo_cols+ ['Category']).max(numeric_only=True).value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "# gen_max_by_tech_subreg = gen_df[gen_df.property == 'Generation'].groupby(\n",
    "#     ['model',geo_cols[1], 'Category']).max(numeric_only=True).value.unstack(level=geo_cols[1]).fillna(0)\n",
    "\n",
    "# gen_max_by_tech_isl = gen_df[gen_df.property == 'Generation'].groupby(\n",
    "#     ['model',geo_cols[0], 'Category']).max(numeric_only=True).value.unstack(level=geo_cols[0]).fillna(0)\n",
    "\n",
    "try:\n",
    "    add_df_column(gen_max_by_tech_reg.stack(geo_cols), 'units', 'MW').to_csv(\n",
    "            os.path.join(save_dir_sum, '06a_gen_max_by_tech_reg.csv'), index=False)\n",
    "#     add_df_column(gen_max_by_tech_subreg.stack(), 'units', 'MW').to_csv(\n",
    "#             os.path.join(save_dir_sum, '06a_gen_max_by_tech_subreg.csv'), index=False)\n",
    "#     add_df_column(gen_max_by_tech_isl.stack(), 'units', 'MW').to_csv(\n",
    "#             os.path.join(save_dir_sum, '06a_gen_max_by_tech_isl.csv'), index=False)\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ouput 7\n",
    "\n",
    "tx_losses = line_yr_df[line_yr_df.property == 'Loss'].groupby(['model', 'timestamp','name']).sum().value\n",
    "try:\n",
    "    add_df_column(tx_losses, 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '07_tx_losses.csv'), index=False)\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vre_techs =['Solar', 'Wind']\n",
    "\n",
    "# vre_cap = gen_yr_df[(gen_yr_df.property == 'Installed Capacity')& (gen_yr_df.Category.isin(vre_techs))].groupby(\n",
    "#     ['model', 'Category'] +geo_cols).max().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "# vre_av_abs = gen_df[(gen_df.property == 'Available Capacity')& (gen_df.Category.isin(vre_techs))].groupby(\n",
    "#     ['model', 'Category'] +geo_cols +[pd.Grouper(key='timestamp',freq='D')]).sum().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)*hour_corr\n",
    "\n",
    "# vre_gen_abs = gen_df[(gen_df.property == 'Generation')& (gen_df.Category.isin(vre_techs))].groupby(\n",
    "#     ['model', 'Category',] +geo_cols +[pd.Grouper(key='timestamp',freq='D')]).sum().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)*hour_corr\n",
    "\n",
    "\n",
    "# # ### Add zero values to regions without VRE\n",
    "# geo_col_filler = pd.Series(data=np.ones(len(load_by_reg.unstack(geo_cols).columns)), index=load_by_reg.unstack(geo_cols).columns)\n",
    "# vre_cap = (vre_cap*geo_col_filler).fillna(0)\n",
    "# vre_av_abs = (vre_av_abs*geo_col_filler).fillna(0)\n",
    "# vre_gen_abs = (vre_gen_abs*geo_col_filler).fillna(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 8 & 9\n",
    "\n",
    "## Fill in data for regions which have no VRE (i.e. zero arrays!) to allow similar arrays for load_ts and vre_add_df_columns\n",
    "## To add something for subregions\n",
    "\n",
    "### There is an error in PLEXOS with Available Capacity versus Generation (Gen exceeds Av Capacity)\n",
    "\n",
    "vre_cap = gen_yr_df[(gen_yr_df.property == 'Installed Capacity')& (gen_yr_df.Category.isin(vre_techs))].groupby(\n",
    "    ['model', 'Category'] +geo_cols).max().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "vre_av_abs = gen_df[(gen_df.property == 'Available Capacity')& (gen_df.Category.isin(vre_techs))].groupby(\n",
    "    ['model', 'Category'] +geo_cols +[pd.Grouper(key='timestamp',freq='D')]).sum().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)*hour_corr\n",
    "\n",
    "vre_gen_abs = gen_df[(gen_df.property == 'Generation')& (gen_df.Category.isin(vre_techs))].groupby(\n",
    "    ['model', 'Category',] +geo_cols +[pd.Grouper(key='timestamp',freq='D')]).sum().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)*hour_corr\n",
    "\n",
    "\n",
    "# ### Add zero values to regions without VRE\n",
    "geo_col_filler = pd.Series(data=np.ones(len(load_by_reg.unstack(geo_cols).columns)), index=load_by_reg.unstack(geo_cols).columns)\n",
    "vre_cap = (vre_cap*geo_col_filler).fillna(0)\n",
    "vre_av_abs = (vre_av_abs*geo_col_filler).fillna(0)\n",
    "vre_gen_abs = (vre_gen_abs*geo_col_filler).fillna(0)      \n",
    "\n",
    "\n",
    "### 24 periods per day for the daily data\n",
    "vre_av_norm =  (vre_av_abs / vre_cap / daily_periods).fillna(0)\n",
    "vre_curtailed = vre_av_abs - vre_gen_abs\n",
    "\n",
    "### Add non-VRE spillage/curtailment\n",
    "constr_techs = ['Hydro', 'Bioenergy', 'Geothermal']\n",
    "\n",
    "other_re_gen_abs = gen_df[(gen_df.property == 'Generation')& (gen_df.Category.isin(constr_techs))].groupby(\n",
    "    ['model', 'Category',] +geo_cols +[pd.Grouper(key='timestamp',freq='D')]).sum().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)*hour_corr \n",
    "other_re_energy_vio = gen_df[(gen_df.property == 'Min Energy Violation') & (gen_df.Category.isin(constr_techs))].groupby(\n",
    "    ['model', 'Category'] +geo_cols +[pd.Grouper(key='timestamp',freq='D')]).sum().value.unstack('Category').fillna(0).stack('Category').unstack(level=geo_cols).fillna(0)*hour_corr\n",
    "\n",
    "other_re_gen_abs = (other_re_gen_abs*geo_col_filler).fillna(0)\n",
    "other_re_energy_vio = (other_re_energy_vio*geo_col_filler).fillna(0)\n",
    "other_re_av = other_re_energy_vio + other_re_gen_abs\n",
    "\n",
    "all_re_av = pd.concat([vre_av_abs, other_re_av], axis=0).reset_index().groupby(['model', 'timestamp', 'Category']).sum()\n",
    "\n",
    "all_re_curtailed = pd.concat([vre_curtailed, other_re_energy_vio], axis=0).reset_index().groupby(['model', 'timestamp', 'Category']).sum()\n",
    "\n",
    "\n",
    "re_curtailment_rate_by_tech = (all_re_curtailed.sum(axis=1).groupby(['model','Category']).sum().unstack('Category')/all_re_av.sum(axis=1).groupby(['model','Category']).sum().unstack('Category')).fillna(0)*100\n",
    "re_curtailment_rate = (all_re_curtailed.sum(axis=1).groupby('model').sum()/all_re_av.sum(axis=1).groupby('model').sum()).fillna(0)*100\n",
    "\n",
    "curtailment_rate = (vre_curtailed.sum(axis=1).groupby('model').sum()/vre_av_abs.sum(axis=1).groupby('model').sum()).fillna(0)*100\n",
    "\n",
    "\n",
    "curtailment_rate_by_tech = (vre_curtailed.sum(axis=1).groupby(['model','Category']).sum().unstack('Category')/vre_av_abs.sum(axis=1).groupby(['model','Category']).sum().unstack('Category')).fillna(0)*100\n",
    "curtailment_rate = (vre_curtailed.sum(axis=1).groupby('model').sum()/vre_av_abs.sum(axis=1).groupby('model').sum()).fillna(0)*100\n",
    "curtailment_rate_reg = (vre_curtailed.groupby(geo_cols[0],axis=1).sum().groupby('model').sum()/vre_av_abs.groupby(geo_cols[0],axis=1).sum().groupby('model').sum()).fillna(0)*100\n",
    "curtailment_rate = pd.concat([curtailment_rate_reg, curtailment_rate.rename('UKR')], axis=1)\n",
    "\n",
    "try:\n",
    "    add_df_column(vre_cap.stack(geo_cols), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '08a_vre_cap.csv'), index=False)\n",
    "    add_df_column(vre_av_abs.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '08b_vre_daily_abs.csv'),index=False)\n",
    "    add_df_column(vre_av_norm.stack(geo_cols), 'units', '-').to_csv(os.path.join(save_dir_sum, '08c_vre_daily_norm.csv'),index=False)\n",
    "    add_df_column(vre_curtailed.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '09a_vre_daily_curtailed.csv'), index=False)\n",
    "    add_df_column(curtailment_rate, 'units', '%').to_csv(os.path.join(save_dir_sum, '09b_curtailment_rate.csv'), index=False)\n",
    "    add_df_column(all_re_curtailed.stack(geo_cols), 'units', 'GWh').to_csv(os.path.join(save_dir_sum, '09c_all_RE_daily_curtailed.csv'), index=False)\n",
    "    add_df_column(re_curtailment_rate, 'units', '%').to_csv(os.path.join(save_dir_sum, '09d_all_RE_curtailment_rate.csv'), index=False)\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 11 & 12 : a) capacity & CFs per technology/region b) CFs per tech only\n",
    "\n",
    "gen_cap_tech_reg = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(\n",
    "    [ 'model'] + geo_cols + ['Category']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "# gen_cap_tech_reg_IPPs = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(\n",
    "#     [ 'model'] + geo_cols + ['IPP', 'Category']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "# gen_cap_tech_subreg = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(\n",
    "#     [ 'model', geo_cols[1], 'Category']).sum().value.unstack(level=geo_cols[1]).fillna(0)\n",
    "\n",
    "gen_cap_subtech_reg = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(\n",
    "    [ 'model'] +geo_cols + ['CapacityCategory']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "gen_cap_COSTtech_reg = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(\n",
    "    [ 'model'] +geo_cols + ['CostTech']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "# #### For Capex calcs\n",
    "# gen_cap_costTech_reg = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(\n",
    "#     [ 'model'] +geo_cols + ['CostCategory']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "# gen_cap_by_weoTech_reg = gen_yr_df[gen_yr_df.property == 'Installed Capacity'].groupby(['model'] + geo_cols + ['WEO_Tech_simpl']).sum().value.unstack(level=geo_cols).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "### Calculate as EN[GWh]/(Capacity[MW]/1000*hours)\n",
    "### Standard\n",
    "### As we make adjustments for co_firing on the energy values, we must re-calculate this\n",
    "\n",
    "cf_tech_reg = (gen_by_tech_reg_orig/(gen_cap_tech_reg/1000*nr_days*24)).fillna(0)\n",
    "\n",
    "cf_tech = (gen_by_tech_reg_orig.sum(axis=1)/(gen_cap_tech_reg.sum(axis=1)/1000*nr_days*24)).unstack(\n",
    "    level='Category').fillna(0)\n",
    "\n",
    "gen_cap_COSTtech_reg\n",
    "try:\n",
    "    add_df_column(gen_cap_tech_reg.stack(geo_cols), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '11a_cap_by_tech_reg.csv'), index=False)\n",
    "    add_df_column(gen_cap_subtech_reg.stack(geo_cols), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '11b_gen_cap_by_subtech_reg.csv'), index=False)\n",
    "    add_df_column(gen_cap_COSTtech_reg.stack(geo_cols), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '11b_gen_cap_by_COSTtech_reg.csv'), index=False)\n",
    "\n",
    "#     add_df_column(gen_cap_costTech_reg.stack(geo_cols), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '11c_gen_cap_by_costTech_reg.csv'), index=False)\n",
    "#     add_df_column(gen_cap_costTech_reg.stack(geo_cols), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '11d_gen_cap_by_weoTech_reg.csv'), index=False)\n",
    "#     add_df_column(gen_cap_tech_reg_IPPs.stack(geo_cols), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '11d_gen_cap_w_IPPs_by_tech_reg.csv'), index=False)\n",
    "\n",
    "#     add_df_column(gen_cap_tech_subreg.stack(), 'units', 'MW').to_csv(os.path.join(save_dir_sum, '11b_gen_cap_by_tech_subreg.csv'), index=False)\n",
    "    add_df_column(cf_tech_reg.stack(geo_cols), 'units', '%').to_csv(os.path.join(save_dir_sum, '12a_cf_tech_reg.csv'), index=False)\n",
    "#     add_df_column(cf_tech_subreg.stack(), 'units', '%').to_csv(os.path.join(save_dir_sum, '12b_cf_tech_subreg.csv'), index=False)\n",
    "    add_df_column(cf_tech, 'units', '%').to_csv(os.path.join(save_dir_sum, '12c_cf_tech.csv'), index=False)\n",
    "\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 14: Emissions\n",
    "\n",
    "## Standard\n",
    "\n",
    "em_by_type_tech_reg = em_gen_yr_df[(em_gen_yr_df.property=='Production')].groupby(\n",
    "    [ 'model', 'parent'] + geo_cols + ['Category']).sum().value.reset_index()\n",
    "\n",
    "em_by_type_tech_reg.loc[:,'parent'] = em_by_type_tech_reg.parent.apply(lambda x: x if '_' not in x else x.split('_')[0])\n",
    "\n",
    "em_by_type_tech_reg = em_by_type_tech_reg.groupby([ 'model', 'parent'] + geo_cols + ['Category']).sum()\n",
    "\n",
    "############\n",
    "\n",
    "co2_by_tech_reg = em_gen_yr_df[em_gen_yr_df.parent.str.contains('CO2')&(em_gen_yr_df.property=='Production')].groupby(\n",
    "    [ 'model'] + geo_cols + ['Category']).sum().value\n",
    "\n",
    "co2_by_reg = em_gen_yr_df[em_gen_yr_df.parent.str.contains('CO2')&(em_gen_yr_df.property=='Production')].groupby(\n",
    "    [ 'model'] + geo_cols).sum().value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    add_df_column(co2_by_tech_reg, 'units', 'tonnes').to_csv(\n",
    "        os.path.join(save_dir_sum, '13a_co2_by_tech_reg.csv'), index=False)\n",
    "    add_df_column(co2_by_reg, 'units', 'tonnes').to_csv(\n",
    "        os.path.join(save_dir_sum, '13b_co2_by_reg.csv'), index=False)\n",
    "except PermissionError:\n",
    "    print(\"Permission error: file not written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a) Load time series; b) Load time-series (regional); c) Load with USE time-series\n",
    "2. a) Generation time-series; b) VRE available; c) VRE available (regional); d) Curtailment; e) Generator availability \n",
    "3. a) Regional generation time-series b) Capacity factor monthly and c) weekly\n",
    "4. a) Net-load regional timeseries; b) Net-load agg timeseries;  c) Inertia agg timeseries; d) Inertia tech timeseries\n",
    "5. a) Regional price time-series b) Shadow price; c) SRMC\n",
    "6. Days of interest/values: max avg load, peak net/total load, max ramp, min avg net load, min inertia, min net/total load\n",
    "7. Generation stacks on days of interest (output to pre-made charts)\n",
    "8. Unit out timeseries\n",
    "9. Peak availability vs installed capacity\n",
    "10. a) GDC; b) Unused generation by tech\n",
    "11. Services by tech\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done for all models....maybe to change if performance suffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_ts = os.path.join(save_dir,'timeseries_out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(save_dir_ts) is False:\n",
    "    os.mkdir(save_dir_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ts = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 1a-b: Load and USE time-series\n",
    "total_load_ts = reg_df[reg_df.property == 'Load'].groupby(['model', 'timestamp']).sum().value\n",
    "\n",
    "\n",
    "add_df_column(total_load_ts, 'units', 'MW').reset_index().to_csv(os.path.join(save_dir_ts, '01a_total_load_ts.csv'))\n",
    "\n",
    "use_ts = reg_df[reg_df.property == 'Unserved Energy'].groupby(['model', 'timestamp']).sum().value\n",
    "add_df_column(use_ts, 'units', 'MW').reset_index().to_csv(os.path.join(save_dir_ts, '01b_use_ts.csv'))\n",
    "\n",
    "### Need to calculate whether its 30mn or 1hr within but for now just assume hourly\n",
    "use_dly_ts = use_ts.groupby([pd.Grouper(level='model'), pd.Grouper(freq='D',level='timestamp')]).sum()/1000\n",
    "add_df_column(use_dly_ts, 'units', 'GWh').reset_index().to_csv(os.path.join(save_dir_ts, '01c_use_dly_ts.csv'))\n",
    "\n",
    "load_w_use_ts = pd.concat([total_load_ts.rename('Load'),use_ts.rename('USE')])\n",
    "add_df_column(load_w_use_ts, 'units', 'MW').reset_index().to_csv(os.path.join(save_dir_ts, '01d_load_w_use_ts.csv'))\n",
    "\n",
    "\n",
    "if reg_ts:\n",
    "    \n",
    "    load_by_reg_ts = node_df[node_df.property == 'Load'].groupby(['model'] + geo_cols + ['timestamp']).sum().value.unstack(\n",
    "    level='timestamp').fillna(0).stack('timestamp')\n",
    "    \n",
    "    use_reg_ts = node_df[node_df.property == 'Unserved Energy'].groupby(['model'] + geo_cols + ['timestamp']).sum().value.unstack(level=geo_cols)\n",
    "    \n",
    "    use_dly_reg_ts = use_reg_ts.groupby([pd.Grouper(level='model'), pd.Grouper(freq='D',level='timestamp')]).sum()/1000\n",
    "    \n",
    "    \n",
    "    for geo in geo_cols:\n",
    "        \n",
    "        geo_suffix = geo[:3].lower()\n",
    "        count = ord('e')\n",
    "        \n",
    "    \n",
    "        add_df_column(load_by_reg_ts.unstack(level=geo_cols).groupby(level=geo, axis=1).sum(), 'units', 'MW').to_csv(os.path.join(\n",
    "            save_dir_ts, '01{}_total_load_{}_ts.csv'.format(chr(count), geo_suffix)), index=False)\n",
    "\n",
    "    \n",
    "        add_df_column(use_reg_ts.groupby(level=geo, axis=1).sum(), 'units', 'MW').to_csv(os.path.join(\n",
    "            save_dir_ts, '01{}_use_{}_ts.csv'.format(chr(count+1), geo_suffix)), index=False)\n",
    "\n",
    "        \n",
    "        add_df_column(use_dly_reg_ts.groupby(level=geo, axis=1).sum(), 'units', 'GWh').to_csv(os.path.join(save_dir_ts, '01{}_use_dly_{}_ts.csv'.format(chr(count+2), geo_suffix)), index=False)\n",
    "        \n",
    "        count += 3 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 2: Generation\n",
    "\n",
    "gen_by_tech_ts = gen_df[gen_df.property == 'Generation'].groupby(\n",
    "                    ['model', 'Category', 'timestamp']).sum().value.unstack(level='Category').fillna(0)\n",
    "\n",
    "gen_by_subtech_ts = gen_df[gen_df.property == 'Generation'].groupby(\n",
    "                    ['model', 'CapacityCategory', 'timestamp']).sum().value.unstack(level='CapacityCategory').fillna(0)\n",
    "\n",
    "av_cap_by_tech_ts = gen_df[gen_df.property == 'Available Capacity'].groupby(\n",
    "                    ['model', 'Category', 'timestamp']).sum().value.unstack(level='Category').fillna(0)\n",
    "\n",
    "add_df_column(gen_by_tech_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '02a_gen_by_tech_ts.csv'),\n",
    "                                                    index=False)\n",
    "\n",
    "add_df_column(gen_by_subtech_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '02b_gen_by_tech_ts.csv'),\n",
    "                                                    index=False)\n",
    "\n",
    "add_df_column(av_cap_by_tech_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '02c_av_cap_by_subtech_ts.csv'),\n",
    "                                                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 3: VRE Time-series\n",
    "\n",
    "vre_av_abs_ts = gen_df[(gen_df.property == 'Available Capacity') & (gen_df.Category.isin(vre_techs))].groupby(\n",
    "    ['model', 'Category', 'timestamp']).sum().value.unstack(\n",
    "    level='Category').fillna(0)\n",
    "\n",
    "vre_gen_abs_ts = gen_df[(gen_df.property == 'Generation') & (gen_df.Category.isin(vre_techs))].groupby(\n",
    "    ['model', 'Category', 'timestamp']).sum().value.unstack(\n",
    "    level='Category').fillna(0)\n",
    "\n",
    "vre_curtailed_ts = vre_av_abs_ts - vre_gen_abs_ts\n",
    "\n",
    "constr_techs = ['Hydro', 'Bioenergy', 'Geothermal']\n",
    "\n",
    "min_energy_vio_tech_ts = gen_df[(gen_df.property == 'Min Energy Violation') & (gen_df.Category.isin(constr_techs))].groupby(\n",
    "    ['model', 'Category', 'timestamp']).sum().value.unstack(\n",
    "    level='Category').fillna(0)\n",
    "\n",
    "re_curtailed_ts = pd.concat([vre_curtailed_ts, min_energy_vio_tech_ts])\n",
    "\n",
    "\n",
    "\n",
    "curtailment_dc = vre_curtailed_ts.sum(axis=1).unstack('model')\n",
    "curtailment_dc = pd.DataFrame(np.flipud(np.sort(curtailment_dc.values, axis=0)), index=curtailment_dc.index, columns=curtailment_dc.columns)\n",
    "\n",
    "\n",
    "add_df_column(vre_av_abs_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '03a_vre_available_ts.csv'),\n",
    "                                                index=False)\n",
    "add_df_column(vre_av_abs_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '03b_vre_gen_ts.csv'),\n",
    "                                                index=False)\n",
    "add_df_column(vre_curtailed_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '03c_vre_curtailed_ts.csv'),\n",
    "                                                index=False)\n",
    "add_df_column(re_curtailed_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '03c_RE_curtailed_ts.csv'),\n",
    "                                                index=False)\n",
    "\n",
    "\n",
    "### Due to the size, we do the gen DFs per model\n",
    "if reg_ts:\n",
    "    \n",
    "    ### Define model regs in multi-level format\n",
    "    model_regs_multi = load_by_reg_ts.unstack(geo_cols).columns\n",
    "\n",
    "\n",
    "    \n",
    "    vre_av_reg_abs_ts = gen_df[(gen_df.property == 'Available Capacity') &(gen_df.Category.isin(vre_techs))].groupby(\n",
    "        (['model'] + geo_cols + ['timestamp'])).sum().value.unstack(\n",
    "        level=geo_cols).fillna(0)\n",
    "\n",
    "    vre_gen_reg_abs_ts = gen_df[(gen_df.property == 'Generation') & (gen_df.Category.isin(vre_techs))].groupby(\n",
    "        (['model'] + geo_cols + ['timestamp'])).sum().value.unstack(\n",
    "        level=geo_cols).fillna(0)\n",
    "    \n",
    "    vre_regs = vre_av_reg_abs_ts.columns\n",
    "\n",
    "    ## Fill in data for regions which have no VRE (i.e. zero arrays!) to allow similar arrays for load_ts and vre_av_ts\n",
    "    for reg in model_regs_multi:\n",
    "        if reg not in vre_regs:\n",
    "            vre_av_reg_abs_ts[reg] = 0\n",
    "            vre_gen_reg_abs_ts[reg] = 0\n",
    "            \n",
    "    ### Columns in alphabetical order        \n",
    "    vre_av_reg_abs_ts = vre_av_reg_abs_ts[model_regs_multi]  \n",
    "    vre_gen_reg_abs_ts = vre_gen_reg_abs_ts[model_regs_multi]\n",
    "    \n",
    "    vre_curtailed_reg_ts = vre_av_reg_abs_ts - vre_gen_reg_abs_ts\n",
    "    \n",
    "\n",
    "    for m in model_names:\n",
    "        save_dir_model = os.path.join(save_dir_ts, m.replace(\"/\",\"\"))\n",
    "        if os.path.exists(save_dir_model) is False:\n",
    "            os.mkdir(save_dir_model)\n",
    "    \n",
    "        vre_av = vre_av_reg_abs_ts.loc[ix[m,:]]\n",
    "\n",
    "        vre_gen = vre_gen_reg_abs_ts.loc[ix[m,:]]\n",
    "        \n",
    "        \n",
    "#         ## Fill in data for regions which have no VRE (i.e. zero arrays!) to allow similar arrays for load_ts and vre_av_ts\n",
    "#         vre_regs = vre_av.columns\n",
    "\n",
    "#         for reg in model_regs_multi:\n",
    "#             if reg not in vre_regs:\n",
    "#                 vre_av[reg] = 0\n",
    "#                 vre_gen[reg] = 0\n",
    "\n",
    "#         ### Columns in alphabetical order\n",
    "#         vre_av = vre_av[model_regs_multi]  \n",
    "#         vre_gen = vre_gen[model_regs_multi]\n",
    "\n",
    "        vre_curtailed =  vre_av - vre_gen\n",
    "        \n",
    "        ## For numbering purposes only\n",
    "        count = ord('d')\n",
    "        \n",
    "        ## Output results for every geographical aggregation\n",
    "        for geo in geo_cols:\n",
    "            \n",
    "            geo_suffix = geo[:3].lower()\n",
    "            ### We probably want to output as wind/solar in columns but maintain the regions as columns for net load\n",
    "            add_df_column(vre_av.groupby(level=geo,axis=1).sum(), 'units', 'MW').to_csv(\n",
    "                os.path.join(save_dir_model, '03{}_vre_available_{}_ts.csv'.format(chr(count), geo_suffix)), index=False)\n",
    "            add_df_column(vre_gen.groupby(level=geo,axis=1).sum(), 'units', 'MW').to_csv(\n",
    "                os.path.join(save_dir_model, '03{}_vre_gen_{}_ts.csv'.format(chr(count+1), geo_suffix)), index=False)\n",
    "            add_df_column(vre_curtailed.groupby(level=geo,axis=1).sum(), 'units', 'MW').to_csv(\n",
    "                os.path.join(save_dir_model, '03{}_vre_curtailed_{}_ts.csv'.format(chr(count+2), geo_suffix)), index=False)\n",
    "            \n",
    "            \n",
    "            ## Three different outputs\n",
    "            count = count + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 4a-b: Generation by region and capacity reserve margin by technology\n",
    "\n",
    "av_cap_by_tech_reg_ts = gen_df[gen_df.property == 'Available Capacity'].groupby(\n",
    "                    ['model', 'Category'] + geo_cols + ['timestamp']).sum().value.unstack(level='Category').fillna(0)\n",
    "\n",
    "gen_by_tech_reg_ts = gen_df[gen_df.property == 'Generation'].groupby(\n",
    "                    ['model', 'Category'] + geo_cols + ['timestamp']).sum().value.unstack(level='Category').fillna(0)\n",
    "\n",
    "av_cap_by_subtech_reg_ts = gen_df[gen_df.property == 'Available Capacity'].groupby(\n",
    "                    ['model', 'CapacityCategory'] + geo_cols + ['timestamp']).sum().value.unstack(level='CapacityCategory').fillna(0)\n",
    "\n",
    "gen_by_subtech_reg_ts = gen_df[gen_df.property == 'Generation'].groupby(\n",
    "                    ['model', 'CapacityCategory'] + geo_cols + ['timestamp']).sum().value.unstack(level='CapacityCategory').fillna(0)\n",
    "\n",
    "\n",
    "### Spare capacity by region and technology\n",
    "undisp_cap_by_tech_reg_ts = av_cap_by_tech_reg_ts - gen_by_tech_reg_ts\n",
    "undisp_cap_by_tech_reg_ts = undisp_cap_by_tech_reg_ts.mask(undisp_cap_by_tech_reg_ts < 0, 0)\n",
    "\n",
    "undisp_cap_by_subtech_reg_ts = av_cap_by_subtech_reg_ts - gen_by_subtech_reg_ts\n",
    "undisp_cap_by_subtech_reg_ts = undisp_cap_by_subtech_reg_ts.mask(undisp_cap_by_subtech_reg_ts < 0, 0)\n",
    "\n",
    "### Name makes no sense\n",
    "load_geo_ts = node_df[node_df.property == 'Load'].groupby(\n",
    "                    ['model'] + geo_cols + ['timestamp']).sum().value.unstack(level=geo_cols)\n",
    "\n",
    "\n",
    "### Cappacity reserves by region\n",
    "av_cap_by_reg = av_cap_by_tech_reg_ts.sum(axis=1).unstack(level=geo_cols)\n",
    "# av_cap_by_reg.columns = [c.replace(' ', '_') for c in av_cap_by_reg.columns]\n",
    "res_by_reg_ts = av_cap_by_reg - load_geo_ts\n",
    "res_margin_by_reg_ts = res_by_reg_ts/load_geo_ts\n",
    "\n",
    "res_w_load_by_reg_ts = res_by_reg_ts.stack(geo_cols).reorder_levels(['model'] +geo_cols + ['timestamp']).rename('CapacityReserves')\n",
    "res_w_load_by_reg_ts = pd.concat([res_w_load_by_reg_ts, load_geo_ts.stack(geo_cols).reorder_levels(['model'] +geo_cols + ['timestamp']).rename('Load')], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "add_df_column(res_w_load_by_reg_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '03a_cap_reserve_w_load_by_reg_ts.csv'),\n",
    "                                                    index=False)\n",
    "\n",
    "add_df_column(res_margin_by_reg_ts, 'units', '%').to_csv(os.path.join(save_dir_ts, '03b_reserve_margin_by_reg_ts.csv'),\n",
    "                                                    index=False)\n",
    "\n",
    "\n",
    "### Output 3c: Regional generation...this is likely redundnat as the calcs are done up by CF calculations\n",
    "\n",
    "if reg_ts:\n",
    "    \n",
    "#     gen_by_tech_reg_ts = gen_df[(gen_df.property == 'Generation')].groupby(\n",
    "#             ['model'] +geo_cols +  ['Category', 'timestamp']).sum().value\n",
    "    \n",
    "#     gen_by_subtech_reg_ts = gen_df[(gen_df.property == 'Generation')].groupby(\n",
    "#         ['model'] +geo_cols +  ['CapacityCategory', 'timestamp']).sum().value\n",
    "    \n",
    "    for m in model_names:\n",
    "        save_dir_model = os.path.join(save_dir_ts, m.replace(\"/\",\"\"))\n",
    "        if os.path.exists(save_dir_model) is False:\n",
    "            os.mkdir(save_dir_model)\n",
    "\n",
    "        gen_by_tech_reg_model_ts = gen_by_tech_reg_ts.loc[ix[m,],].stack('Category')\n",
    "        \n",
    "        gen_by_subtech_reg_model_ts = gen_by_subtech_reg_ts.loc[ix[m,],].stack('CapacityCategory')\n",
    "\n",
    "        add_df_column(gen_by_tech_reg_model_ts, 'units', 'MW').to_csv(\n",
    "            os.path.join(save_dir_model, '04_gen_by_tech_reg_ts.csv'), index=False)\n",
    "        add_df_column(gen_by_subtech_reg_model_ts, 'units', 'MW').to_csv(\n",
    "            os.path.join(save_dir_model, '04_gen_by_subtech_reg_ts.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Output 5: to plot undispatched capacity for USE (and maybe other metrics)\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "if reg_ts:\n",
    "    for m in model_names:\n",
    "        save_dir_model = os.path.join(save_dir_ts, m.replace(\"/\",\"\"))\n",
    "        if os.path.exists(save_dir_model) is False:\n",
    "            os.mkdir(save_dir_model)\n",
    "            \n",
    "        \n",
    "        model_use_ts = use_ts.loc[ix[m,:]].reset_index()\n",
    "        use_periods = model_use_ts[model_use_ts.value>0]\n",
    "        model_use_reg_ts = use_reg_ts.loc[ix[m,:]].reset_index()\n",
    "        \n",
    "        if len(use_periods) == 0:\n",
    "            print(\"No USE for {}\".format(m))\n",
    "        else:\n",
    "            use_days = np.unique(use_periods.timestamp.dt.date)\n",
    "            fig_template_path = 'Z:/Ukraine/2022-3_next_step_modelling/09_ModellingSupportFiles/00_figure_templates/00_reg_data_template.xlsx'\n",
    "            fig_path = os.path.join(save_dir_model,'05a_undisp_cap_tech_during_USE_reg.xlsx')\n",
    "            \n",
    "            # with pd.ExcelWriter(fig_path, engine='openpyxl', engine_kwargs= {'strings_to_formulas': False}) as writer:\n",
    "            with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "                ### load book template i.e. sheet names, etc.\n",
    "#                 book = load_workbook(fig_template_path)\n",
    "#                 writer.book = book\n",
    "#                 writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "                \n",
    "                for reg in model_regs:\n",
    "  \n",
    "                    undisp_cap = undisp_cap_by_tech_reg_ts.groupby(['model',geo_cols[0],'timestamp']).sum().loc[m,reg,:].droplevel([0,1]).reset_index()\n",
    "                    undisp_cap = undisp_cap.loc[undisp_cap.timestamp.dt.date.isin(use_days)].set_index('timestamp')\n",
    "    \n",
    "                    use_reg = model_use_reg_ts.loc[model_use_reg_ts.timestamp.dt.date.isin(use_days)].set_index('timestamp').groupby(level=geo_cols[0], axis=1).sum()[reg].to_frame().rename(columns={reg:'USE'})\n",
    "                    undisp_cap = pd.concat([undisp_cap, use_reg], axis=1)\n",
    "                \n",
    "                    add_df_column(undisp_cap, 'units', 'MW').to_excel(writer, sheet_name=reg, index=False)\n",
    "                \n",
    "                ## Add total column\n",
    "                undisp_cap = undisp_cap_by_tech_reg_ts.groupby(['model','timestamp']).sum().loc[m,:].reset_index()\n",
    "                undisp_cap = undisp_cap.loc[undisp_cap.timestamp.dt.date.isin(use_days)].set_index('timestamp')\n",
    "                use_reg = model_use_reg_ts.loc[model_use_reg_ts.timestamp.dt.date.isin(use_days)].set_index('timestamp').sum(axis=1).to_frame().rename(columns={0:'USE'})\n",
    "                undisp_cap = pd.concat([undisp_cap, use_reg], axis=1)\n",
    "                add_df_column(undisp_cap, 'units', 'MW').to_excel(writer, sheet_name='UKR', index=False)\n",
    "                    \n",
    "            fig_path = os.path.join(save_dir_model,'05b_undisp_cap_subtech_during_USE_reg.xlsx')\n",
    "            \n",
    "            with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "                ### load book template i.e. sheet names, etc.\n",
    "#                 book = load_workbook(fig_template_path)\n",
    "#                 writer.book = book\n",
    "#                 writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "\n",
    "#                 workbook = writer.book\n",
    "#                 worksheet = writer.sheets[reg]\n",
    "                \n",
    "                for reg in model_regs:\n",
    "  \n",
    "                    undisp_cap = undisp_cap_by_subtech_reg_ts.groupby(['model',geo_cols[0],'timestamp']).sum().loc[m,reg,:].droplevel([0,1]).reset_index()\n",
    "                    undisp_cap = undisp_cap.loc[undisp_cap.timestamp.dt.date.isin(use_days)].set_index('timestamp')\n",
    "    \n",
    "                    use_reg = model_use_reg_ts.loc[model_use_reg_ts.timestamp.dt.date.isin(use_days)].set_index('timestamp').groupby(level=geo_cols[0], axis=1).sum()[reg].to_frame().rename(columns={reg:'USE'})\n",
    "                    undisp_cap = pd.concat([undisp_cap, use_reg], axis=1)\n",
    "                \n",
    "                    add_df_column(undisp_cap, 'units', 'MW').to_excel(writer, sheet_name=reg, index=False)\n",
    "                \n",
    "                ## Add total column\n",
    "                undisp_cap = undisp_cap_by_subtech_reg_ts.groupby(['model','timestamp']).sum().loc[m,:].reset_index()\n",
    "                undisp_cap = undisp_cap.loc[undisp_cap.timestamp.dt.date.isin(use_days)].set_index('timestamp')\n",
    "                use_reg = model_use_reg_ts.loc[model_use_reg_ts.timestamp.dt.date.isin(use_days)].set_index('timestamp').sum(axis=1).to_frame().rename(columns={0:'USE'})\n",
    "                undisp_cap = pd.concat([undisp_cap, use_reg], axis=1)\n",
    "                add_df_column(undisp_cap, 'units', 'MW').to_excel(writer, sheet_name='UKR', index=False)\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### CHECK why net_laod_ts is different to net_load_reg_ts, as this appears to be the case.\n",
    "\n",
    "# customer_load_ts_check = node_df[(node_df.property == 'Customer Load')|(node_df.property == 'Unserved Energy')].groupby(['model', 'timestamp']).sum().value\n",
    "# net_load_ts_check = pd.DataFrame(customer_load_ts_check - vre_av_abs_ts.fillna(0).sum(axis=1).groupby(['model', 'timestamp']).sum(), columns=['value'])\n",
    "# nl_min_dly.loc[ix[m,]].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 6: Net-load timeseries and LDCs\n",
    "\n",
    "### So that we can see the total load before USE and pump load\n",
    "customer_load_ts = reg_df[(reg_df.property == 'Customer Load')|(reg_df.property == 'Unserved Energy')].groupby(['model', 'timestamp']).sum().value\n",
    "storage_load_ts = reg_df[(reg_df.property == 'Battery Load')|(reg_df.property == 'Pump Load')].groupby(['model', 'timestamp']).sum().value\n",
    "storage_gen_ts = gen_by_tech_ts.Storage.rename('value')\n",
    "\n",
    "######\n",
    "## Model filler for comparison of models with different inputs (e.g. DSM or EVs not included)\n",
    "### Series with indices matching the columns of the DF for filling in missing columns\n",
    "model_filler = pd.Series(data=[1]*len(model_names), index=model_names).rename_axis('model')\n",
    "\n",
    "\n",
    "if purch_df.shape[0] > 0:\n",
    "    ev_profiles_ts = purch_df[purch_df.name.str.contains('_EV')&(purch_df.property == 'Load')].groupby(['model', 'timestamp']).sum().value\n",
    "    ev_profiles_orig_ts = purch_df[purch_df.name.str.contains('_EV')&(purch_df.property == 'x')].groupby(['model', 'timestamp']).sum().value \n",
    "    if ev_profiles_ts.shape[0] == 0:\n",
    "        ev_profiles_ts = ev_profiles_orig_ts = pd.Series(data=[0]*len(customer_load_ts.index), index=customer_load_ts.index)\n",
    "    else:\n",
    "        ev_profiles_ts = (ev_profiles_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model', 'timestamp'])\n",
    "        ev_profiles_orig_ts = (ev_profiles_orig_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model', 'timestamp'])       \n",
    "    \n",
    "    dsm_profiles_ts = purch_df[purch_df.name.str.contains('_Shift')&(purch_df.property == 'Load')].groupby(['model', 'timestamp']).sum().value\n",
    "    dsm_profiles_orig_ts = purch_df[purch_df.name.str.contains('_Shift')&(purch_df.property == 'x')].groupby(['model', 'timestamp']).sum().value\n",
    "    if dsm_profiles_ts.shape[0] == 0:\n",
    "        dsm_profiles_ts = dsm_profiles_orig_ts = pd.Series(data=[0]*len(customer_load_ts.index), index=customer_load_ts.index)\n",
    "    else:\n",
    "        dsm_profiles_ts = (dsm_profiles_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model', 'timestamp'])\n",
    "        dsm_profiles_orig_ts = (dsm_profiles_orig_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model', 'timestamp'])\n",
    "        \n",
    "        \n",
    "    electr_profiles_ts = purch_df[purch_df.name.str.contains('_Elec')&(purch_df.property == 'Load')].groupby(['model', 'timestamp']).sum().value\n",
    "    electr_profiles_orig_ts = purch_df[purch_df.name.str.contains('_Elec')&(purch_df.property == 'x')].groupby(['model', 'timestamp']).sum().value\n",
    "    if electr_profiles_ts.shape[0] == 0:\n",
    "        electr_profiles_orig_ts = electr_profiles_ts = pd.Series(data=[0]*len(customer_load_ts.index), index=customer_load_ts.index)\n",
    "    else:\n",
    "        electr_profiles_ts = (electr_profiles_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model', 'timestamp'])\n",
    "        electr_profiles_orig_ts = (electr_profiles_orig_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model', 'timestamp'])        \n",
    "    \n",
    "    native_load_ts  = node_df[node_df.property == 'Native Load'].groupby(['model', 'timestamp']).sum().value    \n",
    "    \n",
    "    \n",
    "    customer_load_orig_ts = (native_load_ts + ev_profiles_orig_ts + dsm_profiles_orig_ts + electr_profiles_orig_ts) #.fillna(customer_load_ts) ### For those profiles where EVs are missing, for e.g. ... other DSM to be added\n",
    "else:\n",
    "    customer_load_orig_ts = customer_load_ts\n",
    "\n",
    "    \n",
    "\n",
    "##  net_load_ts is calculated as a series (as we obtain load 'value' and some across the x-axis (technologies) of vre_abs)\n",
    "net_load_ts = pd.DataFrame(customer_load_ts - vre_av_abs_ts.fillna(0).sum(axis=1).groupby(['model', 'timestamp']).sum(), columns=['value'])\n",
    "net_load_curtail_ts = pd.DataFrame(customer_load_ts - vre_gen_abs_ts.fillna(0).sum(axis=1).groupby(['model', 'timestamp']).sum(), columns=['value'])\n",
    "net_load_orig_ts = pd.DataFrame(customer_load_orig_ts - vre_av_abs_ts.fillna(0).sum(axis=1).groupby(['model', 'timestamp']).sum(), columns=['value'])\n",
    "net_load_sto_ts =  pd.DataFrame(customer_load_ts - storage_gen_ts + storage_load_ts - vre_av_abs_ts.fillna(0).sum(axis=1).groupby(['model', 'timestamp']).sum(), columns=['value'])\n",
    "net_load_sto_curtail_ts = pd.DataFrame(customer_load_ts - storage_gen_ts + storage_load_ts - vre_gen_abs_ts.fillna(0).sum(axis=1).groupby(['model', 'timestamp']).sum(), columns=['value'])\n",
    "\n",
    "#### Calculation of all the different variations of LDC for later comparison \n",
    "ldc = customer_load_ts.unstack('model')\n",
    "ldc = pd.DataFrame(np.flipud(np.sort(ldc.values, axis=0)), index=ldc.index, columns=ldc.columns)\n",
    "\n",
    "ldc_orig = customer_load_orig_ts.unstack('model')\n",
    "ldc_orig = pd.DataFrame(np.flipud(np.sort(ldc_orig.values, axis=0)), index=ldc_orig.index, columns=ldc_orig.columns)\n",
    "\n",
    "nldc = net_load_ts.value.unstack('model')\n",
    "nldc = pd.DataFrame(np.flipud(np.sort(nldc.values, axis=0)), index=nldc.index, columns=nldc.columns)\n",
    "\n",
    "nldc_orig = net_load_orig_ts.value.unstack('model')\n",
    "nldc_orig = pd.DataFrame(np.flipud(np.sort(nldc_orig.values, axis=0)), index=nldc_orig.index, columns=nldc_orig.columns)\n",
    "\n",
    "nldc_curtail = net_load_curtail_ts.value.unstack('model')\n",
    "nldc_curtail = pd.DataFrame(np.flipud(np.sort(nldc_curtail.values, axis=0)), index=nldc_curtail.index, columns=nldc_curtail.columns)\n",
    "\n",
    "nldc_sto = net_load_sto_ts.value.unstack('model')\n",
    "nldc_sto = pd.DataFrame(np.flipud(np.sort(nldc_sto.values, axis=0)), index=nldc_sto.index, columns=nldc_sto.columns)\n",
    "\n",
    "nldc_sto_curtail = net_load_sto_curtail_ts.value.unstack('model')\n",
    "nldc_sto_curtail = pd.DataFrame(np.flipud(np.sort(nldc_sto_curtail.values, axis=0)), index=nldc_sto_curtail.index, columns=nldc_sto_curtail.columns)\n",
    "\n",
    "#####\n",
    "\n",
    "add_df_column(net_load_ts.value.unstack('model'), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06a_net_load_ts.csv'), index=True)\n",
    "add_df_column(ldc.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06b_ldc.csv'), index=True)\n",
    "add_df_column(nldc.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06c_net_ldc.csv'), index=True)\n",
    "add_df_column(nldc_curtail.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06d_net_ldc_curtail.csv'), index=True)\n",
    "add_df_column(net_load_orig_ts.value.unstack('model'), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06e_net_load_orig_ts.csv'), index=True)\n",
    "add_df_column(ldc_orig.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06f_ldc_orig.csv'), index=True)\n",
    "add_df_column(nldc_orig.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06g_net_ldc_orig.csv'), index=True)\n",
    "\n",
    "add_df_column(nldc_sto.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06h_nldc_sto.csv'), index=True)\n",
    "add_df_column(nldc_sto_curtail.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06i_net_ldc_sto_curtail.csv'), index=True)\n",
    "\n",
    "\n",
    "if reg_ts:\n",
    "    \n",
    "#     vre_av_reg_abs_ts = gen_df[(gen_df.property == 'Available Capacity') & \\\n",
    "#                                 (gen_df.FlexCategory == 'VRE')].groupby(\n",
    "#          ['model',geo_cols[0], 'timestamp']).sum().value.unstack(level=geo_cols[0])\n",
    "    \n",
    "    \n",
    "    customer_load_reg_ts = node_df[(node_df.property == 'Customer Load')|(node_df.property == 'Unserved Energy')].groupby(\n",
    "        ['model'] +geo_cols +['timestamp']).sum().value.unstack(level=geo_cols)\n",
    "    \n",
    "    storage_load_reg_ts = node_df[(reg_df.property == 'Battery Load')|(node_df.property == 'Pump Load')].groupby( ['model'] +geo_cols +['timestamp']).sum().value.unstack(level=geo_cols)\n",
    "#     storage_gen_reg_ts = gen_by_tech_reg_ts.Storage.unstack(geo_cols)\n",
    "    \n",
    "    \n",
    "    if purch_df.shape[0] > 0:\n",
    "        ev_profiles_reg_ts = purch_df[purch_df.name.str.contains('_EV')&(purch_df.property == 'Load')].groupby(['model']+geo_cols+['timestamp']).sum().value\n",
    "        ev_profiles_orig_reg_ts = purch_df[purch_df.name.str.contains('_EV')&(purch_df.property == 'x')].groupby(['model']+geo_cols+['timestamp']).sum().value\n",
    "        if ev_profiles_reg_ts.shape[0] == 0:\n",
    "            ev_profiles_reg_ts = ev_profiles_orig_reg_ts = pd.Series(data=[0]*len(customer_load_reg_ts.index), index=customer_load_reg_ts.index)\n",
    "        else:\n",
    "            ev_profiles_reg_ts = (ev_profiles_reg_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model'] + geo_cols + ['timestamp'])\n",
    "            ev_profiles_orig_reg_ts = (ev_profiles_orig_reg_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model'] + geo_cols + ['timestamp'])    \n",
    "        \n",
    "        dsm_profiles_reg_ts = purch_df[purch_df.name.str.contains('_Shift')&(purch_df.property == 'Load')].groupby(['model']+geo_cols+['timestamp']).sum().value\n",
    "        dsm_profiles_orig_reg_ts = purch_df[purch_df.name.str.contains('_Shift')&(purch_df.property == 'x')].groupby(['model']+geo_cols+['timestamp']).sum().value\n",
    "        if dsm_profiles_reg_ts.shape[0] == 0:\n",
    "            dsm_profiles_reg_ts = dsm_profiles_orig_reg_ts = pd.Series(data=[0]*len(customer_load_reg_ts.index), index=customer_load_reg_ts.index)\n",
    "        else:\n",
    "            dsm_profiles_reg_ts = (dsm_profiles_reg_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model'] + geo_cols + ['timestamp'])\n",
    "            dsm_profiles_orig_reg_ts = (dsm_profiles_orig_reg_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model'] + geo_cols + ['timestamp'])    \n",
    "            \n",
    "        electr_profiles_reg_ts = purch_df[purch_df.name.str.contains('_Elec')&(purch_df.property == 'Load')].groupby(['model']+geo_cols+['timestamp']).sum().value\n",
    "        electr_profiles_orig_reg_ts = purch_df[purch_df.name.str.contains('_Elec')&(purch_df.property == 'x')].groupby(['model']+geo_cols+['timestamp']).sum().value\n",
    "        if electr_profiles_reg_ts.shape[0] == 0:\n",
    "            electr_profiles_reg_ts = electr_profiles_orig_reg_ts = pd.Series(data=[0]*len(customer_load_reg_ts.index), index=customer_load_reg_ts.index)\n",
    "        else:\n",
    "            electr_profiles_reg_ts = (electr_profiles_reg_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model'] + geo_cols + ['timestamp'])\n",
    "            electr_profiles_orig_reg_ts = (electr_profiles_orig_reg_ts.unstack('model')*model_filler).fillna(0).stack('model').reorder_levels(['model'] + geo_cols + ['timestamp'])    \n",
    "        \n",
    "        native_load_reg_ts  = node_df[node_df.property == 'Native Load'].groupby(['model']+geo_cols+['timestamp']).sum().value\n",
    "        \n",
    "        \n",
    "        \n",
    "        customer_load_orig_reg_ts = (native_load_reg_ts + dsm_profiles_orig_reg_ts + ev_profiles_orig_ts + electr_profiles_orig_reg_ts).fillna(native_load_reg_ts).unstack(geo_cols)\n",
    "    else:\n",
    "         customer_load_orig_reg_ts = customer_load_reg_ts\n",
    "\n",
    "\n",
    "    net_load_reg_ts = customer_load_reg_ts - vre_av_reg_abs_ts\n",
    "    net_load_curtail_reg_ts = customer_load_reg_ts - vre_gen_reg_abs_ts\n",
    "    net_load_orig_reg_ts = customer_load_orig_reg_ts - vre_av_reg_abs_ts    \n",
    "#     net_load_sto_reg_ts =  customer_load_reg_ts - (storage_gen_reg_ts*geo_col_filler).fillna(0)  + (storage_load_reg_ts*geo_col_filler).fillna(0) - (vre_av_reg_abs_ts*geo_col_filler).fillna(0)\n",
    "\n",
    "\n",
    "    \n",
    "    add_df_column(net_load_reg_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06h_net_load_reg_ts.csv'), index=False)\n",
    "    add_df_column(net_load_curtail_reg_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06i_net_load_curtail_reg_ts.csv'), index=False)\n",
    "    add_df_column(net_load_orig_reg_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '06j_net_load_orig_reg_ts.csv'), index=False)\n",
    "    \n",
    "    for m in model_names:\n",
    "        ### One by one to have the LDC (which are not coincident necessarily)\n",
    "        ldc_reg = pd.DataFrame(None)\n",
    "        nldc_reg = pd.DataFrame(None)\n",
    "        ldc_orig_reg = pd.DataFrame(None)\n",
    "        nldc_orig_reg = pd.DataFrame(None)        \n",
    "        \n",
    "        for col in geo_cols:\n",
    "            \n",
    "            save_dir_model = os.path.join(save_dir_ts, m.replace(\"/\",\"\"))\n",
    "            if os.path.exists(save_dir_model) is False:\n",
    "                os.mkdir(save_dir_model)\n",
    "                \n",
    "            ldc_m = customer_load_reg_ts.loc[ix[m,]].groupby(col,axis=1).sum()\n",
    "            ldc_m = pd.DataFrame(np.flipud(np.sort(ldc_m.values, axis=0)), index=ldc_m.index, columns=ldc_m.columns)\n",
    "\n",
    "            nldc_m = net_load_reg_ts.loc[ix[m,]].groupby(col,axis=1).sum()\n",
    "            nldc_m = pd.DataFrame(np.flipud(np.sort(nldc_m.values, axis=0)), index=nldc_m.index, columns=nldc_m.columns)\n",
    "            \n",
    "            ldc_orig_m = customer_load_orig_reg_ts.loc[ix[m,]].groupby(col,axis=1).sum()\n",
    "            ldc_orig_m = pd.DataFrame(np.flipud(np.sort(ldc_orig_m.values, axis=0)), index=ldc_orig_m.index, columns=ldc_orig_m.columns)\n",
    "\n",
    "            nldc_orig_m = net_load_orig_reg_ts.loc[ix[m,]].groupby(col,axis=1).sum()\n",
    "            nldc_orig_m = pd.DataFrame(np.flipud(np.sort(nldc_orig_m.values, axis=0)), index=nldc_orig_m.index, columns=nldc_orig_m.columns)\n",
    "            \n",
    "            ##############\n",
    "                \n",
    "            ldc_reg = pd.concat([ldc_reg, ldc_m], axis=1)\n",
    "            nldc_reg = pd.concat([nldc_reg, nldc_m], axis=1)\n",
    "            ldc_orig_reg = pd.concat([ldc_orig_reg, ldc_orig_m], axis=1)\n",
    "            nldc_orig_reg = pd.concat([nldc_orig_reg, nldc_orig_m], axis=1)        \n",
    "        \n",
    "\n",
    "        add_df_column(ldc_reg.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_model, '06i_ldc_reg.csv'), index=False)\n",
    "        add_df_column(nldc_reg.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_model, '06j_net_ldc_reg.csv'.format(chr(count+1), geo_suffix)), index=False)\n",
    "        add_df_column(nldc_orig_reg.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_model, '06k_net_ldc_orig_reg.csv'.format(chr(count+1), geo_suffix)), index=False)\n",
    "        add_df_column(nldc_orig_reg.reset_index(drop=True), 'units', 'MW').to_csv(os.path.join(save_dir_model, '06l_net_ldc_orig_reg.csv'.format(chr(count+1), geo_suffix)), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 7 : Daily gap between peak and off-peak net load\n",
    "\n",
    "# This could be future proofed with using the geo_col/suffix approach....\n",
    "\n",
    "nl_max_dly_reg = net_load_reg_ts.groupby(geo_cols[0], axis=1).sum().groupby(['model', pd.Grouper(level='timestamp', freq='D')]).max()\n",
    "nl_max_dly_subreg =net_load_reg_ts.groupby(geo_cols[1], axis=1).sum().groupby(['model', pd.Grouper(level='timestamp', freq='D')]).max()\n",
    "nl_max_dly = net_load_reg_ts.sum(axis=1).groupby(['model', pd.Grouper(level='timestamp', freq='D')]).max()\n",
    "\n",
    "nl_min_dly_reg = net_load_reg_ts.groupby(geo_cols[0], axis=1).sum().groupby(['model', pd.Grouper(level='timestamp', freq='D')]).min()\n",
    "nl_min_dly_subreg = net_load_reg_ts.groupby(geo_cols[1], axis=1).sum().groupby(['model', pd.Grouper(level='timestamp', freq='D')]).min()\n",
    "nl_min_dly = net_load_reg_ts.sum( axis=1).groupby(['model', pd.Grouper(level='timestamp', freq='D')]).min()\n",
    "\n",
    "nl_dly_gap_reg = nl_max_dly_reg - nl_min_dly_reg\n",
    "nl_dly_gap_subreg = nl_max_dly_subreg - nl_min_dly_subreg\n",
    "nl_dly_gap = nl_max_dly - nl_min_dly\n",
    "\n",
    "nl_dly_gap_reg_pc = (nl_max_dly_reg - nl_min_dly_reg)/nl_max_dly_reg\n",
    "nl_dly_gap_subreg_pc = (nl_max_dly_subreg - nl_min_dly_subreg)/nl_max_dly_subreg\n",
    "nl_dly_gap_pc = (nl_max_dly - nl_min_dly)/nl_max_dly\n",
    "\n",
    "add_df_column(nl_dly_gap_reg, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '07a_dly_gap_reg.csv'), index=False)\n",
    "add_df_column(nl_dly_gap_subreg, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '07b_dly_gap_subreg.csv'), index=False)\n",
    "add_df_column(nl_dly_gap, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '07d_dly_gap.csv'), index=False)\n",
    "\n",
    "add_df_column(nl_dly_gap_reg_pc, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '07e_dly_gap_reg_pc.csv'), index=False)\n",
    "add_df_column(nl_dly_gap_subreg_pc, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '07f_dly_gap_subreg_pc.csv'), index=False)\n",
    "add_df_column(nl_dly_gap_pc, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '07h_dly_gap_pc.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Output 8: Inertia (total)\n",
    "\n",
    "gen_units_gen = gen_df[gen_df.property == 'Units Generating'].groupby(['model', 'name','timestamp']).sum().value\n",
    "gen_units = gen_df[gen_df.property == 'Units'].groupby(['model', 'name','timestamp']).sum().value\n",
    "\n",
    "## Take only the sum to maintain the capacity value & inertia constant in the dataframe\n",
    "gen_cap = gen_df[gen_df.property == 'Installed Capacity'].groupby(['model', 'name','timestamp']).sum()\n",
    "gen_cap = pd.merge(gen_cap.reset_index(), gen_inertia_idx, on='name', how='left').set_index(['model', 'name', 'timestamp'])\n",
    "\n",
    "##  As installed capacity is [Units] * [Max Capacity], we must calculate the unit capacity\n",
    "gen_inertia_lo = (gen_units_gen/gen_units)*(gen_cap.value*gen_cap.InertiaLOW)\n",
    "gen_inertia_hi = (gen_units_gen/gen_units)*(gen_cap.value*gen_cap.InertiaHI)\n",
    "\n",
    "\n",
    "gen_inertia = pd.merge(\n",
    "    pd.DataFrame(gen_inertia_lo, columns=['InertiaLo']),\n",
    "    pd.DataFrame(gen_inertia_hi, columns=['InertiaHi']),\n",
    "    left_index=True, right_index=True)\n",
    "\n",
    "gen_inertia = pd.merge(gen_inertia.reset_index(), soln_idx[['name', geo_cols[0], geo_cols[1], 'Category', 'CapacityCategory']],\n",
    "                       on='name')\n",
    "\n",
    "inertia_by_tech = gen_inertia.groupby(['model', 'Category', 'timestamp']).sum()\n",
    "# inertia_by_isl = gen_inertia.drop.groupby(['model', geo_cols[0], 'timestamp']).sum()\n",
    "inertia_by_reg = gen_inertia.groupby(['model'] + geo_cols + ['timestamp']).sum()\n",
    "total_inertia_ts  = gen_inertia.groupby(['model', 'timestamp']).sum()\n",
    "\n",
    "add_df_column(inertia_by_tech, 'units', 'MWs').to_csv(os.path.join(save_dir_ts, '08a_inertia_by_tech_ts.csv'), index=False)\n",
    "# add_df_column(inertia_by_isl, 'units', 'MWs').to_csv(os.path.join(save_dir_ts, '08b_inertia_by_isl_ts.csv'), index=False)\n",
    "add_df_column(inertia_by_reg, 'units', 'MWs').to_csv(os.path.join(save_dir_ts, '08c_inertia_by_reg_ts.csv'), index=False)\n",
    "add_df_column(total_inertia_ts, 'units', 'MWs').to_csv(os.path.join(save_dir_ts, '08d_total_inertia_ts.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 9: Ramp time-series\n",
    "\n",
    "ramp_ts = (net_load_ts.unstack(level='model') - net_load_ts.unstack(level='model').shift(1)).fillna(\n",
    "    0).stack().sort_index(level=1).reset_index().set_index(['model', 'timestamp']).rename(columns={0:'value'})\n",
    "th_ramp_ts = (net_load_ts.unstack(level='model') - net_load_ts.unstack(level='model').shift(3)).fillna(\n",
    "    0).stack().sort_index(level=1).reset_index().set_index(['model', 'timestamp']).rename(columns={0:'value'})\n",
    "\n",
    "ramp_orig_ts = (net_load_orig_ts.unstack(level='model') - net_load_orig_ts.unstack(level='model').shift(1)).fillna(\n",
    "    0).stack().sort_index(level=1).reset_index().set_index(['model', 'timestamp']).rename(columns={0:'value'})\n",
    "th_ramp_orig_ts = (net_load_orig_ts.unstack(level='model') - net_load_orig_ts.unstack(level='model').shift(3)).fillna(\n",
    "    0).stack().sort_index(level=1).reset_index().set_index(['model', 'timestamp']).rename(columns={0:'value'})\n",
    "\n",
    "#########\n",
    "daily_pk_ts = customer_load_ts.groupby([pd.Grouper(level='model'), pd.Grouper(level='timestamp', freq='D')]).max().reindex(customer_load_ts.index).ffill()\n",
    "\n",
    "ramp_pc_ts = ramp_ts.value/daily_pk_ts*100\n",
    "th_ramp_pc_ts = th_ramp_ts.value/daily_pk_ts*100\n",
    "\n",
    "########\n",
    "ramp_by_gen_tech_ts = (gen_by_tech_ts - gen_by_tech_ts.shift(1)).fillna(0)\n",
    "ramp_by_gen_subtech_ts = (gen_by_subtech_ts - gen_by_subtech_ts.shift(1)).fillna(0)\n",
    "\n",
    "th_ramp_by_gen_tech_ts = (gen_by_tech_ts - gen_by_tech_ts.shift(3)).fillna(0)\n",
    "th_ramp_by_gen_subtech_ts = (gen_by_subtech_ts - gen_by_subtech_ts.shift(3)).fillna(0)\n",
    "#####\n",
    "\n",
    "add_df_column(ramp_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08a_ramp_ts.csv'), index=False)\n",
    "add_df_column(th_ramp_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08b_3hr_ramp_ts.csv'), index=False)\n",
    "add_df_column(ramp_pc_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08c_ramp_pc_ts.csv'), index=False)\n",
    "add_df_column(th_ramp_pc_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08d_3hr_ramp_pc_ts.csv'), index=False)\n",
    "\n",
    "add_df_column(ramp_by_gen_tech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08e_ramp_by_gen_tech_ts.csv'), index=False)\n",
    "add_df_column(ramp_by_gen_subtech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08f_ramp_by_gen_subtech_ts.csv'), index=False)\n",
    "\n",
    "\n",
    "add_df_column(th_ramp_by_gen_tech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08g_3hr_ramp_by_gen_tech_ts.csv'), index=False)\n",
    "add_df_column(th_ramp_by_gen_subtech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08h_3hr_ramp_by_gen_subtech_ts.csv'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "if reg_ts:\n",
    "    \n",
    "    ### Reg will be only the top order reg by default\n",
    "    ramp_reg_ts = (net_load_reg_ts.unstack(level='model') - net_load_reg_ts.unstack(level='model').shift(1)).fillna(\n",
    "        0).stack(level='model').sort_index(level='model').stack(geo_cols).reorder_levels(['model'] + geo_cols +['timestamp'])\n",
    "    \n",
    "    th_ramp_reg_ts = (net_load_reg_ts.unstack(level='model') - net_load_reg_ts.unstack(level='model').shift(3)).fillna(\n",
    "        0).stack(level='model').sort_index(level='model').stack(geo_cols).reorder_levels(['model'] + geo_cols +['timestamp'])\n",
    "    \n",
    "    \n",
    "    #########\n",
    "    daily_pk_reg_ts = customer_load_reg_ts.groupby([pd.Grouper(level='model'), pd.Grouper(level='timestamp', freq='D')]).max().reindex(customer_load_reg_ts.index).ffill()\n",
    "\n",
    "    ramp_reg_pc_ts = ramp_reg_ts.unstack(geo_cols)/daily_pk_reg_ts*100\n",
    "    th_ramp_reg_pc_ts = th_ramp_reg_ts.unstack(geo_cols)/daily_pk_reg_ts*100\n",
    "\n",
    "    ## These are used in later calculations\n",
    "    ramp_by_gen_reg_tech_ts = (gen_by_tech_reg_ts - gen_by_tech_reg_ts.shift(1)).fillna(0)\n",
    "    ramp_by_gen_reg_subtech_ts = (gen_by_subtech_reg_ts - gen_by_subtech_reg_ts.shift(1)).fillna(0)\n",
    "\n",
    "    th_ramp_by_gen_reg_tech_ts = (gen_by_tech_reg_ts - gen_by_tech_reg_ts.shift(3)).fillna(0)\n",
    "    th_ramp_by_gen_reg_subtech_ts = (gen_by_subtech_reg_ts - gen_by_subtech_reg_ts.shift(3)).fillna(0)    \n",
    "    \n",
    "\n",
    "    \n",
    "    add_df_column(ramp_reg_ts.unstack(geo_cols), 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08g_ramp_reg_ts.csv'), index=False)\n",
    "    add_df_column(ramp_reg_pc_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08g_ramp_reg_pc_ts.csv'), index=False)\n",
    "    add_df_column(th_ramp_reg_pc_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08g_3hr_ramp_reg_pc_ts.csv'), index=False)\n",
    "\n",
    "    add_df_column(th_ramp_reg_ts.unstack(geo_cols), 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08h_3hr_ramp_reg_ts.csv'), index=False)\n",
    "    add_df_column(ramp_by_gen_reg_tech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08i_ramp_by_gen_tech_reg_ts.csv'), index=False)\n",
    "    add_df_column(th_ramp_by_gen_reg_tech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08j_3hr_ramp_by_gen_tech_reg_ts.csv'), index=False)\n",
    "    add_df_column(ramp_by_gen_reg_subtech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08k_ramp_by_gen_subtech_reg_ts.csv'), index=False)\n",
    "    add_df_column(th_ramp_by_gen_reg_subtech_ts, 'units', 'MW.hr-1').to_csv(os.path.join(save_dir_ts, '08l_3hr_ramp_by_gen_subtech_reg_ts.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ouput 4: Costs and savings.\n",
    "\n",
    "\n",
    "gen_cost_props_w_penalty = ['Emissions Cost', 'Fuel Cost', 'Max Energy Violation Cost', 'VO&M Cost',\n",
    "                  'Min Energy Violation Cost', 'Ramp Down Cost', 'Ramp Up Cost', 'Start & Shutdown Cost']\n",
    "\n",
    "### Excludes VO&M as this is excluded for some generators because of bad scaling of the objective function\n",
    "gen_op_cost_props = ['Emissions Cost', 'Fuel Cost', 'Start & Shutdown Cost', 'VO&M Cost']\n",
    "\n",
    "gen_op_cost_props_wo_co2 = ['Fuel Cost', 'Start & Shutdown Cost', 'VO&M Cost']\n",
    "\n",
    "\n",
    "### Standard costs reported as USD'000\n",
    "gen_op_costs = gen_yr_df[gen_yr_df.property.isin(gen_op_cost_props)]\n",
    "gen_op_costs_wo_co2 = gen_yr_df[gen_yr_df.property.isin(gen_op_cost_props_wo_co2)]\n",
    "gen_op_costs_w_pen = gen_yr_df[gen_yr_df.property.isin(gen_cost_props_w_penalty)]\n",
    "\n",
    "\n",
    "try:\n",
    "    ### Scale costs to be also USD'000\n",
    "    gen_vom = gen_yr_df[gen_yr_df.property == 'Generation'].fillna(0)\n",
    "    gen_vom.loc[:,'value'] = gen_vom.apply(lambda x: x.value*x.VOM, axis=1).fillna(0)\n",
    "    gen_vom.loc[:,'property'] = 'VO&M Cost'\n",
    "except:\n",
    "    gen_vom = pd.DataFrame(None)\n",
    "\n",
    "try:\n",
    "    gen_fom = gen_yr_df.loc[gen_yr_df.property == 'Installed Capacity',:]\n",
    "    gen_fom.loc[:,'value'] = gen_fom.apply(lambda x: x.value*x.FOM, axis=1).fillna(0)\n",
    "    gen_fom.loc[:,'property'] = 'FO&M Cost'\n",
    "except:\n",
    "    gen_fom = pd.DataFrame(None)\n",
    "\n",
    "try:\n",
    "    gen_capex = gen_yr_df.loc[gen_yr_df.property == 'Installed Capacity',:]\n",
    "    gen_capex.loc[:,'value'] = gen_capex.apply(lambda x: x.value*x.CAPEX_2037, axis=1).fillna(0)\n",
    "    gen_capex.loc[:,'property'] = 'Investment Cost'\n",
    "    \n",
    "    gen_capex_2030 = gen_yr_df.loc[gen_yr_df.property == 'Installed Capacity',:]\n",
    "    gen_capex_2030.loc[:,'value'] = gen_capex_2030.apply(lambda x: x.value*x.CAPEX_2030, axis=1).fillna(0)\n",
    "    gen_capex_2030.loc[:,'property'] = 'Investment Cost 2030'\n",
    "except:\n",
    "    gen_capex = pd.DataFrame(None)\n",
    "\n",
    "    \n",
    "### Total costs for 2030 and 2037 based on different investment costs. This is very poorly implemented right now\n",
    "gen_total_costs = pd.concat([gen_op_costs, gen_vom, gen_fom, gen_capex], axis=0)\n",
    "gen_total_costs_2030 = pd.concat([gen_op_costs, gen_vom, gen_fom, gen_capex_2030], axis=0)\n",
    "gen_total_costs_w_pen = pd.concat([gen_op_costs_w_pen, gen_vom, gen_fom, gen_capex], axis=0)\n",
    "gen_total_costs_w_pen_2030 = pd.concat([gen_op_costs_w_pen, gen_vom, gen_fom, gen_capex_2030], axis=0)\n",
    "\n",
    "\n",
    "gen_op_costs = pd.concat([gen_op_costs, gen_vom], axis=0)\n",
    "gen_op_cost_wo_co2 = pd.concat([gen_op_costs_wo_co2, gen_vom], axis=0)\n",
    "\n",
    "### Scale to USDm\n",
    "gen_op_costs_by_reg = gen_op_costs.groupby(\n",
    "    ['model']+geo_cols+ ['Category', 'property']).sum().value/1e3\n",
    "gen_op_costs_wo_co2_by_reg = gen_op_cost_wo_co2.groupby(\n",
    "    ['model']+geo_cols+ ['Category', 'property']).sum().value/1e3\n",
    "\n",
    "\n",
    "### Total costs for 2030 and 2037 based on different investment costs. This is very poorly implemented right now\n",
    "gen_total_costs_by_reg = gen_total_costs.groupby(\n",
    "    ['model']+geo_cols+ ['Category', 'property']).sum().value/1e3\n",
    "gen_total_costs_by_reg_w_pen = gen_total_costs_w_pen.groupby(\n",
    "    ['model']+geo_cols+ ['Category', 'property']).sum().value/1e3\n",
    "###\n",
    "gen_total_costs_by_reg_2030 = gen_total_costs_2030.groupby(\n",
    "    ['model']+geo_cols+ ['Category', 'property']).sum().value/1e3\n",
    "gen_total_costs_by_reg_w_pen_2030 = gen_total_costs_w_pen_2030.groupby(\n",
    "    ['model']+geo_cols+ ['Category', 'property']).sum().value/1e3\n",
    "\n",
    "\n",
    "\n",
    "##TOP an other violation costs\n",
    "    \n",
    "#### Non-standard cost elements (i.e. ToP, min CF violations, etc.)\n",
    "if fuelcontract_yr_df.shape[0] > 0:\n",
    "    top_cost_reg = fuelcontract_yr_df[fuelcontract_yr_df.property == 'Take-or-Pay Violation Cost']\n",
    "    top_cost_reg.loc[:,'Category'] = 'Gas'\n",
    "    top_cost_reg.loc[:,'property'] = 'ToP Violation Cost'\n",
    "    top_cost_reg.loc[:,geo_cols] = 'CAC'  ### As there is no region!!    \n",
    "    top_cost_reg = top_cost_reg.groupby(['model'] + geo_cols + ['Category','property']).sum().value/1e3\n",
    "    top_cost_reg = top_cost_reg.reset_index()\n",
    "    \n",
    "else:\n",
    "    top_cost_reg = pd.DataFrame(None)\n",
    "    \n",
    "#### Cost of minimum energy violation for coal contracts\n",
    "\n",
    "gen_min_vio_coal_reg = gen_yr_df[(gen_yr_df.property == 'Min Energy Violation') & (gen_yr_df.Category == 'Coal')].groupby(['model'] + geo_cols + ['Category','timestamp']).sum().value  ## GWh\n",
    "gen_avg_price_coal_reg = gen_yr_df[(gen_yr_df.property == 'Average Cost')& (gen_yr_df.Category == 'Coal')].groupby(['model'] + geo_cols + ['Category','timestamp']).sum().value ## $/Mwh\n",
    "gen_min_vio_cost_coal_reg = gen_min_vio_coal_reg * gen_avg_price_coal_reg ### $'000\n",
    "gen_min_vio_cost_coal_reg = gen_min_vio_cost_coal_reg.reset_index()\n",
    "gen_min_vio_cost_coal_reg.loc[:,'property'] = 'PPA Violation Cost'\n",
    "gen_min_vio_cost_coal_reg = gen_min_vio_cost_coal_reg.groupby(['model'] + geo_cols + ['Category', 'property']).sum().value/1e3\n",
    "\n",
    "### Ramp costs by reg in USDm\n",
    "gen_by_name_ts = gen_df[gen_df.property=='Generation'].set_index(['model', 'name'] +geo_cols + ['Category','timestamp']).value\n",
    "ramp_by_gen_name = (gen_by_name_ts-gen_by_name_ts.shift(1)).fillna(0)\n",
    "ramp_costs_by_gen_name = pd.merge(ramp_by_gen_name.reset_index(), gen_cost_idx[['name', 'RampCost']], on='name', how='left').set_index(['model', 'name'] +geo_cols + ['Category','timestamp'])\n",
    "ramp_costs_by_gen_name.loc[:,'value'] = (ramp_costs_by_gen_name.value.abs()*ramp_costs_by_gen_name.RampCost.fillna(0))\n",
    "ramp_costs_by_gen_name.loc[:,'property'] = 'Ramp Cost'\n",
    "gen_ramp_costs_by_reg = ramp_costs_by_gen_name.reset_index().groupby(['model'] + geo_cols +['Category', 'property']).sum().value/1e6\n",
    "gen_op_costs_by_reg = pd.concat([gen_op_costs_by_reg, gen_ramp_costs_by_reg], axis=0).reset_index().groupby(['model'] + geo_cols +['Category', 'property']).sum().value\n",
    "gen_total_costs_by_reg = pd.concat([gen_total_costs_by_reg, gen_ramp_costs_by_reg], axis=0).reset_index().groupby(['model'] + geo_cols +['Category', 'property']).sum().value\n",
    "gen_total_costs_by_reg_w_pen = pd.concat([gen_total_costs_by_reg_w_pen, gen_ramp_costs_by_reg], axis=0).reset_index().groupby(['model'] + geo_cols +['Category', 'property']).sum().value\n",
    "\n",
    "\n",
    "# ### Final dataframes of costs with ToP etc\n",
    "gen_op_and_vio_costs_reg = pd.concat([gen_op_costs_by_reg.reset_index(), top_cost_reg ], axis=0).groupby(['model'] + geo_cols +['Category', 'property']).sum().value\n",
    "gen_op_and_vio_costs_reg_wo_co2 = pd.concat([gen_op_costs_wo_co2_by_reg.reset_index(), top_cost_reg ], axis=0).groupby(['model'] + geo_cols +['Category', 'property']).sum().value\n",
    "\n",
    "\n",
    "### this is different from the vio costs as it includes actual penalty costs as per the model (which are much higher than real violation costs due to modelling considerations)\n",
    "\n",
    "\n",
    "### USDm/GWh = USD'000/MWh\n",
    "lcoe_by_tech_reg = (gen_total_costs_by_reg.groupby(['model']+geo_cols+ ['Category']).sum().unstack(\n",
    "    geo_cols).fillna(0)/gen_by_tech_reg).fillna(0).stack(geo_cols).reorder_levels(['model']+geo_cols+ ['Category'])*1000\n",
    "lcoe_tech = gen_total_costs_by_reg.groupby(['model', 'Category']).sum()/gen_by_tech_reg.stack(geo_cols).groupby(['model', 'Category']).sum()*1000\n",
    "\n",
    "\n",
    "add_df_column(gen_op_costs_by_reg, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04a_gen_op_costs_reg.csv'),index=False)\n",
    "add_df_column(gen_op_costs_wo_co2_by_reg, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04b_gen_op_costs_wo_co2_reg.csv'),index=False)\n",
    "\n",
    "add_df_column(gen_op_and_vio_costs_reg, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04c_gen_op_and_vio_costs_reg.csv'),index=False)\n",
    "add_df_column(gen_op_and_vio_costs_reg_wo_co2, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04d_gen_op_and_vio_costs_wo_co2_reg.csv'),index=False)\n",
    "\n",
    "\n",
    "add_df_column(gen_total_costs_by_reg, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04e_gen_total_costs_reg.csv'),index=False)\n",
    "add_df_column(gen_total_costs_by_reg_w_pen, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04f_gen_total_costs_reg_w_penalty_costs.csv'),index=False)\n",
    "add_df_column(gen_total_costs_by_reg_2030, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04e_gen_total_costs_reg_2030.csv'),index=False)\n",
    "add_df_column(gen_total_costs_by_reg_w_pen_2030, 'units', 'USDm').to_csv(os.path.join(save_dir_sum, '04f_gen_total_costs_reg_w_penalty_costs_2030.csv'),index=False)\n",
    "\n",
    "\n",
    "add_df_column(lcoe_by_tech_reg, 'units', 'USD/MWh').to_csv(os.path.join(save_dir_sum, '04g_lcoe_tech_reg.csv'),index=False)\n",
    "add_df_column(lcoe_tech, 'units', 'USD/MWh').to_csv(os.path.join(save_dir_sum, '04h_lcoe_tech.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 10: Generation out-of-service \n",
    "### Its noted that units out was done per generator. Unsure why, but this may be a more useful output\n",
    "\n",
    "gen_out_tech_ts = gen_df[(gen_df.property == 'Forced Outage') | \\\n",
    "                    (gen_df.property == 'Maintenance')].groupby(\n",
    "                    ['model', 'Category', 'timestamp']).sum().value.unstack(level='Category')\n",
    "\n",
    "gen_out_by_type_ts = gen_df[(gen_df.property == 'Forced Outage') | \\\n",
    "                    (gen_df.property == 'Maintenance')].groupby(\n",
    "                    ['model', 'property', 'timestamp']).sum().value.unstack(level='property')\n",
    "\n",
    "add_df_column(gen_out_tech_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '10a_outages_by_tech_ts.csv'), index=False)\n",
    "add_df_column(gen_out_by_type_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '10b_outages_by_outtype_ts.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 11a: Calculate days of interest & values\n",
    "### This being max avg net load, min avg net load, max net/total load,, min net/total load, max ramp, min inertia \n",
    "\n",
    "net_load_avg = net_load_ts.unstack(level='model').resample('D').mean().stack().reorder_levels(['model', 'timestamp'])\n",
    "### Time dataframes done nationally////    \n",
    "\n",
    "try:\n",
    "    wet_season = gen_by_tech_ts.loc[ix[m,:]]['Hydro'].groupby([pd.Grouper(level='timestamp',freq='M')]).sum().idxmax()\n",
    "    dry_season = gen_by_tech_ts.loc[ix[m,:]]['Hydro'].groupby([pd.Grouper(level='timestamp',freq='M')]).sum().idxmin()\n",
    "except KeyError:\n",
    "    wet_season = gen_by_tech_ts.loc[ix[m,:]]['RoR'].groupby([pd.Grouper(level='timestamp',freq='M')]).sum().idxmax()\n",
    "    dry_season = gen_by_tech_ts.loc[ix[m,:]]['RoR'].groupby([pd.Grouper(level='timestamp',freq='M')]).sum().idxmin()\n",
    "    \n",
    "hi_vre_season  = gen_by_tech_ts.loc[ix[m,:]][['Wind', 'Solar']].sum(axis=1).groupby( pd.Grouper(level='timestamp',freq='M')).idxmax()\n",
    "low_vre_seaspm =  gen_by_tech_ts.loc[ix[m,:]][['Wind', 'Solar']].sum(axis=1).groupby( [ pd.Grouper(level='timestamp',freq='M')]).sum().idxmin()\n",
    "\n",
    "\n",
    "##########\n",
    "net_load_avg_max = net_load_avg.groupby(level='model').max().rename(columns={'value':'net_load_avg_max'})\n",
    "net_load_avg_min = net_load_avg.groupby(level='model').min().rename(columns={'value':'net_load_avg_min'})\n",
    "net_load_max = net_load_ts.groupby(level='model').max().rename(columns={'value':'net_load_max'})\n",
    "net_load_min = net_load_ts.groupby(level='model').min().rename(columns={'value':'net_load_min'})\n",
    "# net_load_sto_min = net_load_sto_ts.groupby(level='model').min().rename(columns={'value':'net_load_sto_min'})\n",
    "curtail_max = vre_curtailed_reg_ts.sum(axis=1).groupby(level='model').max().rename('curtail_max').to_frame()\n",
    "\n",
    "net_load_max_wet = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == wet_season.month].groupby(level='model').max().rename(columns={'value':'net_load_max_wet'})\n",
    "net_load_min_wet = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == wet_season.month].groupby(level='model').min().rename(columns={'value':'net_load_min_wet'})\n",
    "net_load_max_dry = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == dry_season.month].groupby(level='model').max().rename(columns={'value':'net_load_max_dry'})\n",
    "net_load_min_dry = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == wet_season.month].groupby(level='model').min().rename(columns={'value':'net_load_min_dry'})\n",
    "total_load_max = total_load_ts.groupby(level='model').max().rename('total_load_max').to_frame()\n",
    "total_load_min = total_load_ts.groupby(level='model').min().rename('total_load_min').to_frame()\n",
    "ramp_max = ramp_ts.groupby(level='model').max().rename(columns={'value':'ramp_max'})\n",
    "inertia_min = total_inertia_ts.groupby(level='model').min().drop(columns='InertiaHi').rename(columns={'InertiaLo':'inertia_min'})\n",
    "use_max = use_ts.groupby(level='model').max().rename('use_max').to_frame()\n",
    "use_dly_max = use_dly_ts.groupby(level='model').max().rename('use_dly_max').to_frame()\n",
    "\n",
    "# ###########\n",
    "net_load_max['time_nlmax'] = net_load_ts.unstack(level='model').idxmax().values\n",
    "net_load_min['time_nlmin'] = net_load_ts.unstack(level='model').idxmin().values\n",
    "# net_load_sto_min['time_nlstomin'] = net_load_sto_ts.unstack(level='model').idxmin().values\n",
    "curtail_max['time_curtailmax'] = vre_curtailed_reg_ts.sum(axis=1).unstack(level='model').idxmax().values\n",
    "\n",
    "net_load_max_wet['time_nlmax_wet'] = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == wet_season.month].unstack(level='model').idxmax().values\n",
    "net_load_min_wet['time_nlmin_wet'] = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == wet_season.month].unstack(level='model').idxmin().values\n",
    "net_load_max_dry['time_nlmax_dry'] = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == dry_season.month].unstack(level='model').idxmax().values\n",
    "net_load_min_dry['time_nlmin_dry'] = net_load_ts[net_load_ts.index.get_level_values('timestamp').month == dry_season.month].unstack(level='model').idxmin().values\n",
    "net_load_avg_max['time_nlamax'] = net_load_avg.unstack(level='model').idxmax().values\n",
    "net_load_avg_min['time_nlamin'] = net_load_avg.unstack(level='model').idxmin().values\n",
    "total_load_max['time_tlmax'] = total_load_ts.unstack(level='model').idxmax().values\n",
    "total_load_min['time_tlmin'] = total_load_ts.unstack(level='model').idxmin().values\n",
    "ramp_max['time_ramp'] = ramp_ts.unstack(level='model').idxmax().values\n",
    "inertia_min['time_H'] = total_inertia_ts.InertiaLo.unstack(level='model').idxmin().values\n",
    "use_max['time_usemax'] = use_ts.unstack(level='model').idxmax().values\n",
    "use_dly_max['time_usedmax'] = use_dly_ts.unstack(level='model').idxmax().values\n",
    "# ############\n",
    "\n",
    "\n",
    "doi_summary = pd.concat([net_load_max, net_load_min, curtail_max, net_load_avg_max, net_load_avg_min, net_load_max_wet, net_load_min_wet, net_load_max_dry, net_load_min_dry,\n",
    "                        total_load_max, total_load_min, ramp_max, inertia_min, use_max, use_dly_max], # net_load_sto_min,\n",
    "                        axis=1).stack().unstack(level='model').rename_axis('property', axis=0)\n",
    "\n",
    "\n",
    "doi_summary.to_csv(os.path.join(save_dir_ts, '11a_days_of_interest_summary.csv'), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Output 12_NEW: Import/export by region\n",
    "\n",
    "# node_col = geo_cols[1]\n",
    "# node_idx = soln_idx[soln_idx.Object_type == 'Node'][geo_cols].set_index(node_col)\n",
    "\n",
    "\n",
    "# ### Series with indices matching the columns of the DF for filling in missing columns\n",
    "# reg_filler = pd.Series(data=[1]*len(node_idx.index), index=node_idx.index)\n",
    " \n",
    "\n",
    "# line_flows = line_df[line_df.property == 'Flow']\n",
    "# line_imports = line_flows.copy()\n",
    "# line_exports = line_flows.copy()\n",
    "\n",
    "\n",
    "\n",
    "# line_imports.loc[:,'value'] = [-v if v <=0 else 0 for v in line_imports.value.values  ]\n",
    "# line_exports.loc[:,'value'] = [v if v > 0 else 0 for v in line_exports.value.values  ]\n",
    "# imports_by_reg = line_imports.groupby(['model', 'regTo', 'timestamp']).sum().value.reset_index().rename(columns={'regTo':node_col}).set_index(['model',node_col, 'timestamp']).unstack(node_col).droplevel(0,axis=1)\n",
    "# exports_by_reg = line_exports.groupby(['model', 'regFrom', 'timestamp']).sum().value.reset_index().rename(columns={'regFrom':node_col}).set_index(['model',node_col, 'timestamp']).unstack(node_col).droplevel(0,axis=1)\n",
    "\n",
    "# imports_by_reg = (imports_by_reg*reg_filler).fillna(0).stack().reset_index()\n",
    "# exports_by_reg = (exports_by_reg*reg_filler).fillna(0).stack().reset_index()\n",
    "\n",
    "# ## There will be periods where nodes are both importing and exporting simultaneously\n",
    "# imports_by_reg = pd.merge(imports_by_reg, node_idx, left_on=node_col, right_index=True).set_index(['model'] + geo_cols + ['timestamp']).rename(columns={0:'Imports'})\n",
    "# exports_by_reg = pd.merge(exports_by_reg, node_idx, left_on=node_col, right_index=True).set_index(['model'] + geo_cols + ['timestamp']).rename(columns={0:'Exports'})\n",
    "\n",
    "# imp_exp_by_reg = pd.concat([imports_by_reg.rename(columns={'value':'Imports'}), \n",
    "#                           exports_by_reg.rename(columns={'value':'Exports'})], axis=1)\n",
    "\n",
    "\n",
    "# if reg_ts:\n",
    "#     for m in model_names:\n",
    "#         save_dir_model = os.path.join(save_dir_ts, m)\n",
    "#         if os.path.exists(save_dir_model) is False:\n",
    "#             os.mkdir(save_dir_model)\n",
    "            \n",
    "#         add_df_column(imp_exp_by_reg.loc[ix[m,]], 'units', '%').to_csv(\n",
    "#             os.path.join(save_dir_model, '12c_imports_exports_by_reg.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Output 12: Time-series (daily) on loading and flows (absolute) on Tx corrdiors\n",
    "\n",
    "\n",
    "# line_flows = line_df[line_df.property == 'Flow'].groupby(\n",
    "#     ['model', 'regTo', 'regFrom', 'timestamp']).sum().loc[:,['value']]\n",
    "\n",
    "# ### Seperate imports from exports\n",
    "# ### Currently done per line and not at region as the Exports/Imports dont register without Resistance/Reactance included (?)\n",
    "\n",
    "\n",
    "# line_imports = line_flows.copy()\n",
    "# line_exports = line_flows.copy()\n",
    "\n",
    "# line_imports.loc[:,'value'] = [-v if v <=0 else 0 for v in line_flows.value.values  ]\n",
    "# line_exports.loc[:,'value'] = [v if v > 0 else 0 for v in line_flows.value.values  ]\n",
    "\n",
    "# line_flows = pd.merge(line_imports.rename(columns={'value':'Imports'}), \n",
    "#                         line_exports.rename(columns={'value':'Exports'}),\n",
    "#                         left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "# line_loading_exp = line_exports.value/line_cap['Export Limit']*100\n",
    "# line_loading_imp = line_imports.value/np.abs(line_cap['Import Limit'])*100\n",
    "# line_loading =  line_loading_imp + line_loading_exp\n",
    "\n",
    "# line_flows_daily = line_flows.reset_index().groupby(\n",
    "#     ['model', 'regTo', 'regFrom', pd.Grouper(key='timestamp', freq='D')]).mean()\n",
    "\n",
    "# line_loading_daily = line_loading.reset_index().groupby(\n",
    "#     ['model', 'regTo', 'regFrom', pd.Grouper(key='timestamp', freq='D')]).mean()\n",
    "\n",
    "# add_df_column(line_flows_daily, 'units', 'MW').to_csv(\n",
    "#     os.path.join(save_dir_ts, '12a_line_flow_daily_ts.csv'), index=False)\n",
    "# add_df_column(line_loading_daily, 'units', '%').to_csv(\n",
    "#     os.path.join(save_dir_ts, '12b_line_loading_daily_ts.csv'), index=False)\n",
    "\n",
    "\n",
    "# for m in model_names:\n",
    "    \n",
    "#     save_dir_model = os.path.join(save_dir_ts, m)\n",
    "#     if os.path.exists(save_dir_model) is False:\n",
    "#         os.mkdir(save_dir_model)\n",
    "        \n",
    "#     model_line_loading = line_loading.reset_index()[line_loading.reset_index().model == m]\n",
    "#     model_line_flows = line_flows.reset_index()[line_flows.reset_index().model == m]\n",
    "\n",
    "#     add_df_column(line_flows, 'units', 'MW').reset_index().to_csv(os.path.join(save_dir_model, '17a_line_flow_ts.csv'))\n",
    "#     add_df_column(line_flows, 'units', '%').reset_index().to_csv(os.path.join(save_dir_model, '17a_line_loading_ts.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Output 14: Operational reserves\n",
    "# ### This could be soemwhere in the SolnIdex....\n",
    "# res_df.loc[:,'Type'] = res_df.name.str.split('_').str[0]\n",
    "\n",
    "# res_ts = res_df[res_df.property == 'Risk']\n",
    "# res_ts.groupby(['model','Type','timestamp']).sum().value.unstack(level='Type')\n",
    "# res_shorage_ts = res_df[res_df.property == 'Shortage'].groupby(['model','Type','timestamp']).sum().value.unstack(level='Type')\n",
    "\n",
    "# ## As solution index has the info for generators, not reserves\n",
    "# res_gen_df['ResType'] = res_gen_df.parent.str.split('_').str[0]\n",
    "\n",
    "# # reserve_by_reg_ts = res_df.groupby(['model','name','property','timestamp']).sum().value.unstack(level='property')\n",
    "\n",
    "\n",
    "# spinres_av_ts = res_gen_df[((res_gen_df.property == 'Available Response')| (res_gen_df.property == 'Provision'))& (res_gen_df.ResType == 'Spinning')].groupby(\n",
    "#     ['model','CapacityCategory', 'Category','timestamp']).sum().value\n",
    "\n",
    "# regres_av_ts = res_gen_df[((res_gen_df.property == 'Available Response')| (res_gen_df.property == 'Provision'))&(res_gen_df.ResType == 'Regulating')].groupby(\n",
    "#    ['model','CapacityCategory', 'Category','timestamp']).sum().value\n",
    "\n",
    "# spinres_prov_ts = res_gen_df[(res_gen_df.property == 'Provision')& (res_gen_df.ResType == 'Spinning')].groupby(\n",
    "#     ['model','CapacityCategory', 'Category','timestamp']).sum().value\n",
    "\n",
    "# regres_prov_ts = res_gen_df[(res_gen_df.property == 'Provision')&(res_gen_df.ResType == 'Regulating')].groupby(\n",
    "#    ['model','CapacityCategory', 'Category','timestamp']).sum().value\n",
    "\n",
    "\n",
    "\n",
    "# add_df_column(res_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '14a_reserve_ts.csv'), index=False)\n",
    "# add_df_column(res_shorage_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '14b_reserve_shortage_ts.csv'), index=False)\n",
    "# add_df_column(spinres_av_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '14c_spinres_av_tech_ts.csv'), index=False)\n",
    "# add_df_column(regres_av_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '14d_regres_av_tech_ts.csv'), index=False)\n",
    "# add_df_column(spinres_prov_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '14e_spinres_prov_tech_ts.csv'), index=False)\n",
    "# add_df_column(regres_prov_ts, 'units', 'MW').to_csv(os.path.join(save_dir_ts, '14f_regres_prov_tech_ts.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Output 15: Services by technology\n",
    "# '''\n",
    "# property\n",
    "\n",
    "# Generation\n",
    "# InertiaContribution\n",
    "# PeakContribution\n",
    "# PeakContribution_actual\n",
    "# Regulating reserve\n",
    "# Spinning reserve\n",
    "# UpRampContribution\n",
    "# '''\n",
    "\n",
    "# service_tech_idx = pd.read_excel('Z:/Indonesia/2021_IPSE/09_ModellingSupportFiles/figure_templates/fig00_services_template.xlsx', sheet_name='tech_idx')\n",
    "\n",
    "\n",
    "# for m in model_names:\n",
    "    \n",
    "#     ### All subtechs for consistent solution size/columns\n",
    "#     subtechs = soln_idx[soln_idx.Object_type == 'Generator'].CapacityCategory.unique()\n",
    "#     services_out = pd.DataFrame(index=subtechs).rename_axis('Technology')\n",
    "\n",
    "#     save_dir_model = os.path.join(save_dir_ts, m)\n",
    "    \n",
    "\n",
    "# ## A. Energy contribution\n",
    "#     energy_contr = gen_yr_df[(gen_yr_df.property == 'Generation')&(gen_yr_df.model == m)].groupby('CapacityCategory').sum().value\n",
    "\n",
    "# ## B. Inertia\n",
    "#     ### Calc. lowest 100 inertia periods\n",
    "\n",
    "#     total_inertia = inertia_by_reg.loc[ix[m,]].groupby(level='timestamp').sum().InertiaLo\n",
    "#     inertia_100 = total_inertia.nsmallest(n=100)\n",
    "#     stability_100 = total_inertia.nsmallest(n=100)\n",
    "\n",
    "#     inertia_by_tech100 = gen_inertia.groupby(['model','CapacityCategory','timestamp']).sum().loc[ix[m,]].InertiaLo.unstack(\n",
    "#         'CapacityCategory').loc[inertia_100.index]\n",
    "#     inertia_contr = inertia_by_tech100.sum(axis=0)\n",
    "    \n",
    "# ### C. Peak contribution\n",
    "#     ## Calc 100 top periods\n",
    "#     vre_techs = ['Solar', 'Wind']\n",
    "# #     gen_by_subtech_ts = gen_df[gen_df.property == 'Generation'].groupby(['model','CapacityCategory','timestamp']).sum().loc[ix[m,]]\n",
    "#     non_vre_techs = [c for c in gen_by_subtech_ts.columns if c not in vre_techs ]\n",
    "#     load_100 = customer_load_orig_ts.loc[ix[m,:]].nlargest(100)\n",
    "#     netload_100 = net_load_orig_ts.loc[ix[m,:]].value.nlargest(100)\n",
    "    \n",
    "\n",
    "#     ### NA should be filled up top instead\n",
    "#     pk_contr = gen_by_subtech_ts.fillna(0).loc[ix[m,netload_100.index],:].droplevel(0).reset_index(drop=True)\n",
    "#     pk_contr = pk_contr.mean()\n",
    "#     pk_contr.loc[vre_techs] = 0\n",
    "#     vre_pk_contr = pd.Series(data=np.mean(load_100.values-netload_100.values), index = ['VRE'])\n",
    "#     dsm_pk_contr = pd.Series(data=np.mean(net_load_orig_ts.loc[ix[m,:]].value.nlargest(100) - net_load_ts.loc[ix[m,:]].value.nlargest(100)), index=['DSM'])\n",
    "#     pk_contr = pd.concat([pk_contr, vre_pk_contr, dsm_pk_contr])\n",
    "   \n",
    "# #     ### DSM would go here too!\n",
    "# #     pk_contr = pd.merge(non_vre_pk_contr, vre_pk_contr, left_index=True, right_index=True)\n",
    "# #     pk_contr = pk_contr.sum()/pk_contr.sum().sum()\n",
    "    \n",
    "# ### D. Reserves\n",
    "#     spin_res_contr = spinres_prov_ts.groupby(['model','CapacityCategory']).sum().loc[ix[m,:],].droplevel(0)\n",
    "#     spin_res_av_ann_contr = spinres_av_ts.groupby(['model','CapacityCategory']).sum().loc[ix[m,:],].droplevel(0)\n",
    "#     spin_res_contr = spin_res_contr.mask(spin_res_contr<0).fillna(0)\n",
    "#     spin_res_av_ann_contr = spin_res_av_ann_contr.mask(spin_res_av_ann_contr<0).fillna(0)\n",
    "        \n",
    "#     ## 100 most difficult periods for stability/reserves = highest net load ==> systen is the most strained \n",
    "#     spinres_av100 = spinres_av_ts.loc[ix[:,:,:,netload_100.index]]\n",
    "#     spinres_av100 = spinres_av100.mask(spinres_av100<0).fillna(0)\n",
    "#     spin_res_av_contr = spinres_av100.groupby(['model','CapacityCategory']).sum().loc[ix[m,:],].droplevel(0)\n",
    "\n",
    "\n",
    "\n",
    "#     ### Validation model doesnt have reg reserves so, this avoids this from breaking\n",
    "#     try:\n",
    "#         reg_res_contr = regres_prov_ts.groupby(['model','CapacityCategory']).sum().loc[ix[m,:],].droplevel(0)\n",
    "#         reg_res_av_ann_contr = regres_av_ts.groupby(['model','CapacityCategory']).sum().loc[ix[m,:],].droplevel(0)\n",
    "#         ### Some bug?\n",
    "#         reg_res_contr = reg_res_contr.mask(reg_res_contr<0).fillna(0)\n",
    "#         reg_res_av_ann_contr = reg_res_av_ann_contr.mask(reg_res_av_contr<0).fillna(0)\n",
    "        \n",
    "#         ## 100 most difficult periods for stability/reserves = lowest inertia periods    \n",
    "#         regres_av100 = regres_av_ts.loc[ix[:,:,:,netload_100.index]]\n",
    "#         regres_av100 = regres_av100.mask(regres_av100<0).fillna(0)\n",
    "#         reg_res_av_contr = reg_res_av_contr.groupby(['model','CapacityCategory']).sum().loc[ix[m,:],].droplevel(0)\n",
    "#     except KeyError:\n",
    "#         reg_res_contr = pd.Series(data=np.zeros(len(spin_res_contr)), index = spin_res_contr.index)\n",
    "#         reg_res_av_ann_contr = pd.Series(data=np.zeros(len(spin_res_av_contr)), index = spin_res_av_contr.index)\n",
    "#         reg_res_av_contr = pd.Series(data=np.zeros(len(spin_res_av_contr)), index = spin_res_av_contr.index)\n",
    "        \n",
    "    \n",
    "# ### E. Upward ramp contribution\n",
    "#     ramp_100 = ramp_ts.loc[ix[m,:],:].droplevel(0).value.nlargest(int(0.1*8760*hour_corr))\n",
    "#     ramp_contr = ramp_by_gen_subtech_ts.loc[ix[m,ramp_100.index],:].droplevel(0)\n",
    "#     ramp_contr = ramp_contr.mask(ramp_contr < 0).fillna(0).mean()\n",
    "#     dsm_ramp_contr = pd.Series(data=np.mean(ramp_orig_ts.loc[ix[m,:],:].droplevel(0).value.nlargest(int(0.1*8760*hour_corr)) - ramp_100), index=['DSM'])\n",
    "#     ramp_contr = pd.concat([ramp_contr, dsm_ramp_contr])\n",
    "    \n",
    "# #     services_out = pd.concat([energy_contr, inert])\n",
    "\n",
    "# ### Out\n",
    "\n",
    "#     services_out = pd.concat([services_out, energy_contr.rename('Energy'), pk_contr.rename('Peak'), ramp_contr.rename('UpwardRamps'),\n",
    "#                           spin_res_contr.rename('SpinRes'), reg_res_contr.rename('RegRes'), inertia_contr.rename('Inertia'),\n",
    "#                               spin_res_av_contr.rename('SpinResAv'), reg_res_av_contr.rename('RegResAv'), ], axis=1).fillna(0).T\n",
    "    \n",
    "#     services_out.reset_index().rename(columns={'index':'property'}).to_csv(os.path.join(save_dir_model, '15_services_fig.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import calmap\n",
    "ix = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calplot\n",
    "import numpy as np; np.random.seed(sum(map(ord, 'calplot')))\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_plots = os.path.join(save_dir, 'plots/')\n",
    "if os.path.exists(save_dir_plots) is False:\n",
    "            os.mkdir(save_dir_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plot functions and palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import colors\n",
    "\n",
    "\n",
    "iea_palette = {'grey5': '#f2f2f2','grey10':'#e6e6e6','pl':'#b187ef', 'bl':'#49d3ff', 'tl':'#00e0e0', 'gl':'#68f394', 'yl':'#fff45a',\n",
    "               'ol':'#ffb743', 'rl':'#ff684d', 'gl':'#68f394', 'yl':'#fff45a','grey40':'#949494','grey50':'#6f6f6f',\n",
    "               'p':'#af6ab1', 'b':'#3e7ad3', 't':'#00ada1', 'g':'#1dbe62', 'y':'#fed324',\n",
    "               'o':'#f1a800', 'r':'#e34946', 'grey20':'#afafaf', 'black':'#000000', 'white':'#ffffff', 'iea_b':'#0044ff', 'iea_b50':'#80a2ff' }\n",
    "\n",
    "extended_palette = dict( {'{}'.format(i): plt.matplotlib.colors.rgb2hex(plt.cm.get_cmap('tab20b').colors[i]) for i in np.arange(0,20)},\n",
    "                        **{'{}'.format(i+20): plt.matplotlib.colors.rgb2hex(plt.cm.get_cmap('tab20c').colors[i]) for i in np.arange(0,20)})\n",
    "\n",
    "### For overflow, i.e. things that go beyond the 16 or so colors of IEA palette. \n",
    "iea_palette_plus = dict(iea_palette, **extended_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tech_palette = { 'Coal':'grey20', 'Abated coal':'grey10', 'Cofiring':'grey10','Gas':'p', 'Abated gas':'p', 'Oil':'grey50','Hydro':'bl','Geothermal':'r','Bioenergy':'gl','Solar':'y','Wind':'g', 'Fuel Cell':'t', 'Other':'t', 'Battery':'b','Storage':'b'}\n",
    "\n",
    "# model_palette = {'2019':'rl', '2025 Base':'o', '2025 SolarPlus':'bl', '2025 SolarPlus Lite':'pl',  '2025 SolarPlus Extra':'gl' }\n",
    "\n",
    "\n",
    "### We should add load vs customer load (to see charging effect),\n",
    "### Similarly could add load + exports to get effective load\n",
    "stack_palette = {'Geothermal':'o','Bioenergy':'gl', 'Coal':'grey20', 'Cofiring':'grey5', 'Abated coal':'grey5', 'Gas':'p', 'Abated gas':'pl', 'Hydro':'bl', 'Oil':'grey50', 'Imports':'t', 'Other':'t', 'Fuel Cell':'tl', 'Storage':'b',\n",
    "                 'Solar':'y','Wind':'g', 'Total Load':'black', 'Load2':'white', 'Exports':'p','Net Load': 'r', 'Curtailment':'yl','Unserved Energy':'iea_b', 'Underlying Load':'p' , 'Storage Load':'grey50', 'Nuclear': 'r'}\n",
    "\n",
    "reg_palette = {'CAC':'b', 'MAC':'rl', 'NAC':'pl', 'NEC':'t','SAC':'ol', 'UKR': 'grey20'}\n",
    "iea_cmap_regs = colors.ListedColormap([ iea_palette[reg_palette[i]] for i in reg_palette.keys() if i != 'UKR'])\n",
    "# subreg_palette = {'CAC':'bl', 'MAC':'b', 'NAC':'gl', 'NEC':'t', 'SAC':'ol', 'UKR': 'grey20'}\n",
    "\n",
    "#### General palette for WEO tech stuff which has > 20 items\n",
    "# weo_Tech_palette = { tech : '{}'.format(i) for i, tech in enumerate(gen_yr_df.WEO_Tech_simpl.unique())}\n",
    "\n",
    "\n",
    "iea_palette_l8 = ['rl', 'ol', 'gl', 'bl', 'pl', 'grey10', 'yl', 'tl'] ### got rid of light yellow as its a poor choice for plots.\n",
    "iea_palette_d8 = ['r', 'o', 'y', 'g', 't', 'b', 'p', 'grey50']\n",
    "iea_palette_16 = iea_palette_l8 + iea_palette_d8\n",
    "\n",
    "\n",
    "iea_palette_14 = ['rl', 'ol', 'bl', 'gl', 'pl', 'grey10', 'y', 'tl',  'g', 't', 'b', 'grey50', 'yl', 'r', 'p']\n",
    "iea_cmap_l8 = colors.ListedColormap([ iea_palette[c] for c in iea_palette_l8])\n",
    "iea_cmap_d8 = colors.ListedColormap([ iea_palette[c] for c in iea_palette_d8])\n",
    "iea_cmap_16 = colors.ListedColormap([ iea_palette[c] for c in iea_palette_16])\n",
    "iea_cmap_14 = colors.ListedColormap([ iea_palette[c] for c in iea_palette_14])\n",
    "\n",
    "tab20bc = colors.ListedColormap([ extended_palette[i] for i in extended_palette.keys()])\n",
    "\n",
    "model_palette = dict(zip([m for m in pretty_model_names.values() if m in model_names], iea_palette_14[:len(model_names)]))\n",
    "\n",
    "combined_palette = dict(tech_palette, **reg_palette, **model_palette) #, **weo_Tech_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xlsx_scatter(df, writer, excel_file=None, sheet_name='Sheet1', colour=None, palette=combined_palette, units='', alpha=80, to_combine=False, markersize=4, common_yr=2041):\n",
    "\n",
    "    cm_to_pixel = 37.7953\n",
    "    \n",
    "    \n",
    "    if excel_file:\n",
    "        writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "        \n",
    "    df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    # Access the XlsxWriter workbook and worksheet objects from the dataframe.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    if units == '':\n",
    "        num_fmt = '0%'\n",
    "        units = ''\n",
    "    else:\n",
    "        num_fmt = '# ###'\n",
    "        \n",
    "\n",
    "    # Create a chart object.\n",
    "    chart = workbook.add_chart({'type': 'scatter'})\n",
    "    chart.set_size({'width': 15*cm_to_pixel, 'height': 8.5*cm_to_pixel})\n",
    "\n",
    "\n",
    "    if to_combine:\n",
    "        \n",
    "        chart.set_plotarea({\n",
    "            'layout': {\n",
    "                'x':      0.1,\n",
    "                'y':      0,\n",
    "                'width':  0.9,\n",
    "                'height': 0.8,\n",
    "            },\n",
    "            'fill':{'none':True}\n",
    "        })\n",
    "    else:\n",
    "        chart.set_plotarea({\n",
    "    #         'layout': {\n",
    "    #             'x':      0.1,\n",
    "    #             'y':      0,\n",
    "    #             'width':  0.9,\n",
    "    #             'height': 0.8,\n",
    "    #         },\n",
    "            'fill':{'none':True}\n",
    "        })\n",
    "        \n",
    "        \n",
    "    chart.set_chartarea({\n",
    "        'fill':{'none':True},\n",
    "        'border': {'none': True},\n",
    "\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Configure the series of the chart from the dataframe data.\n",
    "    ## Col_num iterates from first data column, which varies if it is multiindex columns or not\n",
    "    for col_num in np.arange(1, df.shape[1] + 1):\n",
    "        \n",
    "        if colour != None:\n",
    "            fill_colour = colour\n",
    "        else:\n",
    "            try:\n",
    "                fill_colour = iea_palette_plus[palette[df.columns[col_num-df.index.nlevels]]]\n",
    "            except KeyError:\n",
    "                fill_colour = iea_cmap_16.colors[col_num-df.index.nlevels]\n",
    "            \n",
    "        chart.add_series({\n",
    "            'name':       [sheet_name, 0, col_num],\n",
    "            'categories': [sheet_name, 1, 0, df.shape[0], df.index.nlevels-1],\n",
    "            'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "            'marker': { 'type': 'circle',\n",
    "                        'size': markersize,\n",
    "                        'border': {'color': fill_colour},\n",
    "                        'fill':   {'color':fill_colour, 'transparency':alpha},\n",
    "                    },\n",
    "\n",
    "        })\n",
    "\n",
    "    # Configure the chart axes.\n",
    "    ### Set label_position to low if there are negative values\n",
    "    if( df < 0).sum().sum() > 0:\n",
    "        label_position = 'low'\n",
    "    else:\n",
    "        label_position = 'next_to'\n",
    "    \n",
    "\n",
    "\n",
    "    chart.set_x_axis({ 'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                       'num_format': 'mmm',\n",
    "                       'date_axis': True,\n",
    "                       'min': pd.to_datetime('01-01-{}'.format(common_yr)),\n",
    "                       'max': pd.to_datetime('31-12-{}'.format(common_yr)),\n",
    "                      'major_unit': 31,\n",
    "                      'interval_unit':31,\n",
    "                      'interval_tick':31,\n",
    "                      'label_position':label_position\n",
    "                     })\n",
    "    \n",
    "    \n",
    "    chart.set_y_axis({'major_gridlines': {'visible': False},\n",
    "                      'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                      'num_format':num_fmt,\n",
    "                      'name': units,\n",
    "                      'name_font': {'name': 'Arial', 'size': 10, 'bold':False, 'text_rotation':-90},\n",
    "                      'name_layout': { 'x': 0.02, 'y': 0.02 },\n",
    "                      'line': {'none': True},\n",
    "                      'major_gridlines': {\n",
    "                            'visible': True,\n",
    "                            'line': {'width': 1, 'color' : '#d9d9d9'}}\n",
    "                      })\n",
    "    \n",
    "    chart.set_title({'none': True})\n",
    "    \n",
    "    chart.set_legend({'font':  {'name': 'Arial', 'size': 10},\n",
    "                      'position': 'bottom',\n",
    "#                       'layout': {'x':      0,\n",
    "#                                 'y':      0.7,\n",
    "#                                 'width':  1,\n",
    "#                                 'height': 0.25\n",
    "#                                },\n",
    "#                      'delete_series': [leg_del_idx]\n",
    "                     })\n",
    "\n",
    "                      \n",
    "#     chart.set_legend({'num_font':  {'name': 'Arial', 'size': 10}})\n",
    "\n",
    "    # Insert the chart into the worksheet....this probably should depend on the size of the dataframe\n",
    "    if to_combine:\n",
    "        worksheet.insert_chart('K22', chart)\n",
    "    else:\n",
    "        worksheet.insert_chart('K2', chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xlsx_line(df, writer, excel_file=None, sheet_name='Sheet1', subtype='timeseries', palette=combined_palette, units='', ldc_idx=None, label_position='next_to', to_combine=False,line_width=1.5):\n",
    "\n",
    "    cm_to_pixel = 37.7953\n",
    "    \n",
    "    ## Sort columns by order in palettes described above\n",
    "    sort_cols = [c for c in palette.keys() if c in df.columns] + [c for c in df.columns if c not in palette.keys()]\n",
    "    sort_index = [i for i in palette.keys() if i in df.index] + [i for i in df.index if i not in palette.keys()]\n",
    "\n",
    "    if type(df.index) == pd.Index:\n",
    "        df = df.loc[sort_index, sort_cols]\n",
    "    else:\n",
    "        df = df.loc[:,sort_cols]\n",
    "    \n",
    "   \n",
    "    if subtype == 'ldc':\n",
    "        if ldc_idx is None:\n",
    "            df.index = (np.arange(0,df.shape[0]) + 1)/df.shape[0]\n",
    "        else:\n",
    "            df.index = ldc_idx            \n",
    "    \n",
    "    if excel_file:\n",
    "        writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "        \n",
    "    df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    # Access the XlsxWriter workbook and worksheet objects from the dataframe.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    if units == '':\n",
    "        num_fmt = '0%'\n",
    "        units = ''\n",
    "    else:\n",
    "        num_fmt = '# ###'\n",
    "        \n",
    "    if subtype == 'timeseries':\n",
    "        chart = workbook.add_chart({'type': 'line'})\n",
    "    else:\n",
    "        chart = workbook.add_chart({'type': 'scatter', 'subtype': 'line'})\n",
    "\n",
    "    # Create a chart object.\n",
    "    if to_combine:\n",
    "        chart.set_size({'width': 7.5*cm_to_pixel, 'height': 7*cm_to_pixel})\n",
    "    else:\n",
    "        chart.set_size({'width': 15*cm_to_pixel, 'height': 7*cm_to_pixel})\n",
    "\n",
    "\n",
    "    if to_combine:\n",
    "        \n",
    "        chart.set_plotarea({\n",
    "            'layout': {\n",
    "                'x':      0.25,\n",
    "                'y':      0,\n",
    "                'width':  0.75,\n",
    "                'height': 0.65,\n",
    "            },\n",
    "            'fill':{'none':True}\n",
    "        })\n",
    "    else:\n",
    "        chart.set_plotarea({\n",
    "            'layout': {\n",
    "                'x':      0.25,\n",
    "                'y':      0,\n",
    "                'width':  0.75,\n",
    "                'height': 0.65,\n",
    "            },\n",
    "            'fill':{'none':True}\n",
    "        })\n",
    "        \n",
    "        \n",
    "    chart.set_chartarea({\n",
    "        'fill':{'none':True},\n",
    "        'border': {'none': True},\n",
    "\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # Configure the series of the chart from the dataframe data.\n",
    "    ## Col_num iterates from first data column, which varies if it is multiindex columns or not\n",
    "    for col_num in np.arange(df.index.nlevels, df.shape[1] + df.index.nlevels):\n",
    "        \n",
    "        try:\n",
    "            line_colour = iea_palette_plus[palette[df.columns[col_num-df.index.nlevels]]]\n",
    "        except KeyError:\n",
    "            line_colour = iea_cmap_16.colors[col_num-df.index.nlevels]\n",
    "            \n",
    "        chart.add_series({\n",
    "            'name':       [sheet_name, 0, col_num],\n",
    "            'categories': [sheet_name, 1, 0, df.shape[0], df.index.nlevels-1],\n",
    "            'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "            'line':   {'color': line_colour,\n",
    "                       'width':line_width},\n",
    "            'marker': {'type': 'none'}\n",
    "        })\n",
    "        \n",
    "        \n",
    "    ### Set label_position to low if there are negative values\n",
    "    if( df < 0).sum().sum() > 0:\n",
    "        label_position = 'low'\n",
    "    else:\n",
    "        label_position = 'next_to'\n",
    "    \n",
    "\n",
    "    chart.set_x_axis({'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                      'num_format': 'dd mmm hh:mm',\n",
    "                      'major_unit': 24,\n",
    "                      'interval_unit':24,\n",
    "                      'interval_tick':12,\n",
    "                      'line': {'color': 'black'},\n",
    "                      'text_axis': True,\n",
    "                      'label_position':label_position})\n",
    "    \n",
    "\n",
    "    if subtype == 'ldc':\n",
    "        if np.round(np.max(df.index)/5,-1) < 1: \n",
    "            chart.set_x_axis({'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                         'line': {'color': 'black'},\n",
    "                             'major_unit':np.round(np.max(df.index)/5,-1),\n",
    "                             'min':0,\n",
    "                             'max':np.max(df.index),\n",
    "                             'num_format': '0.0%',\n",
    "                             'label_position':label_position\n",
    "                             })\n",
    "        else:\n",
    "            chart.set_x_axis({'num_font':  {'name': 'Arial', 'size': 10},\n",
    "             'line': {'color': 'black'},\n",
    "                 'major_unit':np.round(np.max(df.index)/5,-1),\n",
    "                 'min':0,\n",
    "                 'max':np.max(df.index),\n",
    "                 'num_format': '0%',\n",
    "                 'label_position':label_position\n",
    "                 })\n",
    "    else:\n",
    "        \n",
    "        chart.set_x_axis({'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                     'line': {'color': 'black'},\n",
    "                     'num_format': 'mmm',\n",
    "                      'major_unit': 30,\n",
    "                      'label_position':label_position})\n",
    "    \n",
    "    \n",
    "    chart.set_y_axis({'major_gridlines': {'visible': False},\n",
    "                      'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                      'num_format':num_fmt,\n",
    "                      'name': units,\n",
    "                      'name_font': {'name': 'Arial', 'size': 10, 'bold':False, 'text_rotation':-90},\n",
    "                      'name_layout': { 'x': 0.02, 'y': 0.02 },\n",
    "                      'line': {'none': True},\n",
    "                      'major_gridlines': {\n",
    "                            'visible': True,\n",
    "                            'line': {'width': 1, 'color' : '#d9d9d9'}}\n",
    "                      })\n",
    "    \n",
    "    chart.set_title({'none': True})\n",
    "    \n",
    "    if df.shape[1] > 1:\n",
    "    \n",
    "        chart.set_legend({'font':  {'name': 'Arial', 'size': 10},\n",
    "                      'position':'bottom',\n",
    "                      'layout': {'x': 0,\n",
    "                                'y':      0.85,\n",
    "                                'height': 0.15,\n",
    "                                'width':1\n",
    "                               }}\n",
    "                        )\n",
    "        \n",
    "        \n",
    "#         max_chars = np.max([len(c) for c in df.columns])\n",
    "#         min_width = 0.075\n",
    "#         width = min_width + max_chars*0.01\n",
    "        \n",
    "#         chart.set_legend({'font':  {'name': 'Arial', 'size': 10},\n",
    "#                           'layout': {'x': 1- width,\n",
    "#                                     'y':      0,\n",
    "#                                     'height': 1,\n",
    "#                                     'width':width\n",
    "#                                    }})\n",
    "    else:\n",
    "        chart.set_legend({'visble':False})\n",
    "        chart.set_legend({'position': 'none'})\n",
    "\n",
    "    # Insert the chart into the worksheet....this probably should depend on the size of the dataframe\n",
    "    \n",
    "    if to_combine:\n",
    "        worksheet.insert_chart('K22', chart)\n",
    "    else:\n",
    "        worksheet.insert_chart('K2', chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xlsx_column(df, writer, excel_file=None, sheet_name='Sheet1', palette=combined_palette, subtype='stacked', units='', total_scatter_col=None, total_fill=iea_palette['white'], to_combine=False, right_ax=None):\n",
    "\n",
    "    cm_to_pixel = 37.7953\n",
    "    \n",
    "    ## Sort columns by order in palettes described above\n",
    "    sort_cols = [c for c in palette.keys() if c in df.columns] + [c for c in df.columns if c not in palette.keys()]\n",
    "    sort_index = [i for i in palette.keys() if i in df.index] + [i for i in df.index if i not in palette.keys()]\n",
    "\n",
    "    if type(df.index == pd.Index):\n",
    "        df = df.loc[sort_index, sort_cols]\n",
    "    else:\n",
    "        df = df.loc[:,sort_cols]\n",
    "    \n",
    "\n",
    "    if excel_file:\n",
    "        writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "    \n",
    "    ### Whether we caluclate the scatter col or not. Should probably rename the variable from total_col, as its not always a total\n",
    "    if (total_scatter_col != None)&(total_scatter_col not in df.columns):\n",
    "        df.loc[:,total_scatter_col] = df.sum(axis=1)\n",
    "        \n",
    "    df.to_excel(writer, sheet_name=sheet_name)\n",
    "        \n",
    "\n",
    "    # Access the XlsxWriter workbook and worksheet objects from the dataframe.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    \n",
    "    if units == '%':\n",
    "        num_fmt = '0%'\n",
    "        units = ''\n",
    "    else:\n",
    "        num_fmt = '# ###'\n",
    "        \n",
    "    \n",
    "    # Create a chart object.\n",
    "    chart = workbook.add_chart({'type': 'column', 'subtype': subtype})\n",
    "    chart.set_size({'width': 15*cm_to_pixel, 'height': 7*cm_to_pixel})\n",
    "\n",
    "    if to_combine:\n",
    "        \n",
    "        chart.set_plotarea({\n",
    "            'layout': {\n",
    "                'x':      0.1,\n",
    "                'y':      0,\n",
    "                'width':  0.9,\n",
    "                'height': 0.8,\n",
    "            },\n",
    "            'fill':{'none':True}\n",
    "        })\n",
    "    else:\n",
    "        chart.set_plotarea({\n",
    "    #         'layout': {\n",
    "    #             'x':      0.1,\n",
    "    #             'y':      0,\n",
    "    #             'width':  0.9,\n",
    "    #             'height': 0.8,\n",
    "    #         },\n",
    "            'fill':{'none':True}\n",
    "        })\n",
    "        \n",
    "\n",
    "    chart.set_chartarea({\n",
    "        'fill':{'none':True},\n",
    "        'border': {'none': True},\n",
    "\n",
    "    })\n",
    "    \n",
    "    if (total_scatter_col != None) & (df.shape[1] > 1):\n",
    "        chart2 = workbook.add_chart({'type': 'scatter'})\n",
    "    \n",
    "    \n",
    "    \n",
    "    for col_num in np.arange(df.index.nlevels, df.shape[1] + df.index.nlevels):\n",
    "        if df.columns[col_num-df.index.nlevels] != total_scatter_col:\n",
    "                # Configure the series of the chart from the dataframe data.\n",
    "            ## Col_num iterates from first data column, which varies if it is multiindex columns or not\n",
    "\n",
    "            try:\n",
    "                fill_colour = iea_palette_plus[palette[df.columns[col_num-df.index.nlevels]]]\n",
    "            except KeyError:\n",
    "                fill_colour = iea_cmap_16.colors[col_num-df.index.nlevels]\n",
    "\n",
    "            # fill_colour = matplotlib.colors.rgb2hex(plt.cm.get_cmap('tab20c').colors[20-col_num-df.index.nlevels])\n",
    "\n",
    "\n",
    "    # Or using a list of values instead of category/value formulas:\n",
    "#     [sheetname, first_row, first_col, last_row, last_col]\n",
    "\n",
    "# Or using a list of values instead of category/value formulas:\n",
    "#     [sheetname, first_row, first_col, last_row, last_col]\n",
    "            chart.add_series({\n",
    "                'name':       [sheet_name, 0, col_num],\n",
    "                'categories': [sheet_name, 1, 0, df.shape[0], df.index.nlevels-1],\n",
    "                'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                'gap':        75,\n",
    "                'fill':       {'color': fill_colour, 'border':'#000000'},\n",
    "                'border': {'color': '#000000'}\n",
    "            })\n",
    "        else:\n",
    "            if right_ax != None:\n",
    "                chart2.add_series({\n",
    "                    'name':       [sheet_name, 0, col_num],\n",
    "                    'categories': [sheet_name, 1, 0, df.shape[0], df.index.nlevels-1],\n",
    "                    'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                    'marker': { 'type': 'circle',\n",
    "                                'size': 8,\n",
    "                                    'border': {'color': '#000000'},\n",
    "                                    'fill':   {'color': '#ffffff', 'transparency':30},\n",
    "                                },\n",
    "                    'y2_axis':    True\n",
    "\n",
    "                })\n",
    "            else:\n",
    "                chart2.add_series({\n",
    "                    'name':       [sheet_name, 0, col_num],\n",
    "                    'categories': [sheet_name, 1, 0, df.shape[0], df.index.nlevels-1],\n",
    "                    'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                    'marker': { 'type': 'circle',\n",
    "                                'size': 7,\n",
    "                                    'border': {'color': '#000000'},\n",
    "                                    'fill':   {'color': total_fill, 'transparency':0},\n",
    "                                }\n",
    "                })\n",
    "\n",
    "                \n",
    "            \n",
    "        ### This is the total column and will always be last, so we can just combine here and save an if/else loop\n",
    "            chart.combine(chart2)\n",
    "     \n",
    "    ### Set label_position to low if there are negative values\n",
    "    if( df < 0).sum().sum() > 0:\n",
    "        label_position = 'low'\n",
    "    else:\n",
    "        label_position = 'next_to'\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "    chart.set_x_axis({'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                     'line': {'color': 'black'},\n",
    "                     'label_position':label_position})\n",
    "    \n",
    "    \n",
    "    chart.set_y_axis({'major_gridlines': {'visible': False},\n",
    "                      'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                      'num_format':num_fmt,\n",
    "                      'name': units,\n",
    "                      'name_font': {'name': 'Arial', 'size': 10, 'bold':False, 'text_rotation':-90},\n",
    "                      'name_layout': { 'x': 0.02, 'y': 0.02 },\n",
    "                      'line': {'none': True},\n",
    "                      'major_gridlines': {\n",
    "                            'visible': True,\n",
    "                            'line': {'width': 1, 'color' : '#d9d9d9'}},\n",
    "                      })\n",
    "    \n",
    "    if right_ax != None:\n",
    "        if df.shape[1] > 1:\n",
    "            max_chars = np.max([len(c) for c in df.columns] )\n",
    "            min_width = 0.075\n",
    "            width = min_width + max_chars*0.01\n",
    "        else:\n",
    "            width= 0\n",
    "        \n",
    "        \n",
    "        chart2.set_y2_axis({'major_gridlines': {'visible': False},\n",
    "                      'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                      'num_format':num_fmt,\n",
    "                      'name': right_ax,\n",
    "                      'name_font': {'name': 'Arial', 'size': 10, 'bold':False, 'text_rotation':-90},\n",
    "                      'name_layout': { 'x': 0.98 - width, 'y': 0.02 },\n",
    "                      'line': {'none': True},\n",
    "                      'major_gridlines': {\n",
    "                            'visible': True,\n",
    "                            'line': {'width': 1, 'color' : '#d9d9d9'}},\n",
    "                      })\n",
    "        \n",
    "    \n",
    "    chart.set_title({'none': True})\n",
    "    \n",
    "    if df.shape[1] > 1:\n",
    "        max_chars = np.max([len(c) for c in df.columns] )\n",
    "        \n",
    "        ### Legend should not exceed 16chars and should always be more than 8chars\n",
    "        \n",
    "        if max_chars <8:\n",
    "            max_chars=8\n",
    "        elif max_chars >16:\n",
    "            max_chars =16\n",
    "        \n",
    "        min_width = 0.075\n",
    "        width = min_width + max_chars*0.01\n",
    "        \n",
    "        chart.set_legend({'font':  {'name': 'Arial', 'size': 10},\n",
    "                          'layout': {'x': 1- width,\n",
    "                                    'y':      0,\n",
    "                                    'height': 1,\n",
    "                                    'width':width\n",
    "                                   }}\n",
    "                        )\n",
    "    else:\n",
    "        chart.set_legend({'visble':False } )\n",
    "        chart.set_legend({'position': 'none'})\n",
    "\n",
    "    # Insert the chart into the worksheet....this probably should depend on the size of the dataframe\n",
    "    if to_combine:\n",
    "        worksheet.insert_chart('K22', chart)\n",
    "    else:\n",
    "        worksheet.insert_chart('K2', chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_xlsx_stack(df, writer, excel_file=None, sheet_name='Sheet1', palette=stack_palette, units='MW', to_combine=False):\n",
    "\n",
    "    cm_to_pixel = 37.7953\n",
    "    \n",
    "    ## Sort columns by order in palettes described above\n",
    "    sort_cols = [c for c in palette.keys() if c in df.columns] + [c for c in df.columns if c not in palette.keys()]\n",
    "    df = df.loc[:, sort_cols]\n",
    "    \n",
    "    if excel_file:\n",
    "        writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
    "          \n",
    "    df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "    # Access the XlsxWriter workbook and worksheet objects from the dataframe.\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "          \n",
    "\n",
    "    # Create a chart object.\n",
    "    chart = workbook.add_chart({'type': 'area', 'subtype': 'stacked'})\n",
    "    chart2 = workbook.add_chart({'type': 'line'})\n",
    "\n",
    "    \n",
    "    sec_axis_vars = ['Load2', 'Curtailment']\n",
    "    write_xlsx_stack= ['Net Load', 'Load']\n",
    "    \n",
    "    # Configure the series of the chart from the dataframe data.\n",
    "    ## Col_num iterates from first data column, which varies if it is multiindex columns or not\n",
    "    for col_num in np.arange(df.index.nlevels, df.shape[1] + df.index.nlevels):\n",
    "        \n",
    "        try:\n",
    "            fill_colour = iea_palette_plus[palette[df.columns[col_num-1]]]\n",
    "        except KeyError:\n",
    "            print('Non-specified colour for: {}'.format(df.columns[col_num-1]))\n",
    "            fill_colour = iea_cmap_16.colors[col_num-1]\n",
    "            \n",
    "        if df.columns[col_num-1] == 'Load2':\n",
    "            \n",
    "            chart.add_series({\n",
    "                'name':       [sheet_name, 0, col_num],\n",
    "                'categories': [sheet_name, 1, 0, df.shape[0], 0],\n",
    "                'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                'fill':       {'none': True},\n",
    "                'border': {'none': True},\n",
    "                'y2_axis': True\n",
    "            })\n",
    "            \n",
    "            leg_del_idx = [ int(col_num -1) ]\n",
    "            \n",
    "        elif df.columns[col_num-1] == 'Curtailment':\n",
    "            chart.add_series({\n",
    "                'name':       [sheet_name, 0, col_num],\n",
    "                'categories': [sheet_name, 1, 0, df.shape[0], 0],\n",
    "                'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                'pattern': { 'pattern':  'light_upward_diagonal',\n",
    "                             'fg_color': iea_palette['y'],\n",
    "                             'bg_color': iea_palette['r']\n",
    "                            },\n",
    "                'border': {'none': True},\n",
    "                'y2_axis': True\n",
    "            })\n",
    "        elif df.columns[col_num-1] == 'Total Load':\n",
    "            chart2.add_series({\n",
    "                'name':       [sheet_name, 0, col_num],\n",
    "                'categories': [sheet_name, 1, 0, df.shape[0], 0],\n",
    "                'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                'line': {'width': 0.25, 'color':'black', 'dash_type': 'solid'},\n",
    "            })   \n",
    "        elif df.columns[col_num-1] == 'Underlying Load':\n",
    "            continue\n",
    "        elif df.columns[col_num-1] == 'Storage Load':\n",
    "            continue\n",
    "#             chart2.add_series({\n",
    "#                 'name':       [sheet_name, 0, col_num],\n",
    "#                 'categories': [sheet_name, 1, 0, df.shape[0], 0],\n",
    "#                 'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "#                 'line': {'width': 1.00, 'color':iea_palette['p'], 'dash_type': 'dash'},\n",
    "#             })   \n",
    "        elif df.columns[col_num-1] == 'Net Load':\n",
    "            chart2.add_series({\n",
    "                'name':       [sheet_name, 0, col_num],\n",
    "                'categories': [sheet_name, 1, 0, df.shape[0], 0],\n",
    "                'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                'line': {'width': 1.00, 'color':iea_palette['r'], 'dash_type': 'dash'},\n",
    "            })\n",
    "        else:\n",
    "            chart.add_series({\n",
    "                'name':       [sheet_name, 0, col_num],\n",
    "                'categories': [sheet_name, 1, 0, df.shape[0], 0],\n",
    "                'values':     [sheet_name, 1, col_num, df.shape[0], col_num],\n",
    "                'fill':       {'color': fill_colour},\n",
    "                'border': {'none': True}\n",
    "            })\n",
    "            \n",
    "        \n",
    "    # Configure the chart axes.\n",
    "    num_fmt = '# ###'\n",
    "    \n",
    "    chart.combine(chart2)\n",
    "    \n",
    "        # Create a chart object.\n",
    "    if to_combine:\n",
    "        chart.set_size({'width': 7.5*cm_to_pixel, 'height': 5.5*cm_to_pixel})\n",
    "    else:\n",
    "        chart.set_size({'width': 15*cm_to_pixel, 'height': 7*cm_to_pixel})\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    chart.set_plotarea({\n",
    "        'layout': {\n",
    "            'x':      0.1,\n",
    "            'y':      0,\n",
    "            'width':  0.9,\n",
    "            'height': 0.8,\n",
    "        },\n",
    "        'fill':{'none':True}\n",
    "    })\n",
    "\n",
    "        \n",
    "    chart.set_chartarea({\n",
    "        'fill':{'none':True},\n",
    "        'border': {'none': True},\n",
    "\n",
    "    })\n",
    "    \n",
    "    \n",
    "    ### Set label_position to low if there are negative values\n",
    "    if( df < 0).sum().sum() > 0:\n",
    "        label_position = 'low'\n",
    "    else:\n",
    "        label_position = 'next_to'\n",
    "    \n",
    "\n",
    "    chart.set_x_axis({'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                      'num_format': 'dd mmm',\n",
    "                      'major_unit': 24,\n",
    "                      'interval_unit':24,\n",
    "                      'interval_tick':12,\n",
    "                      'line': {'color': 'black'},\n",
    "                      'text_axis': True,\n",
    "                      'label_position':label_position})\n",
    "    \n",
    "    \n",
    "    chart.set_y_axis({'major_gridlines': {'visible': False},\n",
    "                      'num_font':  {'name': 'Arial', 'size': 10},\n",
    "                      'num_format':num_fmt,\n",
    "                      'name': units,\n",
    "                      'name_font': {'name': 'Arial', 'size': 10, 'bold':False, 'text_rotation':-90},\n",
    "                      'name_layout': { 'x': 0.02, 'y': 0.02 },\n",
    "                      'line': {'none': True},\n",
    "                      'major_gridlines': {\n",
    "                            'visible': True,\n",
    "                            'line': {'width': 1, 'color' : '#d9d9d9'}}\n",
    "                      })\n",
    "                      \n",
    "    chart.set_y2_axis({'visible':False})\n",
    "                      \n",
    "#     leg_del_idx = df.shape[1] \n",
    "\n",
    "    if 'Load2' in df.columns:\n",
    "    \n",
    "        chart.set_legend({'font':  {'name': 'Arial', 'size': 10},\n",
    "                          'position': 'bottom',\n",
    "                          'layout': {'x':      0,\n",
    "                                    'y':      0.7,\n",
    "                                    'width':  1,\n",
    "                                    'height': 0.25\n",
    "                                   },\n",
    "                         'delete_series': leg_del_idx\n",
    "                         })\n",
    "    else:\n",
    "        chart.set_legend({'font':  {'name': 'Arial', 'size': 10},\n",
    "                  'position': 'bottom',\n",
    "                  'layout': {'x':      0,\n",
    "                            'y':      0.7,\n",
    "                            'width':  1,\n",
    "                            'height': 0.25\n",
    "                           }\n",
    "                 })\n",
    "        \n",
    "    # Insert the chart into the worksheet.\n",
    "    if to_combine == True:\n",
    "        worksheet.insert_chart('S32', chart)\n",
    "    else:\n",
    "        worksheet.insert_chart('S2', chart)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Thaialand-specific, floating hydro outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydro_by_model_ts = gen_by_tech_ts.Hydro.unstack('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "northern_hydro_ts = gen_by_tech_reg_ts[(gen_by_tech_reg_ts.index.get_level_values('Region') == 'NEC')|(gen_by_tech_reg_ts.index.get_level_values('Region') == 'NAC')].Hydro.unstack('model').droplevel(geo_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hy_plots = {'hy_monthly': hydro_by_model_ts.groupby(pd.Grouper(level='timestamp', freq='M')).sum()/1e6,\n",
    "#             'hy_weekly':hydro_by_model_ts.groupby(pd.Grouper(level='timestamp', freq='W')).sum()/1e6,\n",
    "#             'hy_daily':hydro_by_model_ts.groupby(pd.Grouper(level='timestamp', freq='D')).sum()/1e3,\n",
    "#             'hy_avg': hydro_by_model_ts.groupby([hydro_by_model_ts.index.hour]).mean(),\n",
    "            \n",
    "#             'nor_hy_monthly': northern_hydro_ts.groupby(pd.Grouper(level='timestamp', freq='M')).sum()/1e6,\n",
    "#             'nor_hy_weekly':northern_hydro_ts.groupby(pd.Grouper(level='timestamp', freq='W')).sum()/1e6,\n",
    "#             'nor_hy_daily':northern_hydro_ts.groupby(pd.Grouper(level='timestamp', freq='D')).sum()/1e3,\n",
    "#             'nor_hy_avg': northern_hydro_ts.groupby([northern_hydro_ts.index.hour]).mean(),\n",
    "            \n",
    "#             'sto_monthly': sto_by_model_ts.groupby(pd.Grouper(level='timestamp', freq='M')).sum()/1e6,\n",
    "#             'sto_avg': sto_by_model_ts.groupby([sto_by_model_ts.index.hour]).mean(),\n",
    "            \n",
    "#             'hy_sto_plus_monthly': hydro_sto_by_model_ts.groupby(pd.Grouper(level='timestamp', freq='M')).sum()/1e6,\n",
    "#             'hy_sto_plus_avg': hydro_sto_by_model_ts.groupby([hydro_sto_by_model_ts.index.hour]).mean(),\n",
    "#            }\n",
    "\n",
    "# hy_plot_units = {'hy_monthly': 'TWh',\n",
    "#             'hy_weekly':'TWh',\n",
    "#             'hy_daily':'GWh',\n",
    "#             'hy_avg': 'MW',\n",
    "            \n",
    "#             'nor_hy_monthly': 'TWh',\n",
    "#             'nor_hy_weekly': 'TWh',\n",
    "#             'nor_hy_daily': 'GWh',\n",
    "#             'nor_hy_avg': 'MW',\n",
    "                 \n",
    "#             'sto_monthly': 'TWh',\n",
    "#             'sto_avg':'MW',\n",
    "#             'hy_sto_plus_monthly':'TWh',\n",
    "#             'hy_sto_plus_avg': 'MW',\n",
    "                 \n",
    "                \n",
    "#            }\n",
    "\n",
    "# hy_plot_type = {'hy_monthly': 'line',\n",
    "#             'hy_weekly':'line',\n",
    "#             'hy_daily':'line',\n",
    "#             'hy_avg': 'line',\n",
    "    \n",
    "                 \n",
    "#             'nor_hy_monthly': 'line',\n",
    "#             'nor_hy_weekly': 'line',\n",
    "#             'nor_hy_daily': 'line',\n",
    "#             'nor_hy_avg': 'line',\n",
    "                \n",
    "                \n",
    "#             'sto_monthly': 'line',\n",
    "#             'sto_avg':'line',\n",
    "#             'hy_sto_plus_monthly':'line',\n",
    "#             'hy_sto_plus_avg': 'line',\n",
    "#            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot 2:  Hydro summary plots by columnn\n",
    "\n",
    "# fig_path = os.path.join(save_dir_plots,'plot2_hy_output.xlsx')\n",
    "\n",
    "# with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "#     for i in hy_plots.keys():\n",
    "#         if hy_plots[i].shape[0] == 0:\n",
    "#             print('Empty dataframe for: {}'.format(i))\n",
    "#         elif hy_plot_type[i] == 'col':\n",
    "#             write_xlsx_column(df=hy_plots[i], writer=writer, sheet_name=i, units=hy_plot_units[i], palette=combined_palette)\n",
    "#         else:\n",
    "#             write_xlsx_line(df=hy_plots[i], writer=writer, sheet_name=i, units=hy_plot_units[i], line_width=0.1) \n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel plots and exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ids = list(np.unique(np.append(load_by_reg.unstack(geo_cols).droplevel(level=[c for c in geo_cols if c != geo_cols[0]], axis=1).replace(0, np.nan).dropna(how='all', axis=1).columns,\n",
    "gen_by_tech_reg.droplevel(level=[c for c in geo_cols if c != geo_cols[0]], axis=1).replace(0, np.nan).dropna(how='all', axis=1).columns)))\n",
    "\n",
    "model_regs = reg_ids + ['UKR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 16: Generation stacks\n",
    "    \n",
    "### Combine variables together to make a stack DF\n",
    "### Return only those regions which have anything modelled\n",
    "\n",
    "\n",
    "pumpload_reg_ts = node_df[(node_df.property == 'Pump Load')|(node_df.property == 'Battery Load')].groupby(['model'] + geo_cols + ['timestamp']).sum().value.rename('Storage Load')\n",
    "underlying_load_reg = (load_by_reg_ts - pumpload_reg_ts).rename('Underlying Load')\n",
    "net_load_reg_sto_ts = (net_load_reg_ts.stack(geo_cols).reorder_levels(['model'] + geo_cols + ['timestamp']) + pumpload_reg_ts).rename('Net Load')\n",
    "\n",
    "gen_stack_by_reg = pd.concat([gen_by_tech_reg_ts.fillna(0), \n",
    "#                               line_flows, \n",
    "                              net_load_reg_sto_ts,\n",
    "                              underlying_load_reg,\n",
    "                              pumpload_reg_ts,\n",
    "                              load_by_reg_ts.rename('Total Load'),\n",
    "                              load_by_reg_ts.rename('Load2'),\n",
    "                              vre_curtailed_reg_ts.stack(geo_cols).reorder_levels(['model'] + geo_cols + ['timestamp']).rename('Curtailment'),\n",
    "                              use_reg_ts.stack(geo_cols).reorder_levels(['model'] + geo_cols + ['timestamp']).rename('Unserved Energy'), \n",
    "                              ], axis=1)\n",
    "\n",
    "### Drop specific columns...order is currently set in the function, though this can be contreolled by a flag\n",
    "\n",
    "### Get island and total summaries..... \n",
    "# gen_stack_by_isl  = gen_stack_by_reg.groupby(['model', geo_cols[0], 'timestamp']).sum().reset_index()\n",
    "gen_stack_total = gen_stack_by_reg.groupby(['model', 'timestamp']).sum().reset_index()\n",
    "\n",
    "# gen_stack_by_isl.loc[:,geo_cols[0]] = gen_stack_by_isl.loc[:,geo_cols[1]] = gen_stack_by_isl.Island\n",
    "gen_stack_total.loc[:,geo_cols[0]] = gen_stack_total.loc[:,geo_cols[0]] = gen_stack_total.loc[:,geo_cols[1]] = 'UKR'\n",
    "\n",
    "# gen_stack_by_isl = gen_stack_by_isl.set_index(['model'] + geo_cols + ['timestamp'])\n",
    "gen_stack_total = gen_stack_total.set_index(['model'] + geo_cols + ['timestamp'])\n",
    "\n",
    "### Avoid doubles for regions that encompass entire islands\n",
    "# incl_isl = [i for i in gen_stack_by_isl.index.get_level_values(geo_cols[0]).unique() if i not in gen_stack_by_reg.index.get_level_values(geo_cols[0]).unique()]\n",
    "\n",
    "\n",
    "gen_stack_by_reg = pd.concat([gen_stack_by_reg, gen_stack_total], axis=0).groupby(['model', geo_cols[0], 'timestamp']).sum() # gen_stack_by_isl.loc[ix[:,incl_isl,:]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 1: Generation stacks for national days of interest\n",
    "\n",
    "\n",
    "####\n",
    "doi_periods = [doi for doi in doi_summary.index if 'time' in doi]\n",
    "doi_names = [doi for doi in doi_summary.index if 'time' not in doi]\n",
    "\n",
    "\n",
    "for i, p in enumerate(doi_periods):\n",
    "    \n",
    "    doi = doi_summary.loc[p]\n",
    "    doi_name = doi_names[i]\n",
    "    \n",
    "    for m in model_names:\n",
    "        \n",
    "        save_dir_model = os.path.join(save_dir_plots, m.replace(\"/\",\"\"))\n",
    "        if os.path.exists(save_dir_model) is False:\n",
    "            os.mkdir(save_dir_model)\n",
    "            \n",
    "        gen_stack = gen_stack_by_reg.loc[ix[m,:,:],:]\n",
    "        toi = doi.loc[m]\n",
    "        \n",
    "        gen_stack_doi = gen_stack.reset_index()\n",
    "        gen_stack_doi = gen_stack_doi.loc[(gen_stack_doi.timestamp.dt.date >= toi.date() - pd.Timedelta('3D')) &\\\n",
    "                                          (gen_stack_doi.timestamp.dt.date <= toi.date() + pd.Timedelta('3D'))]\n",
    "        gen_stack_doi = gen_stack_doi.set_index(['model', geo_cols[0], 'timestamp'])\n",
    "        \n",
    "        #gen_stack_doi = gen_stack_doi_reg.groupby(['model', 'timestamp'], as_index=False).sum()\n",
    "        fig_path = os.path.join(save_dir_model,'plot1a_stack_ntl_doi_{}.xlsx'.format(doi_name))\n",
    "#         shutil.copyfile(fig_template_path, fig_path)\n",
    "\n",
    "\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "            ## ExcelWriter for some reason uses writer.sheets to access the sheet.\n",
    "            ## If you leave it empty it will not know that sheet Main is already there\n",
    "            ## and will create a new sheet.\n",
    "\n",
    "\n",
    "            for reg in model_regs:\n",
    "                gen_stack_doi_reg = gen_stack_doi.loc[ix[:,reg,:],:].droplevel([0,1])\n",
    "                write_xlsx_stack(df=gen_stack_doi_reg, writer=writer, sheet_name=reg, palette=stack_palette)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 1xx: National generation stacks by model in a single file for easy reference and making report figures\n",
    "\n",
    "\n",
    "####\n",
    "doi_periods = [doi for doi in doi_summary.index if 'time' in doi]\n",
    "doi_names = [doi for doi in doi_summary.index if 'time' not in doi]\n",
    "\n",
    "\n",
    "for i, p in enumerate(doi_periods):\n",
    "    \n",
    "    doi = doi_summary.loc[p]\n",
    "    doi_name = doi_names[i]\n",
    "    \n",
    "\n",
    "    gen_stack = gen_stack_by_reg.loc[ix[:,'UKR',:],:]\n",
    "    toi = doi.loc[m]\n",
    "\n",
    "    gen_stack_doi = gen_stack.reset_index()\n",
    "    gen_stack_doi = gen_stack_doi.loc[(gen_stack_doi.timestamp.dt.date >= toi.date() - pd.Timedelta('3D')) &\\\n",
    "                                      (gen_stack_doi.timestamp.dt.date <= toi.date() + pd.Timedelta('3D'))]\n",
    "    gen_stack_doi = gen_stack_doi.set_index(['model', geo_cols[0], 'timestamp'])\n",
    "\n",
    "\n",
    "    #gen_stack_doi = gen_stack_doi_reg.groupby(['model', 'timestamp'], as_index=False).sum()\n",
    "    fig_path = os.path.join(save_dir_plots,'plot1c_ntl_stack_doi_{}.xlsx'.format(doi_name))\n",
    "#         shutil.copyfile(fig_template_path, fig_path)\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "        ## ExcelWriter for some reason uses writer.sheets to access the sheet.\n",
    "        ## If you leave it empty it will not know that sheet Main is already there\n",
    "        ## and will create a new sheet.\n",
    "\n",
    "\n",
    "        for m in model_names:\n",
    "            gen_stack_doi_m = gen_stack_doi.loc[ix[m,],:].droplevel(0)\n",
    "            write_xlsx_stack(df=gen_stack_doi_m, writer=writer, sheet_name=m.replace(' ','').replace('Flex','')[:31], palette=stack_palette)\n",
    "            write_xlsx_stack(df=gen_stack_doi_m, writer=writer, sheet_name=m.replace(' ','').replace('Flex','')[:31], palette=stack_palette, to_combine=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 1b: Generation stacks for national days of interest a specified reference model\n",
    "\n",
    "\n",
    "\n",
    "### Ref model is based on highest USE....this can be changed/even selected via drop-down menu, etc.\n",
    "ref_model = use_reg_ts.groupby('model').sum().idxmax().iloc[0]\n",
    "####|\n",
    "doi_periods = [doi for doi in doi_summary.index if 'time' in doi]\n",
    "doi_names = [doi for doi in doi_summary.index if 'time' not in doi]\n",
    "\n",
    "\n",
    "for i, p in enumerate(doi_periods):\n",
    "    \n",
    "    doi = doi_summary.loc[p]\n",
    "    doi_name = doi_names[i]\n",
    "    \n",
    "    for m in model_names:\n",
    "        \n",
    "        save_dir_model = os.path.join(save_dir_plots, m.replace(\"/\",\"\"))\n",
    "        if os.path.exists(save_dir_model) is False:\n",
    "            os.mkdir(save_dir_model)\n",
    "            \n",
    "        gen_stack = gen_stack_by_reg.loc[ix[m,:,:],:]\n",
    "        toi_ref = doi.loc[ref_model]\n",
    "        \n",
    "        gen_stack_doi = gen_stack.reset_index()\n",
    "        gen_stack_doi = gen_stack_doi.loc[(gen_stack_doi.timestamp.dt.date >= toi_ref.date() - pd.Timedelta('3D')) &\\\n",
    "                                          (gen_stack_doi.timestamp.dt.date <= toi_ref.date() + pd.Timedelta('3D'))]\n",
    "        gen_stack_doi = gen_stack_doi.set_index(['model', geo_cols[0], 'timestamp'])\n",
    "        \n",
    "        #gen_stack_doi = gen_stack_doi_reg.groupby(['model', 'timestamp'], as_index=False).sum()\n",
    "        fig_path = os.path.join(save_dir_model,'plot1b_stack_ntl_ref_doi_{}.xlsx'.format(doi_name))\n",
    "#         shutil.copyfile(fig_template_path, fig_path)\n",
    "\n",
    "\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "            ## ExcelWriter for some reason uses writer.sheets to access the sheet.\n",
    "            ## If you leave it empty it will not know that sheet Main is already there\n",
    "            ## and will create a new sheet.\n",
    "\n",
    "            for reg in model_regs:\n",
    "                gen_stack_doi_reg = gen_stack_doi.loc[ix[:,reg,:],:].droplevel([0,1])\n",
    "                write_xlsx_stack(df=gen_stack_doi_reg, writer=writer, sheet_name=reg, palette=stack_palette)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot 1d: Generation stacks for regional days of interest of a specified reference\n",
    "\n",
    "# model_regs = reg_ids + ['JVB', 'SUM', 'UKR']\n",
    "\n",
    "\n",
    "# ### Ref model is based on highest USE....this can be changed/even selected via drop-down menu, etc.\n",
    "# ref_model = use_reg_ts.groupby('model').sum().idxmax().iloc[0]\n",
    "\n",
    "\n",
    "# if reg_ts:\n",
    "#     for m in model_names:\n",
    "#         save_dir_model = os.path.join(save_dir_plots, m)\n",
    "#         if os.path.exists(save_dir_model) is False:\n",
    "#             os.mkdir(save_dir_model)\n",
    "            \n",
    "#         ### DOI/TOI are based on reference model!\n",
    "#         model_doi = doi_summary_complete[ref_model]\n",
    "#         doi_periods = [doi for doi in model_doi.index if 'time' in doi]\n",
    "#         doi_names = [doi for doi in model_doi.index if 'time' not in doi]\n",
    "        \n",
    "#         gen_stack = gen_stack_by_reg.loc[ix[m,:,:],:]\n",
    "\n",
    "#         for i, p in enumerate(doi_periods):\n",
    "\n",
    "#             doi = model_doi.loc[p].dropna()\n",
    "#             doi_name = doi_names[i]\n",
    "#             fig_path = os.path.join(save_dir_model,'plot1d_stack_reg_ref_doi_{}.xlsx'.format(doi_name))\n",
    "            \n",
    "#             with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "      \n",
    "#                 for reg in model_regs:\n",
    "#                     toi = doi.loc[reg]\n",
    "\n",
    "\n",
    "#                     gen_stack_doi_reg = gen_stack.loc[ix[:,reg,:],:].reset_index()  \n",
    "#                     gen_stack_doi_reg = gen_stack_doi_reg.loc[(gen_stack_doi_reg.timestamp.dt.date >= toi.date() - pd.Timedelta('3D')) &\\\n",
    "#                                               (gen_stack_doi_reg.timestamp.dt.date <= toi.date() + pd.Timedelta('3D'))]\n",
    "#                     gen_stack_doi_reg = gen_stack_doi_reg.set_index(['model', geo_cols[0], 'timestamp']).droplevel(['model',geo_cols[0]])\n",
    "\n",
    "#                     write_xlsx_stack(df=gen_stack_doi_reg, writer=writer, sheet_name=reg, palette=stack_palette)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New outputs for plots only.\n",
    "To merge with rest of script at some point during clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot0: Battery plots\n",
    "\n",
    "# # Battery calculations #\n",
    "# # -------------------- #\n",
    "\n",
    "# sto_cap_reg = gen_yr_df[(gen_yr_df.Category == 'Storage')&(gen_yr_df.property == 'Installed Capacity')]\n",
    "# # sto_cap_reg.loc[:,'Subcategory'] = sto_cap_reg.name.str[:-4]\n",
    "# sto_cap_reg = sto_cap_reg.groupby(['model']+geo_cols + ['StoDuration','timestamp']).sum().value/1000\n",
    "# sto_cap_reg = sto_cap_reg.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# sto_cap_avg_duration = sto_cap_reg.copy()\n",
    "# sto_cap_avg_duration.loc[:,'Average Duration (hrs) - right-axis'] = sto_cap_avg_duration.StoDuration*sto_cap_avg_duration.value\n",
    "# sto_cap_avg_duration = sto_cap_avg_duration.rename(columns={'value': 'Installed Capacity (GW)'}).drop(columns='StoDuration')\n",
    "# sto_cap_avg_duration = sto_cap_avg_duration.groupby(['model']+geo_cols).sum()\n",
    "# sto_cap_avg_duration.loc[:,'Average Duration (hrs) - right-axis'] = sto_cap_avg_duration.loc[:,'Average Duration (hrs) - right-axis']/sto_cap_avg_duration.loc[:,'Installed Capacity (GW)']\n",
    "\n",
    "# ########\n",
    "# sto_cap_reg = sto_cap_reg.groupby(['model']+geo_cols + ['StoDuration']).sum().value.unstack('StoDuration').fillna(0)\n",
    "# sto_cap_reg.columns = ['Installed Capacity (GW) - {:0.1f} hr'.format(c) if c ==2.5 else 'Installed Capacity (GW) - {:0.0f} hr'.format(c)  for c in sto_cap_reg.columns]\n",
    "# sto_cap_reg = pd.concat([sto_cap_reg, sto_cap_avg_duration['Average Duration (hrs) - right-axis']], axis=1)\n",
    "\n",
    "\n",
    "# for m in sto_cap_reg.index.get_level_values(0):\n",
    "    \n",
    "#     save_dir_model = os.path.join(save_dir_plots, m)\n",
    "#     if os.path.exists(save_dir_model) is False:\n",
    "#         os.mkdir(save_dir_model)\n",
    "\n",
    "#     sto_cap_reg_m = sto_cap_reg.loc[ix[m,]].groupby(geo_cols[0]).sum()\n",
    "#     fig_path = os.path.join(save_dir_model,'plot0_battery_plots_{}.xlsx'.format(m))\n",
    "#     with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "#         write_xlsx_column(df=sto_cap_reg_m, writer=writer, sheet_name='battey_cap_plot', subtype='stacked', units='GW', total_scatter_col='Average Duration (hrs) - right-axis', right_ax='hours')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annual summary plots in Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_years = {}\n",
    "\n",
    "for m in model_names:\n",
    "    for y in co2_targets.index.astype(str):\n",
    "        if y in m:\n",
    "            model_years[m] = np.float64(y)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For plotting of emission tagerets vs. actual emissions\n",
    "co2_target_by_model = pd.Series({m:np.round(co2_targets[model_years[m]],2) for m in model_years.keys()} , name='Emission target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = {'load_by_reg': customer_load_by_reg.groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "             \n",
    "             'pk_load_by_reg': pd.concat([ customer_load_reg_ts.stack(geo_cols).groupby(['model', geo_cols[0]]).max().unstack(level='model')/1000,\n",
    "                                          customer_load_reg_ts.sum(axis=1).groupby(['model']).max().rename('UKR').to_frame().T/1000], axis=0),\n",
    "             \n",
    "             'pk_netload_by_reg': pd.concat([ net_load_reg_ts.stack(geo_cols).groupby(['model', geo_cols[0]]).max().unstack(level='model')/1000, \n",
    "                                   net_load_reg_ts.sum(axis=1).groupby(['model']).max().rename('UKR').to_frame().T/1000], axis=0),\n",
    "\n",
    "             \n",
    "#              'line_cap_isl': line_cap_isl['Export Limit'].rename('value').unstack(level = 'line')/1000,\n",
    "#              'line_net_exports_isl': (line_imp_exp_isl['Flow'] - line_imp_exp_isl['Flow Back']).unstack('line')/1000,\n",
    "#              'line_exports_isl': (line_imp_exp_isl['Flow']).unstack('line')/1000,\n",
    "#              'line_imports_isl': (line_imp_exp_isl['Flow Back']).unstack('line')/1000,\n",
    "             \n",
    "             'use_by_reg': use_by_reg.groupby(['model',geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "          \n",
    "             'gen_by_tech': gen_by_tech_reg.stack(geo_cols).groupby(['model', 'Category']).sum().unstack(level='Category')/1000, \n",
    "             'gen_by_reg': gen_by_tech_reg.stack(geo_cols).groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0]), \n",
    "\n",
    "             'net_gen_by_reg': gen_by_tech_reg.stack(geo_cols).groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0]).fillna(0)/1000-load_by_reg.groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "             'gen_cap_by_reg': gen_cap_tech_reg.stack(geo_cols).groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "             'gen_cap_by_tech': gen_cap_tech_reg.stack(geo_cols).groupby(['model', 'Category']).sum().unstack(level='Category')/1000,  \n",
    "\n",
    "             'cf_tech': cf_tech,\n",
    "             'cf_tech_transposed': cf_tech.T,\n",
    "             'vre_by_reg_byGen': vre_by_reg,\n",
    "             'vre_by_reg_byAv': pd.concat([vre_av_reg_abs_ts.groupby('model').sum().groupby(geo_cols[0], axis=1).sum()/1000/gen_by_tech_reg.groupby('model').sum().groupby(geo_cols[0], axis=1).sum(), \n",
    "                                           (vre_av_reg_abs_ts.groupby('model').sum().sum(axis=1)/1000/gen_by_tech_reg.groupby('model').sum().sum(axis=1)).rename('UKR')],axis=1),\n",
    "             're_by_reg': re_by_reg,\n",
    "             \n",
    "             'curtailment_rate': curtailment_rate/100,\n",
    "             're_curtailed_by_tech': re_curtailment_rate_by_tech,\n",
    "             'fuels_by_type': fuel_by_type.groupby(['model', 'Type']).sum().unstack('Type').replace(0,np.nan).dropna(axis=1,how=\"all\").fillna(0),\n",
    "             'fuels_by_subtype': fuel_by_type.groupby(['model', 'Category']).sum().unstack('Category').replace(0,np.nan).dropna(axis=1,how=\"all\").fillna(0),\n",
    "             \n",
    "             'co2_by_tech': pd.concat([co2_by_tech_reg.groupby(['model', 'Category']).sum().unstack(level='Category')/1e6, co2_target_by_model],axis=1).fillna(0),\n",
    "#              'co2_by_subfuels': co2_fuels_by_reg.groupby(['model', 'Category']).sum().unstack('Category')/1e6,\n",
    "             'co2_by_reg': pd.concat([co2_by_tech_reg.groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1e6, co2_target_by_model],axis=1).fillna(0),\n",
    "#              'co2_by_isl': co2_by_tech_reg.groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1e6,\n",
    "             'co2_intensity_reg': co2_by_reg.unstack(geo_cols).groupby(geo_cols[0], axis=1).sum()/gen_by_tech_reg.groupby('model').sum().groupby(geo_cols[0],axis=1).sum(),\n",
    "#              'co2_intensity_isl': co2_by_reg.unstack(geo_cols).groupby(geo_cols[0], axis=1).sum()/gen_by_tech_reg.groupby('model').sum().groupby(geo_cols[0],axis=1).sum(),             \n",
    "             \n",
    "\n",
    "             'op_costs_by_tech' : gen_op_costs_by_reg.groupby(['model', 'Category']).sum().unstack(level='Category'),\n",
    "             'op_costs_by_prop' : gen_op_costs_by_reg.groupby(['model', 'property']).sum().unstack(level='property'),\n",
    "             'op_and_vio_costs_by_prop': gen_op_and_vio_costs_reg.groupby(['model', 'property']).sum().unstack(level='property'),\n",
    "             'tsc_by_tech' : gen_total_costs_by_reg.groupby(['model', 'Category']).sum().unstack(level='Category'),\n",
    "             'tsc_by_prop' : gen_total_costs_by_reg.groupby(['model', 'property']).sum().unstack(level='property'),\n",
    "             'tsc_by_tech_2030' : gen_total_costs_by_reg_2030.groupby(['model', 'Category']).sum().unstack(level='Category'),\n",
    "             'tsc_by_prop_2030' : gen_total_costs_by_reg_2030.groupby(['model', 'property']).sum().unstack(level='property'),\n",
    "             \n",
    "             'lcoe_by_tech' : lcoe_tech.unstack(level='Category'),\n",
    "             'lcoe_by_tech_T' : lcoe_tech.unstack(level='model'),\n",
    "             'ramp_pc_by_reg' :pd.concat([(ramp_reg_ts.groupby(['model', geo_cols[0], 'timestamp']).sum()/daily_pk_reg_ts.stack(geo_cols).groupby(['model', geo_cols[0], 'timestamp']).sum()).groupby(['model', geo_cols[0]]).max().unstack(level=geo_cols[0])*100, \n",
    "                                          ramp_pc_ts.groupby(['model']).max().rename('UKR')], axis=1),\n",
    "             'th_ramp_pc_by_reg' :pd.concat([(th_ramp_reg_ts.groupby(['model', geo_cols[0], 'timestamp']).sum()/daily_pk_reg_ts.stack(geo_cols).groupby(['model', geo_cols[0], 'timestamp']).sum()).groupby(['model', geo_cols[0]]).max().unstack(level=geo_cols[0])*100, \n",
    "                                          th_ramp_pc_ts.groupby(['model']).max().rename('UKR')], axis=1),\n",
    "             'ramp_by_reg' : pd.concat([ramp_reg_ts.unstack(geo_cols).groupby(level=geo_cols[0],axis=1).sum().groupby(['model']).max(), ramp_ts.groupby(['model']).max().value.rename('UKR')], axis=1),\n",
    "             \n",
    "             'dsm_pk_contr': (nldc_orig.iloc[:100,:] - nldc.iloc[:100,:]).mean().rename('value').to_frame()\n",
    "            }\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_type = {'load_by_reg': 'stacked',\n",
    "\n",
    "                          \n",
    "             'pk_load_by_reg':  'clustered',\n",
    "\n",
    "             'pk_netload_by_reg': 'clustered',\n",
    "\n",
    "             \n",
    "#              'line_cap_isl': 'clustered',\n",
    "#              'line_net_exports_isl': 'clustered',\n",
    "#              'line_exports_isl': 'clustered',\n",
    "#              'line_imports_isl': 'clustered',\n",
    "             \n",
    "             'use_by_reg': 'stacked',\n",
    "\n",
    "             'gen_by_tech': 'stacked', \n",
    "             'gen_by_WEOtech': 'stacked', \n",
    "             'gen_by_reg': 'stacked', \n",
    "             'net_gen_by_reg': 'clustered', \n",
    "             'vre_by_reg_byGen': 'clustered',\n",
    "             'vre_by_reg_byAv': 'clustered',\n",
    "             're_by_reg': 'clustered',\n",
    "             \n",
    "             'fuels_by_type': 'stacked',\n",
    "             'fuels_by_subtype': 'stacked',\n",
    "             'co2_by_tech': 'stacked',\n",
    "             'co2_by_fuels': 'stacked',\n",
    "             'co2_by_subfuels': 'stacked',\n",
    "             \n",
    "             'co2_by_tech':'stacked',\n",
    "             'co2_by_reg': 'stacked',\n",
    "\n",
    "             'co2_intensity_reg': 'clustered',\n",
    "    \n",
    "             'curtailment_rate': 'clustered',\n",
    "             're_curtailed_by_tech':'clustered',\n",
    "             'gen_cap_by_reg': 'stacked',\n",
    "\n",
    "             'gen_cap_by_tech': 'stacked',\n",
    "             'gen_cap_by_WEOtech': 'stacked',\n",
    "             'cf_tech': 'clustered',\n",
    "             'cf_tech_transposed': 'clustered',\n",
    "             'op_costs_by_tech' : 'stacked',\n",
    "             'op_costs_by_prop' :'stacked',\n",
    "             'op_and_vio_costs_by_prop' :'stacked',\n",
    "             'tsc_by_tech' : 'stacked',\n",
    "             'tsc_by_prop' : 'stacked',\n",
    "             'tsc_by_tech_2030' : 'stacked',\n",
    "             'tsc_by_prop_2030' : 'stacked',\n",
    "             \n",
    "             'lcoe_by_tech' : 'clustered',\n",
    "             'lcoe_by_tech_T' : 'clustered',\n",
    "             'ramp_pc_by_reg' :'clustered',\n",
    "             'th_ramp_pc_by_reg' :'clustered',\n",
    "             'ramp_by_reg' :'clustered',\n",
    "             'th_ramp_by_reg' :'clustered',\n",
    "             'dsm_pk_contr':'clustered'\n",
    "            }\n",
    "               \n",
    "\n",
    "\n",
    "plot_units = {'load_by_reg': 'TWh',\n",
    "\n",
    "             'use_by_reg': 'TWh',\n",
    "\n",
    "             'gen_by_tech': 'TWh', \n",
    "             'gen_by_WEOtech': 'TWh', \n",
    "             'gen_by_reg': 'TWh', \n",
    "\n",
    "             'net_gen_by_reg':'TWh',\n",
    "             'vre_by_reg_byGen': '%',\n",
    "             'vre_by_reg_byAv': '%',\n",
    "             're_by_reg': '%',       \n",
    "              \n",
    "             'pk_load_by_reg':  'GW',\n",
    "\n",
    "             'pk_netload_by_reg': 'GW',\n",
    "\n",
    "              \n",
    "             'fuels_by_type': 'TJ',\n",
    "             'fuels_by_subtype': 'TJ',\n",
    "             'co2_by_tech': 'million tonnes',\n",
    "             'co2_by_fuels': 'million tonnes',\n",
    "             'co2_by_subfuels': 'million tonnes',\n",
    "              \n",
    "             'co2_by_tech':'million tonnes',\n",
    "             'co2_by_reg': 'million tonnes',\n",
    "\n",
    "             'co2_intensity_reg': 'kg/MWh',\n",
    "\n",
    "             'curtailment_rate': '%',\n",
    "              're_curtailed_by_tech':'%',\n",
    "             'gen_cap_by_reg': 'GW',\n",
    "\n",
    "             'gen_cap_by_tech': 'GW',\n",
    "             'gen_cap_by_WEOtech': 'GW',\n",
    "             'cf_tech': '%',\n",
    "             'cf_tech_transposed': '%',\n",
    "             'op_costs_by_tech' : 'USDm',\n",
    "             'op_costs_by_prop' :'USDm',\n",
    "            'op_and_vio_costs_by_prop' :'USDm',\n",
    "             'tsc_by_tech' : 'USDm',\n",
    "             'tsc_by_prop' : 'USDm',\n",
    "             'tsc_by_tech_2030' : 'USDm',\n",
    "             'tsc_by_prop_2030' : 'USDm',\n",
    "             'lcoe_by_tech' : 'USD/MWh',\n",
    "             'lcoe_by_tech_T' : 'USD/MWh',\n",
    "             'ramp_pc_by_reg' :'%/hr',\n",
    "             'th_ramp_pc_by_reg' :'%/hr',\n",
    "             'ramp_by_reg' :'MW/hr',\n",
    "             'th_ramp_by_reg' :'MW/hr',\n",
    "             'dsm_pk_contr': 'GW'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Gen by tech/reg plots per model\n",
    "# for ref_m in model_names:\n",
    "#     save_dir_model = os.path.join(save_dir_plots,ref_m)\n",
    "#     if os.path.exists(save_dir_model) is False:\n",
    "#         os.mkdir(save_dir_model)\n",
    "        \n",
    "#     fig_path = os.path.join(save_dir_model,'plot3_gen_by_tech_reg_{}.xlsx'.format(ref_m))\n",
    "#     with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "#         gen_tech_reg_m = gen_by_tech_reg.loc[ix[ref_m,:]].groupby(geo_cols[0], axis=1).sum().T/1000\n",
    "#         gen_cap_tech_reg_m = gen_cap_tech_reg.loc[ix[ref_m,:]].groupby(geo_cols[0], axis=1).sum().T/1000\n",
    "        \n",
    "        \n",
    "\n",
    "#         write_xlsx_column(df=gen_tech_reg_m, writer=writer, sheet_name='gen_tech_reg', subtype='stacked', units='TWh', palette=combined_palette)   \n",
    "#         write_xlsx_column(df=gen_cap_tech_reg_m, writer=writer, sheet_name='gen_cap_tech_reg', subtype='stacked', units='GW', palette=combined_palette)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 2: Annual summary plots by columnn\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot2_annual_summary_plots.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "    for i in plot_cols.keys():\n",
    "        total_col = None \n",
    "        \n",
    "        if plot_cols[i].shape[0] == 0:\n",
    "            print('Empty dataframe for: {}'.format(i))\n",
    "        else:\n",
    "\n",
    "            if 'UKR' in plot_cols[i].columns:\n",
    "                total_col = 'UKR'\n",
    "            elif 'co2' in i:\n",
    "                total_col = plot_cols[i].columns[-1]\n",
    "                \n",
    "            write_xlsx_column(df=plot_cols[i], writer=writer, sheet_name=i, subtype=plot_type[i], units=plot_units[i], palette=combined_palette, total_scatter_col=total_col)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 6: Cost savings plots by reference model\n",
    "\n",
    "### Get rid of cofiring if any for the purpose of comparison\n",
    "gen_op_costs_by_tech = gen_op_costs_by_reg.unstack('Category').rename(columns={'Cofiring':'Coal'}).stack().groupby(['model','Category']).sum().unstack('model').fillna(0)\n",
    "gen_total_costs_by_tech = gen_total_costs_by_reg.unstack('Category').rename(columns={'Cofiring':'Coal'}).stack().groupby(['model','Category']).sum().unstack('model').fillna(0)\n",
    "\n",
    "gen_op_costs_by_prop = gen_op_costs_by_reg.groupby(['model','property']).sum().unstack('model').fillna(0)\n",
    "gen_op_costs_wo_co2_by_prop = gen_op_costs_wo_co2_by_reg.groupby(['model','property']).sum().unstack('model').fillna(0)\n",
    "\n",
    "\n",
    "gen_total_costs_by_prop = gen_total_costs_by_reg.groupby(['model','property']).sum().unstack('model').fillna(0)\n",
    "gen_op_vio_costs_by_prop = gen_op_and_vio_costs_reg.groupby(['model','property']).sum().unstack('model').fillna(0)\n",
    "gen_op_vio_costs_wo_co2_by_prop = gen_op_and_vio_costs_reg_wo_co2.groupby(['model','property']).sum().unstack('model').fillna(0)\n",
    "\n",
    "\n",
    "for ref_m in model_names:\n",
    "    save_dir_model = os.path.join(save_dir_plots,ref_m.replace(\"/\",\"\"))\n",
    "    if os.path.exists(save_dir_model) is False:\n",
    "        os.mkdir(save_dir_model)\n",
    "        \n",
    "    fig_path = os.path.join(save_dir_model,'plot6_cost_savings_ref_{}.xlsx'.format(ref_m.replace(\"/\",\"\")))\n",
    "    with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "        ref_op_prop = gen_op_costs_by_prop[ref_m]\n",
    "        ref_op_prop_wo_co2 = gen_op_costs_wo_co2_by_prop[ref_m]\n",
    "        ref_op_vio_prop = gen_op_vio_costs_by_prop[ref_m]\n",
    "        ref_op_vio_prop_wo_co2 = gen_op_vio_costs_wo_co2_by_prop[ref_m]\n",
    "\n",
    "        ref_tsc_prop = gen_total_costs_by_prop[ref_m]\n",
    "        ref_op_tech = gen_op_costs_by_tech[ref_m]\n",
    "        ref_tsc_tech = gen_total_costs_by_tech[ref_m]\n",
    "\n",
    "        savings_op_by_prop =  (-gen_op_costs_by_prop).drop(columns=ref_m).subtract(-ref_op_prop, axis=0).T\n",
    "        savings_op_vio_by_prop =  (-gen_op_vio_costs_by_prop).drop(columns=ref_m).subtract(-ref_op_vio_prop, axis=0).T\n",
    "        savings_op_wo_co2_by_prop =  (-gen_op_costs_wo_co2_by_prop).drop(columns=ref_m).subtract(-ref_op_prop_wo_co2, axis=0).T\n",
    "        savings_op_vio_wo_co2_by_prop =  (-gen_op_vio_costs_wo_co2_by_prop).drop(columns=ref_m).subtract(-ref_op_vio_prop_wo_co2, axis=0).T\n",
    "        savings_tsc_by_prop = (-gen_total_costs_by_prop).drop(columns=ref_m).subtract(-ref_tsc_prop, axis=0).T\n",
    "        savings_op_by_tech = (-gen_op_costs_by_tech).drop(columns=ref_m).subtract(-ref_op_tech, axis=0).T\n",
    "        savings_tsc_by_tech = (-gen_total_costs_by_tech).drop(columns=ref_m).subtract(-ref_tsc_tech, axis=0).T\n",
    "        \n",
    "        savings_op_by_prop_pc  =  savings_op_by_prop/ref_op_prop.sum()\n",
    "        savings_op_vio_by_prop_pc  =  savings_op_vio_by_prop/ref_op_vio_prop.sum()\n",
    "        savings_op_wo_co2_by_prop_pc  =  savings_op_wo_co2_by_prop/ref_op_prop_wo_co2.sum()\n",
    "        savings_op_vio_wo_co2_by_prop_pc  =  savings_op_vio_wo_co2_by_prop/ref_op_vio_prop_wo_co2.sum()\n",
    "        savings_tsc_by_prop_pc = savings_tsc_by_prop/ref_tsc_prop.sum()\n",
    "        savings_op_by_tech_pc = savings_op_by_tech/ref_op_tech.sum()\n",
    "        savings_tsc_by_tech_pc = savings_tsc_by_tech/ref_tsc_tech.sum()\n",
    "        \n",
    "        write_xlsx_column(df=savings_op_by_prop, writer=writer, sheet_name='savings_op_by_prop', subtype='stacked', units='USDm', total_scatter_col='Total savings')   \n",
    "        write_xlsx_column(df=savings_op_vio_by_prop, writer=writer, sheet_name='savings_op_vio_by_prop', subtype='stacked', units='USDm', total_scatter_col='Total savings')   \n",
    "        write_xlsx_column(df=savings_op_wo_co2_by_prop, writer=writer, sheet_name='savings_op_wo_co2_by_prop', subtype='stacked', units='USDm', total_scatter_col='Total savings')   \n",
    "        write_xlsx_column(df=savings_op_vio_wo_co2_by_prop, writer=writer, sheet_name='savings_op_vio_wo_co2_by_prop', subtype='stacked', units='USDm', total_scatter_col='Total savings')   \n",
    "        write_xlsx_column(df=savings_tsc_by_prop, writer=writer, sheet_name='savings_tsc_by_prop', subtype='stacked', units='USDm', total_scatter_col='Total savings')     \n",
    "        write_xlsx_column(df=savings_op_by_tech, writer=writer, sheet_name='savings_op_by_tech', subtype='stacked', units='USDm', total_scatter_col='Total savings')   \n",
    "        write_xlsx_column(df=savings_tsc_by_tech, writer=writer, sheet_name='savings_tsc_by_tech', subtype='stacked', units='USDm', total_scatter_col='Total savings') \n",
    "        \n",
    "        write_xlsx_column(df=savings_op_by_prop_pc, writer=writer, sheet_name='savings_op_by_prop_pc', subtype='stacked', units='', total_scatter_col='Relative savings')   \n",
    "        write_xlsx_column(df=savings_op_vio_by_prop_pc, writer=writer, sheet_name='savings_op_vio_by_prop_pc', subtype='stacked', units='', total_scatter_col='Relative savings') \n",
    "        write_xlsx_column(df=savings_op_wo_co2_by_prop_pc, writer=writer, sheet_name='savings_op_wo_co2_by_prop_pc', subtype='stacked', units='', total_scatter_col='Relative savings')   \n",
    "        write_xlsx_column(df=savings_op_vio_wo_co2_by_prop_pc, writer=writer, sheet_name='savings_opvio_noco2_by_prop_pc', subtype='stacked', units='', total_scatter_col='Relative savings')   \n",
    "        write_xlsx_column(df=savings_tsc_by_prop_pc, writer=writer, sheet_name='savings_tsc_by_prop_pc', subtype='stacked', units='', total_scatter_col='Relative savings')     \n",
    "        write_xlsx_column(df=savings_op_by_tech_pc, writer=writer, sheet_name='savings_op_by_tech_pc', subtype='stacked', units='', total_scatter_col='Relative savings')   \n",
    "        write_xlsx_column(df=savings_tsc_by_tech_pc, writer=writer, sheet_name='savings_tsc_by_tech_pc', subtype='stacked', units='', total_scatter_col='Relative savings')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot7: CO2 savings plots by reference model\n",
    "\n",
    "### Get rid of cofiring if any for the purpose of comparison\n",
    "co2_by_tech = co2_by_tech_reg.unstack('Category').rename(columns={'Cofiring':'Coal'}).stack().groupby(['model','Category']).sum().unstack('model').fillna(0)/1e6\n",
    "co2_by_reg_plt = co2_by_tech_reg.groupby(['model',geo_cols[0]]).sum().unstack('model').fillna(0)/1e6\n",
    "\n",
    "em_by_type = em_by_type_tech_reg.groupby(['model','parent']).sum().value.unstack('model')/1e6\n",
    "\n",
    "\n",
    "for ref_m in model_names:\n",
    "    save_dir_model = os.path.join(save_dir_plots,ref_m.replace(\"/\",\"\"))\n",
    "    if os.path.exists(save_dir_model) is False:\n",
    "        os.mkdir(save_dir_model)\n",
    "        \n",
    "    fig_path = os.path.join(save_dir_model,'plot7_co2_savings_ref_{}.xlsx'.format(ref_m.replace(\"/\",\"\")))\n",
    "    with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "        ref_co2_tech = co2_by_tech[ref_m]\n",
    "        ref_co2_reg = co2_by_reg_plt[ref_m]\n",
    "        ref_em_type = em_by_type[ref_m]\n",
    "\n",
    "        co2_savings_by_tech =  (-co2_by_tech).drop(columns=ref_m).subtract(-ref_co2_tech, axis=0).T\n",
    "        co2_savings_by_reg = (-co2_by_reg_plt).drop(columns=ref_m).subtract(-ref_co2_reg, axis=0).T\n",
    "\n",
    "        em_savings_by_reg = (-em_by_type).drop(columns=ref_m).subtract(-ref_em_type, axis=0).T\n",
    "        \n",
    "        co2_savings_by_tech_pc =  (-co2_by_tech).drop(columns=ref_m).subtract(-ref_co2_tech, axis=0).T/ref_co2_tech.sum()\n",
    "        co2_savings_by_reg_pc = (-co2_by_reg_plt).drop(columns=ref_m).subtract(-ref_co2_reg, axis=0).T/ref_co2_reg.sum()\n",
    "        em_savings_by_reg_pc = (-em_by_type).drop(columns=ref_m).subtract(-ref_em_type, axis=0).T/ref_em_type\n",
    "        \n",
    "        write_xlsx_column(df=co2_savings_by_tech, writer=writer, sheet_name='co2_savings_by_tech_abs', subtype='stacked', units='million tonnes', total_scatter_col='Total reduction')   \n",
    "        write_xlsx_column(df=co2_savings_by_reg, writer=writer, sheet_name='co2_savings_by_reg_abs', subtype='stacked', units='million tonnes', total_scatter_col='Total reduction')    \n",
    "\n",
    "        write_xlsx_column(df=em_savings_by_reg, writer=writer, sheet_name='em_savings_by_type_abs', subtype='stacked', units='million tonnes', total_scatter_col='Total reduction') \n",
    "        \n",
    "        write_xlsx_column(df=co2_savings_by_tech_pc, writer=writer, sheet_name='co2_savings_by_tech_pc', subtype='stacked', units='', total_scatter_col='Relative reduction')   \n",
    "        write_xlsx_column(df=co2_savings_by_reg_pc, writer=writer, sheet_name='co2_savings_by_reg_pc', subtype='stacked', units='', total_scatter_col='Relative reduction')    \n",
    "\n",
    "        write_xlsx_column(df=em_savings_by_reg_pc, writer=writer, sheet_name='em_savings_by_type_pc', subtype='clustered', units='')  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-series summary plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 3: Daily gap plot\n",
    "\n",
    "nl_dly_gap_complete = pd.concat([nl_dly_gap_reg, nl_dly_gap.rename('UKR')], axis=1)\n",
    "nl_dly_gap_pc_complete = pd.concat([nl_dly_gap_reg_pc, nl_dly_gap_pc.rename('UKR')], axis=1)\n",
    "\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot3_daily_gap.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "    for reg in nl_dly_gap_complete.columns:\n",
    "        nl_dly_gap_plot = nl_dly_gap_complete[reg].unstack('model')\n",
    "        \n",
    "        write_xlsx_scatter(df=nl_dly_gap_plot, writer=writer, sheet_name=reg, palette=combined_palette, units='MW', markersize=4, alpha=90) \n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot3_daily_gap_pc.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "    for reg in nl_dly_gap_complete.columns:\n",
    "        nl_dly_gap_pc_plot = nl_dly_gap_pc_complete[reg].unstack('model')\n",
    "        \n",
    "       \n",
    "        write_xlsx_scatter(df=nl_dly_gap_pc_plot, writer=writer, sheet_name=reg, palette=combined_palette, units='', markersize=4, alpha=90) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Plot 3: Annual summary line plots\n",
    "\n",
    "plot_lines = {'ldc':ldc/1000,\n",
    "             'nldc':nldc/1000,\n",
    "             'nldc_curtail':nldc_curtail/1000,\n",
    "             'nldc_sto':nldc_sto/1000,\n",
    "             'nldc_sto_curtail':nldc_sto_curtail/1000,\n",
    "             'curtailment_dc':curtailment_dc/1000\n",
    "            }      \n",
    "\n",
    "ln_plot_type = {'ldc':'ldc',\n",
    "             'nldc':'ldc',\n",
    "             'nldc_curtail':'ldc',\n",
    "             'nldc_sto':'ldc',\n",
    "             'nldc_sto_curtail':'ldc',\n",
    "             'curtailment_dc':'ldc',\n",
    "\n",
    "            }      \n",
    " \n",
    "\n",
    "ln_plot_units = {'ldc':'GW',\n",
    "             'nldc':'GW',\n",
    "             'nldc_curtail':'GW',\n",
    "             'nldc_sto':'GW',\n",
    "             'nldc_sto_curtail':'GW',\n",
    "             'curtailment_dc':'GW',\n",
    "             \n",
    "            }      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 3: Annual summary plots by columnn\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot3_ldc_plots.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "    for i in plot_lines.keys():\n",
    "        write_xlsx_line(df=plot_lines[i], writer=writer, sheet_name=i,subtype=ln_plot_type[i], units=ln_plot_units[i], line_width=1.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 3: Annual summary plots by columnn\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot3_ldc_plots_by_model.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "    for i in plot_lines.keys():\n",
    "        write_xlsx_line(df=plot_lines[i], writer=writer, sheet_name=i,subtype=ln_plot_type[i], units=ln_plot_units[i], line_width=1.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 5: Inertia plots\n",
    "\n",
    "inertia_geo_col = [geo_cols[0]]\n",
    "\n",
    "for col in inertia_geo_col:\n",
    "    inertia_col = inertia_by_reg.groupby([col,'model', 'timestamp']).sum()\n",
    "    for reg in inertia_col.index.get_level_values(col).unique():\n",
    "        \n",
    "        inertia_reg = inertia_col.loc[ix[reg,]]\n",
    "        \n",
    "        fig_path = os.path.join(save_dir_plots,'plot5_inertia_plot_{}.xlsx'.format(reg))\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "            for m in model_names:\n",
    "                inertia_plot = inertia_reg.loc[ix[m,]].rename(columns={'InertiaLo':'Inertia (lower range)', 'InertiaHi':'Inertia (higher range)'})\n",
    "                write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs') \n",
    "                write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs', to_combine=True) \n",
    "                \n",
    "                \n",
    "fig_path = os.path.join(save_dir_plots,'plot5_inertia_plot_total.xlsx')\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for m in model_names:\n",
    "        inertia_plot = total_inertia_ts.loc[ix[m,]].rename(columns={'InertiaLo':'Inertia (lower range)', 'InertiaHi':'Inertia (higher range)'})\n",
    "        write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs', line_width=0.1)\n",
    "        write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs', to_combine=True, line_width=0.1) \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Output 5: Inertia plots\n",
    "\n",
    "inertia_geo_col = [geo_cols[0]]\n",
    "\n",
    "for col in inertia_geo_col:\n",
    "    \n",
    "    inertia_col = inertia_by_reg.groupby([pd.Grouper(level=col), pd.Grouper(level='model'),pd.Grouper(level='timestamp', freq='D')]).min()\n",
    "    \n",
    "    for reg in inertia_col.index.get_level_values(col).unique():\n",
    "        \n",
    "        inertia_reg = inertia_col.loc[ix[reg,]]\n",
    "        \n",
    "        fig_path = os.path.join(save_dir_plots,'plot5_inertia_plot_daily_min_{}.xlsx'.format(reg))\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "            for m in model_names:\n",
    "                inertia_plot = inertia_reg.loc[ix[m,]].rename(columns={'InertiaLo':'Inertia (lower range)', 'InertiaHi':'Inertia (higher range)'})\n",
    "                write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs') \n",
    "                write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs', to_combine=True) \n",
    "                \n",
    "                \n",
    "fig_path = os.path.join(save_dir_plots,'plot5_inertia_plot_total_daily_min.xlsx')\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for m in model_names:\n",
    "        inertia_plot = total_inertia_ts.loc[ix[m,]].rename(columns={'InertiaLo':'Inertia (lower range)', 'InertiaHi':'Inertia (higher range)'})\n",
    "        inertia_plot = inertia_plot.groupby(pd.Grouper(level='timestamp', freq='D')).min()\n",
    "        write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs', line_width=0.1)\n",
    "        write_xlsx_line(df=inertia_plot, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), subtype='timeseries', units='MWs', to_combine=True, line_width=0.1) \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 4a: GDCs\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot4a_gdc_plots_national.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for tech in gen_by_tech_ts.columns:\n",
    "\n",
    "\n",
    "        gdc = gen_by_tech_ts[tech].unstack('model')/1000\n",
    "        gdc = pd.DataFrame(np.flipud(np.sort(gdc.values, axis=0)), index=gdc.index, columns=gdc.columns)\n",
    "        write_xlsx_line(df=gdc, writer=writer, sheet_name=tech, subtype='ldc', units='GW') \n",
    "\n",
    "### Output 4b: GDC regs\n",
    "\n",
    "gdc_reg_dict = {}\n",
    "gdc_geo_col = [geo_cols[0]]\n",
    "\n",
    "for col in gdc_geo_col:\n",
    "    gen_by_tech_col = gen_by_tech_reg_ts.groupby(['model',col,'timestamp']).sum()\n",
    "    \n",
    "    for reg in gen_by_tech_col.index.get_level_values(col).unique():\n",
    "        \n",
    "        fig_path = os.path.join(save_dir_plots,'plot4a_gdc_plots_{}.xlsx'.format(reg))\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "        \n",
    "            for tech in gen_by_tech_col.columns:\n",
    "\n",
    "                gdc_reg = gen_by_tech_col[tech].unstack('model').loc[reg]\n",
    "                gdc_reg = pd.DataFrame(np.flipud(np.sort(gdc_reg.values, axis=0)), index=gdc_reg.index, columns=gdc_reg.columns)\n",
    "                write_xlsx_line(df=gdc_reg, writer=writer, sheet_name=tech, subtype='ldc', units='GW')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 4c: Normalised GDCs\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot4c_gdc_normalised_plots_national.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for tech in gen_by_tech_ts.columns:\n",
    "        ### In case of validation being present, we need to include only the models as per the model_names\n",
    "        cap = gen_cap_tech_reg.sum(axis=1).unstack('model').loc[tech,][model_names]\n",
    "        gdc_norm = np.divide(gen_by_tech_ts[tech].unstack('model'), cap)\n",
    "        gdc_norm = pd.DataFrame(np.flipud(np.sort(gdc_norm.values, axis=0)), index=gdc_norm.index, columns=gdc_norm.columns)\n",
    "        write_xlsx_line(df=gdc_norm, writer=writer, sheet_name=tech, subtype='ldc', units='-')  \n",
    "        \n",
    "### Output 4d: GDC normalised regs\n",
    "\n",
    "gdc_reg_dict = {}\n",
    "gdc_geo_col = [geo_cols[0]]\n",
    "\n",
    "for col in gdc_geo_col:\n",
    "    gen_by_tech_col = gen_by_tech_reg_ts.groupby(['model',col,'timestamp']).sum()\n",
    "    gen_cap_tech_col = gen_cap_tech_reg.groupby(col, axis=1).sum()\n",
    "\n",
    "    \n",
    "    for reg in gen_by_tech_col.index.get_level_values(col).unique():\n",
    "        \n",
    "        fig_path = os.path.join(save_dir_plots,'plot4d_gdc_normalised_plots_{}.xlsx'.format(reg))\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "        \n",
    "            for tech in gen_by_tech_col.columns:\n",
    "                        ### In case of validation being present, we need to include only the models as per the model_names\n",
    "                cap = gen_cap_tech_col[reg].unstack('model').loc[tech][model_names]\n",
    "                gdc_reg_norm = np.divide(gen_by_tech_col[tech].unstack('model').loc[reg],cap)\n",
    "                gdc_reg_norm = pd.DataFrame(np.flipud(np.sort(gdc_reg_norm.values, axis=0)), index=gdc_reg_norm.index, columns=gdc_reg_norm.columns)\n",
    "                write_xlsx_line(df=gdc_reg_norm, writer=writer, sheet_name=tech, subtype='ldc', units='GW')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 5a: GDC zoomed\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot5a_gdc_1pc_national.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for tech in gen_by_tech_ts.columns:\n",
    "\n",
    "\n",
    "        gdc = gen_by_tech_ts[tech].unstack('model')/1000\n",
    "        gdc = pd.DataFrame(np.flipud(np.sort(gdc.values, axis=0)), index=gdc.index, columns=gdc.columns)\n",
    "        gdc = gdc.iloc[:int(np.ceil(gdc.shape[0]*0.01))]\n",
    "        gdc_idx = (np.arange(0,gdc.shape[0]) + 1)/gdc.shape[0]*0.01\n",
    "        write_xlsx_line(df=gdc, writer=writer, sheet_name=tech, subtype='ldc', units='GW', ldc_idx=gdc_idx)\n",
    "\n",
    "### Output 5b: GDC regs zoomed\n",
    "\n",
    "gdc_reg_dict = {}\n",
    "gdc_geo_col = [geo_cols[0]]\n",
    "\n",
    "for col in gdc_geo_col:\n",
    "    gen_by_tech_col = gen_by_tech_reg_ts.groupby(['model',col,'timestamp']).sum()\n",
    "    \n",
    "    for reg in gen_by_tech_col.index.get_level_values(col).unique():\n",
    "        \n",
    "        fig_path = os.path.join(save_dir_plots,'plot5a_gdc_1pc_{}.xlsx'.format(reg))\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "        \n",
    "            for tech in gen_by_tech_col.columns:\n",
    "                gdc_reg = gen_by_tech_col[tech].unstack('model').loc[reg]\n",
    "                gdc_reg = pd.DataFrame(np.flipud(np.sort(gdc_reg.values, axis=0)), index=gdc_reg.index, columns=gdc_reg.columns)\n",
    "                gdc_reg = gdc_reg.iloc[:int(np.ceil(gdc_reg.shape[0]*0.01))]\n",
    "                gdc_idx = (np.arange(0,gdc_reg.shape[0]) + 1)/gdc_reg.shape[0]*0.01\n",
    "                write_xlsx_line(df=gdc_reg, writer=writer, sheet_name=tech, subtype='ldc', units='GW', ldc_idx=gdc_idx)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 4c: Normalised GDCs zoomed to top 1%\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot5c_gdc_normalised_1pc_national.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for tech in gen_by_tech_ts.columns:\n",
    "        ### In case of validation being present, we need to include only the models as per the model_names\n",
    "        cap = gen_cap_tech_reg.sum(axis=1).unstack('model').loc[tech,][model_names]\n",
    "        gdc_norm = np.divide(gen_by_tech_ts[tech].unstack('model'), cap)\n",
    "        gdc_norm = pd.DataFrame(np.flipud(np.sort(gdc_norm.values, axis=0)), index=gdc_norm.index, columns=gdc_norm.columns)\n",
    "        gdc_norm = gdc_norm.iloc[:int(np.ceil(gdc_norm.shape[0]*0.01))]\n",
    "        gdc_idx = (np.arange(0,gdc_norm.shape[0]) + 1)/gdc_norm.shape[0]*0.01\n",
    "        write_xlsx_line(df=gdc_norm, writer=writer, sheet_name=tech, subtype='ldc', units='-', ldc_idx=gdc_idx)  \n",
    "        \n",
    "### Output 4d: GDC normalised regs zoomed to top 1%\n",
    "\n",
    "gdc_reg_dict = {}\n",
    "gdc_geo_col = [geo_cols[0]]\n",
    "\n",
    "for col in gdc_geo_col:\n",
    "    gen_by_tech_col = gen_by_tech_reg_ts.groupby(['model',col,'timestamp']).sum()\n",
    "    gen_cap_tech_col = gen_cap_tech_reg.groupby(col, axis=1).sum()\n",
    "\n",
    "    \n",
    "    for reg in gen_by_tech_col.index.get_level_values(col).unique():\n",
    "        \n",
    "        fig_path = os.path.join(save_dir_plots,'plot4d_gdc_normalised_1pc_{}.xlsx'.format(reg))\n",
    "        with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "        \n",
    "            for tech in gen_by_tech_col.columns:\n",
    "                ### In case of validation being present, we need to include only the models as per the model_names\n",
    "                cap = gen_cap_tech_col[reg].unstack('model').loc[tech][model_names]\n",
    "                gdc_reg_norm = np.divide(gen_by_tech_col[tech].unstack('model').loc[reg],cap)\n",
    "                gdc_reg_norm = pd.DataFrame(np.flipud(np.sort(gdc_reg_norm.values, axis=0)), index=gdc_reg_norm.index, columns=gdc_reg_norm.columns)\n",
    "                gdc_reg_norm = gdc_reg_norm.iloc[:int(np.ceil(gdc_reg_norm.shape[0]*0.01))]\n",
    "                gdc_idx = (np.arange(0,gdc_reg_norm.shape[0]) + 1)/gdc_reg_norm.shape[0]*0.01\n",
    "                write_xlsx_line(df=gdc_reg_norm, writer=writer, sheet_name=tech, subtype='ldc', units='GW', ldc_idx=gdc_idx)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 4c: CF tech/reg\n",
    "\n",
    "### Future-proofing\n",
    "cf_geo_col = [geo_cols[0]]\n",
    "cf_geo_col = [c for c in cf_geo_col if c in geo_cols]\n",
    "if len(cf_geo_col) == 0:\n",
    "    cf_geo_col = geo_cols[:1]\n",
    "\n",
    "for col in cf_geo_col:\n",
    "    cf_col = cf_tech_reg.stack(geo_cols).groupby([col,'model', 'Category']).sum()\n",
    "    fig_path = os.path.join(save_dir_plots,'plot6_cf_plots_reg.xlsx')\n",
    "    with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "        for reg in cf_col.index.get_level_values(col).unique():\n",
    "\n",
    "            cf_reg = cf_col.loc[ix[reg,]].unstack('Category')\n",
    "            write_xlsx_column(df=cf_reg, writer=writer, sheet_name=reg, subtype='clustered', units='%')\n",
    "            write_xlsx_column(df=cf_reg.T, writer=writer, sheet_name='{}_T'.format(reg), subtype='clustered', units='%')  \n",
    "\n",
    "        ### Write total\n",
    "        write_xlsx_column(df=cf_tech, writer=writer, sheet_name='UKR', subtype='clustered', units='%')  \n",
    "        write_xlsx_column(df=cf_tech.T, writer=writer, sheet_name='UKR_T', subtype='clustered', units='%')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output 6: Seasonality by tech\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot6a_seasonality_plots_by_tech_national.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for tech in gen_stack_total.drop(columns=['Load2']).columns:\n",
    "        col_monthly = (gen_stack_total[tech].reset_index().groupby(['model'] + [pd.Grouper(key='timestamp', freq='M')]).sum()/1e6).unstack('model').droplevel(0, axis=1)\n",
    "        write_xlsx_line(df=col_monthly, writer=writer, sheet_name=tech, units='TWh', palette=combined_palette)\n",
    "\n",
    "fig_path = os.path.join(save_dir_plots,'plot6b_seasonality_plots_by_model_national.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "\n",
    "    for m in model_names:\n",
    "        col_monthly = gen_stack_total[gen_yr_df.Category.unique()].loc[ix[m,],].reset_index().groupby([pd.Grouper(key='timestamp', freq='M')]).sum()/1e6\n",
    "        write_xlsx_stack(df=col_monthly, writer=writer, sheet_name=m[:31].replace(\"/\",\"\"), units='TWh', palette=stack_palette)\n",
    "        \n",
    "# def write_xlsx_stack(df, writer, excel_file=None, sheet_name='Sheet1', palette=stack_palette, units='MW', to_combine=False):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TEST Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = pd.date_range('1/1/2019', periods=8760, freq='60min')\n",
    "calplot.calplot(net_load_ts.loc[ix[model_names[0],:]].reset_index().groupby(pd.Grouper(key='timestamp', freq='D')).max(numeric_only=True).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calplot.calplot(ramp_ts[ramp_ts.index.get_level_values('timestamp').hour != 0].reset_index().groupby(pd.Grouper(key='timestamp', freq='D')).max().value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_demand = pd.read_excel(\"R:/RISE/DOCS/04 PROJECTS/COUNTRIES/INDONESIA/Power system enhancement 2020_21/Modelling/01 InputData/03 Demand/from_WEO/\\\n",
    "WEO_demand_input_byRegion_NZE_2040_20220415.xlsx\", sheet_name=\"Total\", header =2)\n",
    "\n",
    "\n",
    "### drop leap days\n",
    "input_demand = input_demand[~((input_demand.Datetime.dt.month == 2)&(input_demand.Datetime.dt.day == 29))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_factor = 1.066\n",
    "\n",
    "if 'NZE' in soln_choice.value:\n",
    "    input_load_reg = input_demand.set_index('Datetime').sum().rename('2040 NZE - Input').to_frame().T/1000*loss_factor    \n",
    "    input_pk_load_reg = input_demand.set_index('Datetime').max().rename('2040 NZE - Input').to_frame().T*loss_factor\n",
    "else:\n",
    "    input_load_reg = pd.DataFrame(None)      \n",
    "    input_pk_load_reg = pd.DataFrame(None)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cols = {\n",
    "             'load_by_reg': pd.concat([customer_load_by_reg.groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "                               input_load_reg],axis=0),\n",
    "             'pk_load_by_reg':pd.concat([pd.concat([customer_load_reg_ts.stack(geo_cols).groupby(['model', geo_cols[0]]).max().unstack(level=geo_cols[0]), customer_load_reg_ts.sum(axis=1).groupby(['model']).max().rename('UKR')],axis=1)/1000,\n",
    "                              input_pk_load_reg],axis=0),\n",
    "             'pk_netload_by_reg': pd.concat([net_load_reg_ts.stack(geo_cols).groupby(['model', geo_cols[0]]).max().unstack(level=geo_cols[0]), net_load_reg_ts.stack(geo_cols).groupby(['model']).max().rename('UKR')],axis=1)/1000,\n",
    "    \n",
    "\n",
    "             'use_by_reg': use_by_reg.groupby(['model',geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "             'gen_by_reg': gen_by_tech_reg.stack(geo_cols).groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0]).fillna(0)/1000, \n",
    "             'net_gen_by_reg': gen_by_tech_reg.stack(geo_cols).groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0]).fillna(0)/1000-load_by_reg.groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "             'gen_by_tech': gen_by_tech_reg.stack(geo_cols).groupby(['model', 'Category']).sum().unstack(level='Category').fillna(0)/1000, \n",
    "#              'gen_by_WEOtech': gen_by_weoTech_reg.groupby(['model', 'WEO_Tech_simpl']).sum().sum(axis=1).unstack(level='WEO_Tech_simpl').fillna(0)/1000, \n",
    "             'gen_cap_by_reg':  gen_cap_tech_reg.stack(geo_cols).groupby(['model', geo_cols[0]]).sum().unstack(level=geo_cols[0])/1000,\n",
    "             'gen_cap_by_tech': gen_cap_tech_reg.stack(geo_cols).groupby(['model', 'Category']).sum().unstack(level='Category').fillna(0)/1000,  \n",
    "#              'gen_cap_by_WEOtech': gen_cap_by_weoTech_reg.groupby(['model', 'WEO_Tech_simpl']).sum().sum(axis=1).unstack(level='WEO_Tech_simpl').fillna(0)/1000,  \n",
    "             'vre_by_reg_byGen': vre_by_reg,\n",
    "             'vre_by_reg_byAv': pd.concat([vre_av_reg_abs_ts.groupby('model').sum().groupby(geo_cols[0], axis=1).sum()/1000/gen_by_tech_reg.groupby('model').sum().groupby(geo_cols[0], axis=1).sum(), \n",
    "                                           (vre_av_reg_abs_ts.groupby('model').sum().sum(axis=1)/1000/gen_by_tech_reg.groupby('model').sum().sum(axis=1)).rename('UKR')],axis=1),\n",
    "             're_by_reg': re_by_reg,\n",
    "             'curtailment_rate': curtailment_rate/100,\n",
    "            'cf_tech_transposed': cf_tech.T,\n",
    "            'co2_by_tech': pd.concat([co2_by_tech_reg.groupby(['model', 'Category']).sum().unstack(level='Category')/1e6, co2_target_by_model],axis=1),\n",
    "            'op_and_vio_costs_by_prop': gen_op_and_vio_costs_reg.groupby(['model', 'property']).sum().unstack(level='property'),\n",
    "            }      \n",
    "\n",
    "\n",
    "summary_type = {\n",
    "             'load_by_reg': 'stacked',\n",
    "             'pk_load_by_reg':  'clustered',\n",
    "             'pk_netload_by_reg': 'clustered',\n",
    "    \n",
    "             'line_cap_reg': 'clustered',\n",
    "             'line_net_exports_reg': 'clustered',\n",
    "             'net_gen_by_reg':'clustered',\n",
    "    \n",
    "             'use_by_reg': 'stacked',\n",
    "             'gen_by_tech': 'stacked', \n",
    "             'gen_by_WEOtech': 'stacked', \n",
    "             'gen_by_reg': 'stacked', \n",
    "             'net_gen_by_reg':'clustered',\n",
    "             'gen_cap_by_reg': 'stacked',\n",
    "             'gen_cap_by_tech': 'stacked',\n",
    "             'gen_cap_by_WEOtech': 'stacked',\n",
    "             'vre_by_reg_byGen': 'clustered',\n",
    "             'vre_by_reg_byAv': 'clustered',\n",
    "             're_by_reg': 'clustered',\n",
    "             'curtailment_rate': 'clustered',\n",
    "            'cf_tech_transposed': 'clustered',\n",
    "            'co2_by_tech': 'stacked',\n",
    "            'op_and_vio_costs_by_prop' : 'stacked'\n",
    "}\n",
    "        \n",
    "summary_units = {\n",
    "             'load_by_reg': 'TWh',\n",
    "             'use_by_reg': 'TWh',\n",
    "             'gen_by_tech': 'TWh', \n",
    "             'gen_by_WEOtech': 'TWh', \n",
    "             'gen_by_reg': 'TWh', \n",
    "             'net_gen_by_reg':'TWh',\n",
    "             'gen_cap_by_reg': 'GW',\n",
    "             'gen_cap_by_tech': 'GW',\n",
    "             'gen_cap_by_WEOtech': 'GW',\n",
    "             'vre_by_reg_byGen': '%',\n",
    "             'vre_by_reg_byAv': '%',\n",
    "             're_by_reg': '%',    \n",
    "             'pk_load_by_reg':  'GW',\n",
    "             'pk_netload_by_reg': 'GW',\n",
    "    \n",
    "             'line_cap_reg': 'GW',\n",
    "             'line_net_exports_reg': 'TWh',\n",
    "             'curtailment_rate': '%',\n",
    "            'cf_tech_transposed': '%',\n",
    "            'co2_by_tech': 'million tonnes',\n",
    "            'op_and_vio_costs_by_prop' : 'USDm'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 2: High-level summary\n",
    "fig_path = os.path.join(save_dir_plots,'plot2b_high_level_summary_plots.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(fig_path, engine='xlsxwriter') as writer:\n",
    "    \n",
    "    for i in summary_cols.keys():   \n",
    "        if summary_cols[i].shape[0] == 0:\n",
    "            print('mpty dataframe for: {}'.format(i))\n",
    "        else:\n",
    "            \n",
    "            if 'UKR' in summary_cols[i].columns:\n",
    "                total_col = 'UKR'\n",
    "            elif 'co2' in i:\n",
    "                total_col= summary_cols[i].columns[-1]\n",
    "            else:\n",
    "                total_col = None            \n",
    "            \n",
    "            write_xlsx_column(df=summary_cols[i], writer=writer, sheet_name=i, subtype=summary_type[i], units=summary_units[i], palette=combined_palette, total_scatter_col=total_col, total_fill=iea_palette['yl'])   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_plot_table(df):\n",
    "\n",
    "    data = df.values\n",
    "\n",
    "    columns = df.columns\n",
    "    rows = summary_cols[i].index\n",
    "\n",
    "    # Get some pastel shades for the colors\n",
    "    colors = plt.cm.BuPu(np.linspace(0, 0.5, len(rows)))\n",
    "    n_rows = len(data)\n",
    "\n",
    "    index = np.arange(len(columns)) + 0.3\n",
    "    bar_width = 0.4\n",
    "\n",
    "    # Initialize the vertical-offset for the stacked bar chart.\n",
    "    y_offset = np.array([0.0] * len(columns))\n",
    "\n",
    "    # Plot bars and create text labels for the table\n",
    "    cell_text = []\n",
    "    for row in range(n_rows):\n",
    "        cell_text.append(['{:.2f}'.format(x) for x in data_row])\n",
    "    # Reverse colors and text labels to display the last value at the top.\n",
    "    colors = colors[::-1]\n",
    "    cell_text.reverse()\n",
    "\n",
    "#     # Add a table at the bottom of the axes\n",
    "#     the_table = plt.table(cellText=cell_text,\n",
    "#                           rowLabels=rows,\n",
    "#                           rowColours=colors,\n",
    "#                           colLabels=columns,\n",
    "#                           loc='bottom')\n",
    "\n",
    "    return cell_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the PdfPages object to which we will save the pages:\n",
    "# The with statement makes sure that the PdfPages object is closed properly at\n",
    "# the end of the block, even if an Exception occurs.\n",
    "with PdfPages(os.path.join(save_dir,'summary_info.pdf')) as pdf:\n",
    "    \n",
    "    ### Plot 2: Annual summary plots by columnn\n",
    "\n",
    "    \n",
    "    for i in summary_cols.keys():\n",
    "        \n",
    "        \n",
    "        if summary_type[i] == 'stacked':\n",
    "            stack=True\n",
    "        else:\n",
    "            stack=False\n",
    "           \n",
    "        \n",
    "        if summary_cols[i].shape[0] == 0:\n",
    "            print('Empty dataframe for: {}'.format(i))\n",
    "            \n",
    "            \n",
    "        elif summary_cols[i].shape[1] > 16:\n",
    "            print(\"Skipping {}\".format(i))\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9, 5))\n",
    "            summary_cols[i].plot(ax=ax, kind='bar', stacked=stack,cmap=tab20bc, legend=True)\n",
    "            plt.title('{} - {}'.format(i, summary_units[i]))\n",
    "            ax.legend(loc=(1.1, 0.1))\n",
    "            plt.rc('text', usetex=False)\n",
    "            pdf.savefig(bbox_inches=\"tight\")  # saves the current figure into a pdf page\n",
    "            plt.close()\n",
    "            ##### Table portion, new pagew\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(9, 5),constrained_layout=True)\n",
    "            summary_cols[i].plot(ax=ax1, kind='bar', stacked=stack,cmap=iea_cmap_16)\n",
    "\n",
    "            plt.title('{} - {}'.format(i, summary_units[i]))\n",
    "            ax1.legend(loc=(1.1, 0.1))\n",
    "\n",
    "            cell_text = []\n",
    "            data = summary_cols[i].values\n",
    "            for row in range(0, summary_cols[i].shape[0]):\n",
    "                cell_text.append(['{:.2f}'.format(data[row,col]) for col in np.arange(0,summary_cols[i].shape[1]) ])\n",
    "\n",
    "            ax2.table(cellText=cell_text, colLabels=summary_cols[i].columns, rowLabels=summary_cols[i].index, bbox=[0,0,1,1])\n",
    "            ax2.axis(\"off\")\n",
    "            plt.rc('text', usetex=False)\n",
    "            pdf.savefig(bbox_inches=\"tight\")  # saves the current figure into a pdf page\n",
    "\n",
    "\n",
    "            plt.close()\n",
    "        \n",
    "\n",
    "    # We can also set the file's metadata via the PdfPages object:\n",
    "    d = pdf.infodict()\n",
    "    d['Title'] = 'UKR PSE/NZE 2021 Results'\n",
    "    d['Author'] = 'Craig Hart'\n",
    "    d['Subject'] = 'Quick view of key results'\n",
    "    d['Keywords'] = 'Indonesia power system flexibility'\n",
    "    d['CreationDate'] = datetime.datetime.today()\n",
    "    d['ModDate'] = datetime.datetime.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo_stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in admin layer (state-level) shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Issues with the CRS disappearing and the concatenation of multiple read shapefiles without a CRS implicit\n",
    "from pyproj import CRS\n",
    "\n",
    "default_crs = CRS('EPSG:4326')\n",
    "utm_crs = CRS('EPSG:3857')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_root_folder = \"Y:/GIS/ASEAN/ASEAN-GIS/\"\n",
    "\n",
    "adm1_shp_files = get_files(gis_root_folder, \"shp\", \"_adm1\")\n",
    "adm0_shp_files = get_files(gis_root_folder, \"shp\", \"_adm0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukr_adm = gpd.GeoDataFrame(pd.concat([gpd.read_file(shp)for shp in adm1_shp_files if 'UKR' in shp]), crs= default_crs)\n",
    "asean_adm = gpd.GeoDataFrame(pd.concat([gpd.read_file(shp)for shp in adm0_shp_files if 'UKR' not in shp]), crs= default_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukr_adm_info = pd.read_csv('Y:/GIS/Ukraine/thailand_modelling_regs.csv')\n",
    "ukr_regs = ukr_adm_info.PLEXOS_GRP2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukr_adm = gpd.GeoDataFrame(pd.merge(ukr_adm, ukr_adm_info, on='NAME_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "# %matplotlib inline\n",
    "\n",
    "# fig, ax = plt.subplots(1, figsize=(15, 7.5))\n",
    "\n",
    "\n",
    "# ukr_adm.plot(ax=ax, column='PLEXOS_GRP1', cmap='RdYlGn', alpha = 0.8);\n",
    "\n",
    "# ## Map formatting\n",
    "# # apply_iea_style(ax, tick_spacing=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in packages and create palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "### Function to apply IEA map style (based on FoC CDD map)\n",
    "\n",
    "def apply_iea_style(ax, tick_spacing=20, font_type='Arial', fontsize=10, grid_lines=True):\n",
    "    ### Remove frame\n",
    "    sns.despine(ax=ax, left=True, top=True, bottom= True, right=True)\n",
    "\n",
    "    plt.rcParams['axes.labelsize'] = fontsize\n",
    "    plt.rcParams['axes.titlesize'] = fontsize\n",
    "    plt.rcParams['ytick.labelsize'] = fontsize\n",
    "\n",
    "    plt.rcParams['font.size'] = fontsize\n",
    "    plt.rcParams['font.family'] = font_type\n",
    "    \n",
    "#     plt.rcParams['legend.fontsize'] = fontsize\n",
    "#     plt.rcParams['legend.title_fontsize'] = fontsize\n",
    "\n",
    "    \n",
    "    plot_ax = ax.get_figure().get_axes()[0]\n",
    "    plot_ax.grid(False)\n",
    "    \n",
    "    ## Add gridlines\n",
    "    if grid_lines==True:\n",
    "        plot_ax.grid(b=True, axis='y', color='gray', linestyle='--')\n",
    "        plot_ax.grid(clip_on=False)\n",
    "        \n",
    "        ## Force y-axis tick spacing to each 20deg\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n",
    "\n",
    "        plot_ax.set_yticklabels(['{}\\xb0 N'.format(int(y)) if y > 0 else '{}\\xb0 S'.format(int(abs(y))) if y < 0 \n",
    "                                     else '{}\\xb0'.format(int(y)) for y in plot_ax.get_yticks()])\n",
    "    else:\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    ## Hide x-axis\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "\n",
    "    ## Plot gridlines etc below the map\n",
    "    ax.set_axisbelow(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "def alpha_blending(hex_color, alpha) :\n",
    "    \"\"\" alpha blending as if on the white background.\n",
    "    \"\"\"\n",
    "    foreground_tuple  = matplotlib.colors.hex2color(hex_color)\n",
    "    foreground_arr = np.array(foreground_tuple)\n",
    "    final = tuple( (1. -  alpha) + foreground_arr*alpha )\n",
    "    return(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Alternative categorisation with 2/3 IPPs with bundled/unbundled combined`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_oceans = gpd.read_file('Y:/GIS/global/oceans/ne_10m_ocean.shp').to_crs(default_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.floor(ukr_adm.total_bounds[0] - 2)\n",
    "x1 = np.ceil(ukr_adm.total_bounds[2] + 2)\n",
    "y0 = np.floor(ukr_adm.total_bounds[1])\n",
    "y1 = np.ceil(ukr_adm.total_bounds[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(20,7.5))\n",
    "\n",
    "ax.set_facecolor(color=alpha_blending(iea_palette['bl'], alpha=0.5));\n",
    "\n",
    "# global_oceans.plot(ax=ax, color=iea_palette['bl'], alpha=0.5)\n",
    "\n",
    "ukr_adm.plot(ax=ax, column='PLEXOS_GRP2', cmap=iea_cmap_regs, edgecolor=iea_palette['grey20'], linewidth=0.2,\n",
    "                          missing_kwds={\"color\": iea_palette['grey10'], \"label\": \"Missing values\"})\n",
    "asean_adm.plot(ax=ax, color=iea_palette['grey10'], edgecolor=iea_palette['grey10'], linewidth=0.2)\n",
    "\n",
    "\n",
    "legend_elems = [ Patch(facecolor=iea_cmap_regs(i), label= reg) for i, reg in enumerate(np.sort(ukr_adm.PLEXOS_GRP2.unique()))]\n",
    "               \n",
    "\n",
    "leg = ax.legend(handles=legend_elems, loc=(0.8,0.02), title='Region:', framealpha=1);\n",
    "ax.add_artist(leg)\n",
    "\n",
    "\n",
    "### IEA style formatting of map\n",
    "ax.set_xlim(x0,x1)\n",
    "ax.set_ylim(y0,y1)\n",
    "apply_iea_style(ax=ax, tick_spacing=20, font_type='Arial', fontsize=10, grid_lines=False)\n",
    "\n",
    "fig.savefig(os.path.join(save_dir_plots,'UKR_modelled_regions.png'), dpi=300, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
