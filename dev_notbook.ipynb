{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:47:10 00:00] I:117:__init__ - Logging to C:\\Users\\hart_c\\showcase\\solution-file-processing\\logs\\20240319_104710-UKR.log.\n",
      "[10:47:11 00:00] I:149:__init__ - Initialized SolutionFilesConfig for config_archive/ukraine/UKR.toml.\n"
     ]
    }
   ],
   "source": [
    "import solution_file_processing as sfp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "\n",
    "\n",
    "from solution_file_processing.utils.utils import catch_errors\n",
    "from solution_file_processing.utils.write_excel import write_xlsx_column, write_xlsx_stack, STACK_PALETTE\n",
    "from solution_file_processing.constants import VRE_TECHS\n",
    "from solution_file_processing.timeseries import create_output_11 as create_ts_output_11\n",
    "from solution_file_processing.timeseries import create_output_4 as create_timeseries_output_4\n",
    "from solution_file_processing import log\n",
    "from solution_file_processing.plots import _get_plot_1_variables\n",
    "\n",
    "# Initialize config with toml file\n",
    "c = sfp.SolutionFilesConfig('config_archive/ukraine/UKR.toml')\n",
    "\n",
    "from solution_file_processing.plots import create_plot_1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.v.gen_op_costs_by_reg.groupby([\"model\", \"property\"]).sum().unstack(\"model\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, _,_,gen_stack_by_reg = _get_plot_1_variables(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.v.exports_ts.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stack_total[c.GEO_COLS] = 'Overall'\n",
    "gen_stack_total = gen_stack_total.set_index(['model'] + c.GEO_COLS + ['timestamp'])\n",
    "    \n",
    "\n",
    "exports_total[c.GEO_COLS] = 'Overall'\n",
    "exports_total = exports_total.set_index(['model'] + c.GEO_COLS + ['timestamp'])\n",
    "imports_total = imports_total.set_index(['model'] + c.GEO_COLS + ['timestamp'])\n",
    "\n",
    "\n",
    "gen_stack_total.loc[pd.IndexSlice[:,'Overall', 'Overall', :],'Imports'] = imports_total.values\n",
    "gen_stack_total.loc[pd.IndexSlice[:,'Overall', 'Overall', :],'Net Load w/ Exports'] = \\\n",
    "    gen_stack_total.loc[pd.IndexSlice[:,'Overall', 'Overall', :],'Net Load w/ Exports'].values + exports_total.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stuff for validation .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    c.v.line_imp_exp.droplevel('regZo')\n",
    "except KeyError:\n",
    "    c.v.line_imp_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split hourly demand using split in snapshots\n",
    "from solution_file_processing.utils.write_excel import write_xlsx_column, write_xlsx_stack, STACK_PALETTE\n",
    "import openpyxl as xl \n",
    "  \n",
    "modelled_regs = ['Ukraine', 'CR', 'ER', 'NR', 'SR', 'SWR', 'SR', 'EOR']\n",
    "demand_sector_dict = {}\n",
    "demand_by_reg_df = pd.DataFrame(None)\n",
    "\n",
    "folder_path = 'Y:/Modelling/Ukraine/2023_UKR_ST_Security/01_Data/02_Demand'\n",
    "demand_file_paths = [f for f in os.listdir(folder_path) if ('demand' in f) &('$' not in f)]\n",
    "\n",
    "# for path in demand_file_paths:\n",
    "for path in demand_file_paths:\n",
    "\n",
    "    full_path = os.path.join(folder_path, path)\n",
    "    # load the workbook \n",
    "    wb = xl.load_workbook(full_path)     \n",
    "    sheet_names = wb.sheetnames\n",
    "    est_gen_by_tech_df = {}\n",
    "\n",
    "    for reg in sheet_names:\n",
    "\n",
    "        aux_loss_df = pd.read_excel(full_path, sheet_name=reg, header=7, usecols='A:Z', nrows = 52)\n",
    "        \n",
    "        aux_loss_df = aux_loss_df.loc[aux_loss_df['consumer groups'].str.contains('needs'),:]\n",
    "        aux_loss_df = aux_loss_df.iloc[1:,]\n",
    "\n",
    "        rename_cols = {\"The CHP's own needs\":\"CHP\",\n",
    "                            \"Own needs of HPP\": \"Hydro\",\n",
    "                            \"The NPP's own needs\":\"Nuclear\",\n",
    "                            \"Production needs\":\"Thermal\"}\n",
    "        \n",
    "        aux_loss_df['consumer groups'] = aux_loss_df['consumer groups'].apply(lambda x: rename_cols[x])\n",
    "        aux_loss_df = aux_loss_df.rename(columns={'consumer groups':'Tech'}).set_index('Tech').drop(columns='line').T.rename_axis('Hour')\n",
    "\n",
    "            \n",
    "        aux_losses_by_tech = pd.Series({'CHP':0.05, ## Gas is 4%, coal is 6% \n",
    "                            'Hydro':0.01,\n",
    "                            'Nuclear':0.06,\n",
    "                            'Thermal':0.05})\n",
    "        \n",
    "        tech_palette = {'CHP':'grey20', ## Gas is 4%, coal is 6% \n",
    "                            'Hydro':'bl',\n",
    "                            'Nuclear':'p',\n",
    "                            'Thermal':'brown'}\n",
    "        \n",
    "        est_gen_by_tech_df[reg] = aux_loss_df/aux_losses_by_tech\n",
    "\n",
    "        with pd.ExcelWriter(os.path.join(folder_path,'est_gen_snapshot_{}.xlsx'.format(path[-13:])),\n",
    "                            engine=\"xlsxwriter\") as writer:\n",
    "            # ExcelWriter for some reason uses writer.sheets to access the sheet.\n",
    "            # If you leave it empty it will not know that sheet Main is already there\n",
    "            # and will create a new sheet.\n",
    "\n",
    "            for reg in est_gen_by_tech_df.keys():\n",
    "                write_xlsx_stack(df=est_gen_by_tech_df[reg],\n",
    "                                    writer=writer,\n",
    "                                    sheet_name=reg,\n",
    "                                    palette=tech_palette)\n",
    "                print(f'Created sheet \"{reg}\" in {folder_path}.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.v.gen_by_tech_ts.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solution-file-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
