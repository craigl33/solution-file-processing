{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:09:18 00:00] I:119:__init__ - Logging to C:\\Users\\hart_c\\showcase\\solution-file-processing\\logs\\20240903_180918-CHN.log.\n",
      "[18:09:18 00:00] I:156:__init__ - Initialized SolutionFilesConfig for config_files/china/CHN.toml.\n"
     ]
    }
   ],
   "source": [
    "import solution_file_processing as sfp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "\n",
    "from solution_file_processing.utils.utils import catch_errors\n",
    "from solution_file_processing.utils.write_excel import write_xlsx_column, write_xlsx_stack, STACK_PALETTE, IEA_PALETTE_16, IEA_PALETTE_PLUS, EXTENDED_PALETTE\n",
    "from solution_file_processing.constants import VRE_TECHS, PRETTY_MODEL_NAMES\n",
    "from solution_file_processing.timeseries import create_output_11 as create_ts_output_11\n",
    "from solution_file_processing.timeseries import create_output_4 as create_timeseries_output_4\n",
    "from solution_file_processing import log\n",
    "from solution_file_processing.plots import _get_plot_1_variables\n",
    "\n",
    "# Initialize confi||g with toml file\n",
    "c = sfp.SolutionFilesConfig('config_files/china/CHN.toml')\n",
    "\n",
    "# c = sfp.SolutionFilesConfig('config_files/thailand/THA.toml')\n",
    "ix = pd.IndexSlice  \n",
    "\n",
    "from solution_file_processing.plots import create_plot_1a, create_plot_2b_ref_plots, create_plot_2, create_plot_10_ts_by_model\n",
    "from solution_file_processing.plots import create_plot_6_ldc, create_plot_7_co2_savings, create_plot_8_services, create_plot_9_av_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purch_df = c.o.purch_df\n",
    "dsm_profiles_ts = purch_df[\n",
    "                purch_df.name.str.contains('_Shift') & (purch_df.property == 'Load')].groupby(\n",
    "                ['model', 'timestamp']).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.from_pandas(pd.DataFrame(None), npartitions=1)\n",
    "df = df.compute()\n",
    "df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = purch_df.loc[1,:].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm_profiles_orig_ts = x[\n",
    "        x.name.str.contains('_Shift') & (x.property == 'x')].groupby(\n",
    "        ['model', 'timestamp']).agg({'value': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purch_df.shape[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DSM profiles\n",
    "\"\"\"\n",
    "# Model filler for comparison of models with different inputs (e.g. DSM or EVs not included)\n",
    "# Series with indices matching the columns of the DF for filling in missing columns\n",
    "\n",
    "model_filler = pd.Series(data=[1] * len(c.v.model_names), index=c.v.model_names).rename_axis('model')\n",
    "\n",
    "len(c.o.purch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if purch_df.shape[0] > 0:\n",
    "    dsm_profiles_orig_ts = purch_df[\n",
    "        purch_df.name.str.contains('_Shift') & (purch_df.property == 'x')].groupby(\n",
    "        ['model', 'timestamp']).sum()\n",
    "    if not dsm_profiles_orig_ts.shape[0] == 0:\n",
    "        dsm_profiles_orig_ts = (dsm_profiles_orig_ts.value.unstack('model') * model_filler).fillna(0).stack('model').reorder_levels(\n",
    "            ['model', 'timestamp'])\n",
    "    else:\n",
    "        dsm_profiles_orig_ts = pd.DataFrame(data=[0] * len(self.c.v.customer_load_ts.index),\n",
    "                                index=self.c.v.customer_load_ts.index, columns=['value'])\n",
    "else:\n",
    "    dsm_profiles_orig_ts = pd.DataFrame(data=[0] * len(self.c.v.customer_load_ts.index),    \n",
    "                                index=self.c.v.customer_load_ts.index, columns=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def extract_plexos_LT_results(model_path):\n",
    "    \n",
    "    if not isinstance(model_path, Path):\n",
    "        model_path = Path(model_path)\n",
    "\n",
    "    exp_out_path = model_path / '03_Modelling/01_InputData/08_ExpUnits/'\n",
    "    lt_solns_path = exp_out_path / 'LT_solution_files/'\n",
    "    lt_soln_zips = [ lt_solns_path / f for f in  os.listdir(lt_solns_path)]\n",
    "\n",
    "    for z in lt_soln_zips:\n",
    "        with ZipFile(z, 'r') as zipObj:\n",
    "            file_list = zipObj.namelist()\n",
    "            pattern = '* Units.csv'\n",
    "\n",
    "            filtered_list = []\n",
    "            for file in file_list:\n",
    "                if fnmatch.fnmatch(file, pattern):\n",
    "                    filtered_list.append(file)\n",
    "            \n",
    "            for f in filtered_list:\n",
    "                try:\n",
    "                    zipObj.extract(f, exp_out_path)\n",
    "                    print(f'Extracted {f} to {exp_out_path}')\n",
    "                except PermissionError:\n",
    "                    print(f'Could not extract {f} to {exp_out_path}. File already exists.')\n",
    "                    continue\n",
    "            \n",
    "            # Create a list of all the extracted files for Generator Units\n",
    "            # This can then be used to increase the capacity of the units by 5% and integerise the capacity\n",
    "            exp_units_files = [ exp_out_path / f for f in  os.listdir(exp_out_path) if 'Units.csv' in f]\n",
    "\n",
    "            for f in exp_units_files:\n",
    "                df = pd.read_csv(f)\n",
    "                df['Value'] = df['Value'] * 1.05\n",
    "                df.loc[df.Name.str.contains('Gas'),'Value'] = df['Value'].apply(np.round)\n",
    "                try:\n",
    "                    df.to_csv(f, index=False)\n",
    "                except PermissionError:\n",
    "                    print(f'Could not write to {f}. File already exists.')\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('Y:/Modelling/Ukraine/2023_UKR_ST_Security/')\n",
    "extract_plexos_LT_results(model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solution-file-processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
